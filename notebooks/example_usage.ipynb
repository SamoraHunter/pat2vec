{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pat2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import logging\n",
    "from elasticsearch.exceptions import AuthenticationException\n",
    "\n",
    "# Set up a basic logger to be used throughout the notebook\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    stream=sys.stdout,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Fix the random seed for reproducibility in unit testing\n",
    "\n",
    "random_seed_value = 42\n",
    "\n",
    "np.random.seed(random_seed_value)\n",
    "\n",
    "random.seed(random_seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Log the current working directory\n",
    "logger.info(f\"Current Working Directory: {os.getcwd()}\")\n",
    "\n",
    "# 2. Log Python's sys.path\n",
    "logger.info(f\"Python Path: {sys.path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove dir\n",
    "clear_previous_outputs = True\n",
    "\n",
    "if clear_previous_outputs:\n",
    "\n",
    "    shutil.rmtree(\"new_project\", ignore_errors=True)\n",
    "\n",
    "    shutil.rmtree(\"new_project_ipw\", ignore_errors=True)\n",
    "\n",
    "    shutil.rmtree(\"treatment_doc_extract\", ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure dependencies are on path\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Define relative paths from the current working directory\n",
    "path_to_medcat_model_pack = os.path.abspath(\n",
    "    os.path.join(\n",
    "        current_dir,\n",
    "        \"..\",\n",
    "        \"..\",\n",
    "        \"medcat_models\",\n",
    "        \"medcat_model_pack_422d1d38fc58f158.zip\",\n",
    "    )\n",
    ")\n",
    "\n",
    "path_to_snomed_ct_file = os.path.abspath(\n",
    "    os.path.join(\n",
    "        current_dir,\n",
    "        \"..\",\n",
    "        \"..\",\n",
    "        \"snomed\",\n",
    "        \"SnomedCT_InternationalRF2_PRODUCTION_20231101T120000Z\",\n",
    "        \"SnomedCT_InternationalRF2_PRODUCTION_20231101T120000Z\",\n",
    "        \"Full\",\n",
    "        \"Terminology\",\n",
    "        \"sct2_StatedRelationship_Full_INT_20231101.txt\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define the relative path\n",
    "path_to_gloabl_files = \"../../\"\n",
    "\n",
    "additional_path_to_pat2vec = \"pat2vec\"\n",
    "\n",
    "additional_path_to_pat2vec = os.path.abspath(\n",
    "    os.path.join(path_to_gloabl_files, additional_path_to_pat2vec)\n",
    ")\n",
    "\n",
    "# Get the absolute path of the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Combine the current directory with the relative path\n",
    "absolute_path = os.path.abspath(os.path.join(current_dir, path_to_gloabl_files))\n",
    "\n",
    "# Usage examples\n",
    "logger.info(f\"Path to medcat model pack: {path_to_medcat_model_pack}\")\n",
    "logger.info(f\"Path to SNOMED CT file: {path_to_snomed_ct_file}\")\n",
    "logger.info(f\"Path to global files: {path_to_gloabl_files}\")\n",
    "logger.info(f\"Additional path to pat2vec: {additional_path_to_pat2vec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, path_to_gloabl_files)\n",
    "sys.path.insert(0, additional_path_to_pat2vec)\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Add the grandparent directory of the current directory to the Python path\n",
    "grandparent_dir = os.path.dirname(parent_dir)\n",
    "sys.path.append(grandparent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.logger_setup import setup_logger\n",
    "\n",
    "# Get the logger (this will reconfigure the logger set up earlier)\n",
    "logger = setup_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get treatment_docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.config_pat2vec import config_class\n",
    "\n",
    "config_obj = config_class(\n",
    "    medcat=False,  # Load medcat, ensure model pack is in gloabl_files/medcat_models/ ..examplemodelpack.zip\n",
    "    override_medcat_model_path=path_to_medcat_model_pack,\n",
    "    proj_name=\"treatment_doc_extract\",\n",
    "    verbosity=0,\n",
    "    global_start_year=1995,  # Set the start date, this will extract data between these dates.\n",
    "    global_end_year=2024,\n",
    "    global_start_month=1,\n",
    "    global_end_month=12,\n",
    "    global_start_day=1,\n",
    "    global_end_day=31,\n",
    "    lookback=False,  # Set to True if you want to look back at the previous year and month\n",
    "    testing=True,  # Set to True if you want to run in testing mode, this will use dummy data for testing.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.main_pat2vec import main\n",
    "\n",
    "pat2vec_obj = main(\n",
    "    cogstack=True,\n",
    "    use_filter=False,\n",
    "    json_filter_path=None,\n",
    "    random_seed_val=random_seed_value,\n",
    "    hostname=None,\n",
    "    config_obj=config_obj,\n",
    ")  # initialize the pat2vec object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pat2vec_obj.config_obj.testing:\n",
    "    logger.info(\"Testing mode is enabled, skipping authentication check.\")\n",
    "else:\n",
    "    # Check if the Elasticsearch client is authenticated # advise user to check credentials\n",
    "    try:\n",
    "        pat2vec_obj.cs.elastic.info()\n",
    "        logger.info(\"Elasticsearch authentication successful.\")\n",
    "    except AuthenticationException as e:\n",
    "        logger.error(f\"Authentication failed: {e.info['error']['reason']}\")\n",
    "        logger.warning(\n",
    "            \"Please check your Elasticsearch credentials in the configuration file.\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            f\"An error occurred while checking Elasticsearch authentication: {e}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snomed_example = False\n",
    "\n",
    "if snomed_example:\n",
    "\n",
    "    from snomed_methods import snomed_methods_v1\n",
    "\n",
    "    path_to_sct2 = path_to_snomed_ct_file\n",
    "\n",
    "    medcat_path = path_to_medcat_model_pack\n",
    "\n",
    "    snomed_relations_obj = snomed_methods_v1.snomed_relations(\n",
    "        medcat=True, snomed_rf2_full_path=path_to_sct2, medcat_path=medcat_path\n",
    "    )\n",
    "\n",
    "    outcome_variable_cui_for_filter = \"109989006\"  # myeloma\n",
    "\n",
    "    logger.info(f\"Outcome variable CUI for filter: {outcome_variable_cui_for_filter}\")\n",
    "\n",
    "    filter_root_cui = outcome_variable_cui_for_filter\n",
    "    logger.info(f\"Filter root CUI: {filter_root_cui}\")\n",
    "\n",
    "    retrieved_codes_snomed_tree, retrieved_names_snomed_tree = (\n",
    "        snomed_relations_obj.recursive_code_expansion(\n",
    "            filter_root_cui, n_recursion=3, debug=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    logger.info(\n",
    "        f\"Retrieved codes (first 5): {retrieved_codes_snomed_tree[0:5]}, Total codes: {len(retrieved_codes_snomed_tree)}, Total names: {len(retrieved_names_snomed_tree)}\"\n",
    "    )\n",
    "\n",
    "    retrieved_codes_medcat_cdb, retrieved_names_medcat_cdb = (\n",
    "        snomed_relations_obj.get_medcat_cdb_most_similar(\n",
    "            filter_root_cui, context_type=\"xxxlong\", type_id_filter=[], topn=50\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add terms to search the document indicies for\n",
    "\n",
    "term_list = [\n",
    "    \"myeloma\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.pre_processing import (\n",
    "    get_treatment_docs_by_iterative_multi_term_cohort_searcher_no_terms_fuzzy,\n",
    ")\n",
    "\n",
    "# Example getting a patient cohort by the presence of terms in their clinical documents\n",
    "\n",
    "# We start by extracting the documents across textual document sources with fuzzy string matching\n",
    "\n",
    "treatment_docs = get_treatment_docs_by_iterative_multi_term_cohort_searcher_no_terms_fuzzy(\n",
    "    pat2vec_obj=pat2vec_obj,\n",
    "    term_list=term_list,  # List of terms to search for\n",
    "    overwrite=True,  # overwrite existing treatment_docs.csv else append results\n",
    "    append=False,  # Append results to existing treatment_docs.csv\n",
    "    verbose=9,  # Adjust verbosity for logging\n",
    "    mct=True,  # Include clinical notes text sources, this will search an additional document index\n",
    "    textual_obs=True,  # Include observations index text sources, this will search an additional document index\n",
    "    additional_filters=None,  # Add additional filters to the search such as document type.\n",
    "    all_fields=False,  # Return all fields from indicies instead of just a typical subset.\n",
    ")\n",
    "\n",
    "treatment_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example get cohort by drug treatment for cross reference etc\n",
    "\n",
    "# Example, I want to get a cohort of patients who have drug orders to check against their diagnosis status from the previous step.\n",
    "\n",
    "from pat2vec.util.pre_get_drug_treatment_docs import iterative_drug_treatment_search\n",
    "import pandas as pd\n",
    "\n",
    "retrieve_cohort_by_drug_treatment = False\n",
    "\n",
    "if retrieve_cohort_by_drug_treatment:\n",
    "\n",
    "    search_terms_list = [\"asprin\", \"ibuprofen\", \"Emtricitabine\", \"Mepacrine\"]\n",
    "    output_file_path = \"drug_treatment_records.csv\"\n",
    "\n",
    "    iterative_drug_treatment_search(\n",
    "        pat2vec_obj=pat2vec_obj,\n",
    "        search_terms=search_terms_list,\n",
    "        output_file_path=output_file_path,\n",
    "        verbose=5,  # Adjust verbosity for logging\n",
    "        drop_duplicates=True,  # Search terms can produce duplicates, remove by order guid.\n",
    "        overwrite=True,  # Overwrite initial output file\n",
    "    )\n",
    "\n",
    "    # Load the csv file\n",
    "    df_drug_treatment_cohort = pd.read_csv(output_file_path)\n",
    "    df_drug_treatment_cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.config_pat2vec import config_class\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from pat2vec.util.post_processing_process_csv_files import process_csv_files\n",
    "from pat2vec.util.post_processing import extract_datetime_to_column\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "# Configuration dictionary for main options in pat2vec\n",
    "main_options_dict = {\n",
    "    \"demo\": True,  # Enable demographic information (Ethnicity mapped to UK census categories, age, death).\n",
    "    \"bmi\": True,  # Enable BMI (Body Mass Index) information.\n",
    "    \"bloods\": True,  # Enable blood-related information\n",
    "    \"drugs\": True,  # Enable drug-related information\n",
    "    \"diagnostics\": True,  # Enable diagnostic information\n",
    "    \"core_02\": True,  # Enable core_02 information\n",
    "    \"bed\": True,  # Enable bed n information\n",
    "    \"vte_status\": True,  # Enable VTE () status information\n",
    "    \"hosp_site\": True,  # Enable hospital site information\n",
    "    \"core_resus\": True,  # Enable core resuscitation information\n",
    "    \"news\": True,  # Enable NEWS (National Early Warning Score) information\n",
    "    \"smoking\": True,  # Enable smoking-related information\n",
    "    \"annotations\": True,  # Enable EPR documents annotations via MedCat\n",
    "    \"annotations_mrc\": True,  # Enable MRC (Additional clinical note observations index) annotations via MedCat\n",
    "    \"negated_presence_annotations\": False,  # Enable or disable negated presence annotations\n",
    "    \"appointments\": False,  # Enable appointments information\n",
    "    \"annotations_reports\": False,  # Enable reports information\n",
    "    \"textual_obs\": False,  # Enable textual observations (basic_observations index) annotations via MedCat\n",
    "}\n",
    "\n",
    "# Configuration dictionary for annotation filtering, only base annotations meeting this threshold will be included.\n",
    "annot_filter_arguments = {\n",
    "    \"acc\": 0.8,  # base concept accuracy\n",
    "    \"types\": [\n",
    "        \"qualifier value\",\n",
    "        \"procedure\",\n",
    "        \"substance\",\n",
    "        \"finding\",\n",
    "        \"environment\",\n",
    "        \"disorder\",\n",
    "        \"observable entity\",\n",
    "    ],  # umls list of types for medcat filter\n",
    "    # 'types': ['qualifier value', 'procedure', 'substance', 'finding', 'environment', 'disorder', 'observable entity', 'organism', 'phenomenon', 'anatomy', 'conceptual entity', 'physical object', 'intellectual product', 'occupation or discipline', 'mental or behavioral dysfunction', 'geographic area', 'population group', 'biomedical or dental material', 'medical device', 'classification', 'regulation or law', 'health care activity', 'health care related organization', 'professional or occupational group', 'group', 'attribute', 'individual behavior']\n",
    "    \"Time_Value\": [\n",
    "        \"Recent\",\n",
    "        \"Past\",\n",
    "    ],  # Specify the values you want to include in a list. Must be defined in medcat model. # Example ['Recent', 'Past', 'Subject/Experiencer']\n",
    "    \"Time_Confidence\": 0.8,  # Specify the confidence threshold as a float\n",
    "    \"Presence_Value\": [\"True\"],  # Specify the values you want to include in a list\n",
    "    \"Presence_Confidence\": 0.8,  # Specify the confidence threshold as a float\n",
    "    \"Subject_Value\": [\"Patient\"],  # Specify the values you want to include in a list\n",
    "    \"Subject_Confidence\": 0.8,  # Specify the confidence threshold as a float\n",
    "}\n",
    "\n",
    "# Filter data batches by terms before processing.\n",
    "\n",
    "epr_docs_term_regex: Optional[Union[str, None]] = None\n",
    "mct_docs_term_regex: Optional[Union[str, None]] = None\n",
    "\n",
    "# Example bloods_filter_term_list: Optional[Union[List[str], None]] = ['wbc'] # This will only include basic observations with this item name analysed.\n",
    "bloods_filter_term_list: Optional[Union[List[str], None]] = None\n",
    "\n",
    "# Example mct_docs_document_type_filter_list: Optional[Union[List[str], None]] = ['KHMDC Integrated report'] # This will only include documents with this document type field value.\n",
    "\n",
    "mct_docs_document_type_filter_list: Optional[Union[List[str], None]] = None\n",
    "epr_docs_document_type_filter_list: Optional[Union[List[str], None]] = None\n",
    "\n",
    "data_type_filter_dict: Dict[str, any] = {\n",
    "    \"filter_term_lists\": {\n",
    "        \"epr_docs\": epr_docs_document_type_filter_list,\n",
    "        \"mct_docs\": mct_docs_document_type_filter_list,\n",
    "        \"bloods\": bloods_filter_term_list,\n",
    "    },\n",
    "    \"epr_docs_term_regex\": epr_docs_term_regex,\n",
    "    \"mct_docs_term_regex\": mct_docs_term_regex,\n",
    "}\n",
    "\n",
    "# Example date settings:\n",
    "# start_date=(datetime(2020, 1, 1)) Start date for processing\n",
    "\n",
    "# Define the length of the time window, example 1 year and 15 days, only data within this window will be processed.\n",
    "# years=1,      # Number of years to add to the start date\n",
    "# months=0,  # Number of months to add to the start date\n",
    "# days=15,  # Number of days to add to the start date\n",
    "\n",
    "# Define the interval between time windows. Example 1 year. Each vector/row output will be based on this interval.\n",
    "# time_window_interval_delta = relativedelta(years=1)\n",
    "\n",
    "# lookback = True #This determines the direction of the time length window. True = backward, False = forward. Our time window (+1 years, 15 days) is therefore 2020, 1, 1 - 2021, 1, 15.\n",
    "\n",
    "# IPW settings:\n",
    "\n",
    "# Init config obj\n",
    "\n",
    "# Hypothetical date config_obj configuration:\n",
    "# I want all patients data between Feb 2015 and Jul 2020. This date window will extract and create the batched patient data for this time window.\n",
    "\n",
    "# global_start_year=2015,\n",
    "# global_start_month=2,\n",
    "# global_end_year=2020,\n",
    "# global_end_month=6,\n",
    "# global_start_day = 1,\n",
    "# global_end_day = 1,\n",
    "\n",
    "# I want patient vectors starting from Feb 2019 to Feb 2020 as I would like to see if X medical event is recorded on those taking medication Y\n",
    "# start_date=(datetime(2019, 2, 1)),\n",
    "# years=1,\n",
    "# months=0,\n",
    "# days=0,\n",
    "# lookback = False # 2019 to 2020 is forward in time.\n",
    "# I would like a single vector for each patient\n",
    "# time_window_interval_delta = relativedelta(years=1)\n",
    "# I would like 1 vector per month per patient for the 1 year time window\n",
    "# time_window_interval_delta = relativedelta(months=1)\n",
    "\n",
    "# Creating a configuration object for a specific task or project\n",
    "config_obj = config_class(\n",
    "    remote_dump=False,  # Flag for remote data dumping. partially deprecated.\n",
    "    suffix=\"\",  # Suffix for file names\n",
    "    treatment_doc_filename=\"test_files/treatment_docs.csv\",  # Filename for treatment documentation\n",
    "    treatment_control_ratio_n=1,  # Ratio for treatment to control\n",
    "    proj_name=\"new_project\",  # Project name. patient data batches and vectors stored here.\n",
    "    current_path_dir=\"\",  # Current path directory\n",
    "    main_options=main_options_dict,  # Dictionary for main options\n",
    "    start_date=(datetime(1995, 1, 1)),  # Starting date for processing\n",
    "    years=30,  # Number of years to add to the start date. Set the duration of the time window. Window is defined as the start date + years/months/days set here.\n",
    "    months=0,  # Number of months to add to the start date\n",
    "    days=0,  # Number of days to add to the start date\n",
    "    batch_mode=True,  # Flag for batch processing mode. Only functioning mode.\n",
    "    store_annot=True,  # Flag to store annotations. partially deprecated.\n",
    "    share_sftp=True,  # Flag for sharing via SFTP. partially deprecated\n",
    "    multi_process=False,  # Flag for multi-process execution. deprecated.\n",
    "    strip_list=True,  # Flag for stripping lists, this will check for completed patients before starting to avoid redundancy.\n",
    "    verbosity=0,  # Verbosity level 0-9 printing debug messages\n",
    "    random_seed_val=random_seed_value,  # Random seed value for reproducibility of controls.\n",
    "    testing=True,  # Flag for testing mode. Will use dummy data.\n",
    "    dummy_medcat_model=True,  # Flag for dummy MedCAT model, used if testing == True, this will simulate a MedCAT model.\n",
    "    use_controls=False,  # If true this will add desired ratio of controls at random from global pool, requires configuring with a master list of patients.\n",
    "    medcat=False,  # Flag for MedCAT processing. #will load medcat into memory and use for annotating.\n",
    "    start_time=datetime.now(),  # Current timestamp as the start time for logging and progress bar\n",
    "    patient_id_column_name=\"auto\",  # Column name for patient ID, auto will try to find it. Example \"client_idcode\"\n",
    "    annot_filter_options=annot_filter_arguments,  # Annotation filtering options\n",
    "    # Global start year. #set the limits of the time window data can be drawn from. Start should not precede start date set above.\n",
    "    global_start_year=1995,  # Global dates are overwritten by individual patient windows to match patient window. # Ensure that global start year/month/day is before end year/month/day\n",
    "    global_start_month=1,  # Global start month\n",
    "    global_end_year=2025,  # Global end year\n",
    "    global_end_month=1,  # Global end month\n",
    "    global_start_day=1,\n",
    "    global_end_day=1,\n",
    "    ## Use these if each patient has their own individual time window. Requires preparing a table of start dates.\n",
    "    # individual_patient_window = True,\n",
    "    # individual_patient_window_df = pd.read_csv('ipw_overlap.csv'),\n",
    "    # individual_patient_window_start_column_name = 'updatetime_manual_offset',\n",
    "    # individual_patient_id_column_name = 'client_idcode',\n",
    "    # individual_patient_window_controls_method = 'full',\n",
    "    shuffle_pat_list=False,  # Flag for shuffling patient list\n",
    "    time_window_interval_delta=relativedelta(\n",
    "        years=31\n",
    "    ),  # specify the time window to collapse each feature vector into, years=1 is one vector per year within the global time window\n",
    "    split_clinical_notes=True,  # will split clinical notes by date and treat as individual documents with extracted dates. Requires note splitter module.\n",
    "    lookback=False,  # when calculating individual patient window from table of start dates, will calculate backwards in time if true. Else Forwards. When calculating from global start date, will calculate backwards or forwards respectively.\n",
    "    add_icd10=False,  # append icd 10 codes to annot batches. Can be found under current_pat_documents_annotations/%client_idcode%.csv.\n",
    "    add_opc4s=False,  # needs icd10 true also. Can be found under current_pat_documents_annotations/%client_idcode%.csv\n",
    "    override_medcat_model_path=path_to_medcat_model_pack,  # Force medcat model path, if None uses defaults for env. #Can be set in paths.py with medcat_path = %path to medcat model pack.zip\"\n",
    "    data_type_filter_dict=None,  # Dictionary for data type filter, see examples above.\n",
    "    filter_split_notes=True,  # If enabled, will reapply global time window filter post clinical note splitting. Recommended to enable if split notes enabled.\n",
    "    prefetch_pat_batches=False,  # If enabled, will fetch batches for entire patient list and pre poulate batch folders with individual pat batches. Out of memory issues.\n",
    "    sample_treatment_docs=5,  # If int > 0, will sample treatment documents from the treatment_docs.csv file. This is useful for testing and debugging / pilot run purposes.\n",
    "    test_data_path=None,  # Path to test data. None and testing True uses 'test_files/treatment_docs.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.main_pat2vec import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj = main(\n",
    "    cogstack=True,\n",
    "    use_filter=False,\n",
    "    json_filter_path=None,\n",
    "    random_seed_val=42,\n",
    "    hostname=None,\n",
    "    config_obj=config_obj,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View patient list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj.all_patient_list[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj.config_obj.date_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make pat vectors for pat 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj.pat_maker(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove specific patient raw documents and annotations:\n",
    "from pat2vec.util.post_processing import remove_file_from_paths\n",
    "\n",
    "# remove_file_from_paths(pat2vec_obj.all_patient_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the maximum number of retries\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "# Iterate through the patient list starting from index 0\n",
    "for i in tqdm(range(0, len(pat2vec_obj.all_patient_list))):\n",
    "    retries = 0\n",
    "    success = False\n",
    "\n",
    "    while retries < MAX_RETRIES and not success:\n",
    "        try:\n",
    "            # Try to process the patient\n",
    "            pat2vec_obj.pat_maker(i)\n",
    "            success = True  # Mark as successful if no exception is raised\n",
    "\n",
    "        except KeyError as e:\n",
    "            # Handle specific exception\n",
    "            logger.warning(\n",
    "                f\"KeyError at index {i} for patient {pat2vec_obj.all_patient_list[i]}: {e}. Retrying after removal...\"\n",
    "            )\n",
    "            remove_file_from_paths(pat2vec_obj.all_patient_list[i])\n",
    "            retries += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            # Handle generic exceptions\n",
    "            logger.error(\n",
    "                f\"Exception at index {i} for patient {pat2vec_obj.all_patient_list[i]}: {e}. Skipping this patient...\"\n",
    "            )\n",
    "            break  # Break the retry loop for non-retryable exceptions\n",
    "\n",
    "        finally:\n",
    "            pat2vec_obj.t.update(1)  # Update progress\n",
    "\n",
    "    if not success:\n",
    "        logger.error(\n",
    "            f\"Failed to process index {i} for patient {pat2vec_obj.all_patient_list[i]} after {MAX_RETRIES} retries.\"\n",
    "        )\n",
    "\n",
    "pat2vec_obj.t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = f\"{pat2vec_obj.proj_name}/current_pat_lines_parts\"  # Patient vectors are stored individually in this directory.\n",
    "output_csv_file = \"output_file\"\n",
    "\n",
    "# Specify the directory where you want to create the file\n",
    "directory = pat2vec_obj.proj_name + \"/output_directory\"\n",
    "\n",
    "# Check if the directory exists, if not, create it\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# We will join the individual patient vectors into a single output file. This is useful for filtering.\n",
    "output_csv_file_filename = process_csv_files(\n",
    "    input_directory,\n",
    "    out_folder=directory,\n",
    "    output_filename_suffix=output_csv_file,\n",
    "    part_size=336,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_csv_file_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_datetime_to_column(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build all document batches dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will merge all document source batches into a single file. This is useful for filtering. May produce a large file.\n",
    "\n",
    "from pat2vec.util.post_processing_build_methods import build_merged_epr_mct_doc_df\n",
    "\n",
    "all_pat_list = pat2vec_obj.all_patient_list\n",
    "\n",
    "dfd = build_merged_epr_mct_doc_df(all_pat_list, pat2vec_obj.config_obj, overwrite=True)\n",
    "\n",
    "# dfd = pd.read_csv(dfd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build all annotation batches dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will merge all annotation source batches into a single file. This is useful for filtering. May produce a large file.\n",
    "\n",
    "from pat2vec.util.post_processing_build_methods import build_merged_epr_mct_annot_df\n",
    "\n",
    "all_pat_list = pat2vec_obj.all_patient_list\n",
    "\n",
    "dfa = build_merged_epr_mct_annot_df(\n",
    "    all_pat_list, pat2vec_obj.config_obj, overwrite=True\n",
    ")\n",
    "\n",
    "dfa = pd.read_csv(dfa)\n",
    "\n",
    "dfa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build additional batches from individual patient data batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will merge all drug source batches into a single file. This is useful for filtering. May produce a large file.\n",
    "\n",
    "from pat2vec.util.post_processing_build_methods import merge_drugs_csv\n",
    "\n",
    "all_pat_list = pat2vec_obj.all_patient_list\n",
    "\n",
    "merged_drugs_path = merge_drugs_csv(\n",
    "    all_pat_list, pat2vec_obj.config_obj, overwrite=True\n",
    ")\n",
    "\n",
    "merged_drugs = pd.read_csv(merged_drugs_path)\n",
    "merged_drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfmdi = pd.read_csv('new_project/merged_input_pat_batches/merged_drugs_batches.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in dfmdi.select_dtypes(exclude=[np.number]).columns:\n",
    "#     assert dfmdi[col].astype(str).equals(merged_drugs[col].astype(str)), f\"Mismatch in column: {col}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will merge all diagnostics source batches into a single file. This is useful for filtering. May produce a large file.\n",
    "\n",
    "from pat2vec.util.post_processing_build_methods import merge_diagnostics_csv\n",
    "\n",
    "all_pat_list = pat2vec_obj.all_patient_list\n",
    "\n",
    "merged_diagnostics_path = merge_diagnostics_csv(\n",
    "    all_pat_list, pat2vec_obj.config_obj, overwrite=True\n",
    ")\n",
    "\n",
    "merged_diagnostics = pd.read_csv(merged_diagnostics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.post_processing_build_methods import merge_news_csv\n",
    "\n",
    "all_pat_list = pat2vec_obj.all_patient_list\n",
    "\n",
    "merged_news_path = merge_news_csv(all_pat_list, pat2vec_obj.config_obj, overwrite=True)\n",
    "\n",
    "# merged_news = pd.read_csv(merged_news_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.post_processing_build_methods import merge_bmi_csv\n",
    "\n",
    "all_pat_list = pat2vec_obj.all_patient_list\n",
    "\n",
    "merged_bmi_path = merge_bmi_csv(all_pat_list, pat2vec_obj.config_obj, overwrite=True)\n",
    "\n",
    "# merged_bmi = pd.read_csv(merged_bmi_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.post_processing_build_methods import build_merged_bloods\n",
    "\n",
    "all_pat_list = pat2vec_obj.all_patient_list\n",
    "\n",
    "merged_bloods_path = build_merged_bloods(\n",
    "    all_pat_list, pat2vec_obj.config_obj, overwrite=True\n",
    ")\n",
    "\n",
    "merged_bloods = pd.read_csv(merged_bloods_path)\n",
    "merged_bloods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv('new_project/merged_input_pat_batches/merged_bloods_batches.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.post_processing_build_methods import merge_demographics_csv\n",
    "\n",
    "all_pat_list = pat2vec_obj.all_patient_list\n",
    "\n",
    "merged_demographics_path = merge_demographics_csv(\n",
    "    all_pat_list, pat2vec_obj.config_obj, overwrite=True\n",
    ")\n",
    "\n",
    "merged_demographics = pd.read_csv(merged_demographics_path)\n",
    "\n",
    "merged_demographics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the annotation batches by a snomed cui and its related codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snomed_methods_example = False\n",
    "\n",
    "if snomed_methods_example:\n",
    "\n",
    "    from snomed_methods import snomed_methods_v1\n",
    "\n",
    "    path_to_sct2 = path_to_snomed_ct_file\n",
    "\n",
    "    medcat_path = path_to_medcat_model_pack\n",
    "\n",
    "    snomed_relations_obj = snomed_methods_v1.snomed_relations(\n",
    "        medcat=True, snomed_rf2_full_path=path_to_sct2, medcat_path=medcat_path\n",
    "    )\n",
    "    outcome_variable_cui_for_filter = \"40733004\"  # infection\n",
    "\n",
    "    logger.info(f\"Outcome variable CUI for filter: {outcome_variable_cui_for_filter}\")\n",
    "\n",
    "    filter_root_cui = outcome_variable_cui_for_filter\n",
    "    logger.info(f\"Filter root CUI: {filter_root_cui}\")\n",
    "\n",
    "    retrieved_codes_snomed_tree, retrieved_names_snomed_tree = (\n",
    "        snomed_relations_obj.recursive_code_expansion(\n",
    "            filter_root_cui, n_recursion=3, debug=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    logger.info(\n",
    "        f\"Retrieved codes (Snomed tree, first 5): {retrieved_codes_snomed_tree[0:5]}, Total codes: {len(retrieved_codes_snomed_tree)}, Total names: {len(retrieved_names_snomed_tree)}\"\n",
    "    )\n",
    "\n",
    "    logger.info(\n",
    "        f\"Retrieved names (Snomed tree, first 10): {retrieved_names_snomed_tree[0:10]}\"\n",
    "    )\n",
    "\n",
    "    retrieved_codes_medcat_cdb, retrieved_names_medcat_cdb = (\n",
    "        snomed_relations_obj.get_medcat_cdb_most_similar(\n",
    "            filter_root_cui, context_type=\"xxxlong\", type_id_filter=[], topn=25\n",
    "        )\n",
    "    )\n",
    "\n",
    "    logger.info(\n",
    "        f\"Retrieved names (MedCAT CDB, first 10): {retrieved_names_medcat_cdb[0:10]}\"\n",
    "    )\n",
    "\n",
    "    all_names_list = list(set(retrieved_names_medcat_cdb + retrieved_names_snomed_tree))\n",
    "\n",
    "    all_codes_list = list(set(retrieved_codes_medcat_cdb + retrieved_codes_snomed_tree))\n",
    "\n",
    "    logger.info(f\"Total unique names combined: {len(all_names_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply misc methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pat2vec.all_methods import pat2vec_methods\n",
    "\n",
    "# p2v = pat2vec_methods()\n",
    "\n",
    "# p2v.produce_filtered_annotation_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build IPW dataframe\n",
    "\n",
    "\n",
    "Find the latest/earliest record for one of [268910001, 62315008, 55822004, 49727002]\n",
    "\n",
    "We can use this in another main block with:\n",
    "\n",
    "individual_patient_window = True,\n",
    "\n",
    "individual_patient_window_df = pd.read_csv('ipw_overlap.csv'),\n",
    "\n",
    "individual_patient_window_start_column_name = 'updatetime_manual_offset',\n",
    "\n",
    "individual_patient_id_column_name = 'client_idcode',\n",
    "\n",
    "individual_patient_window_controls_method = 'full', \n",
    "\n",
    "To limit each patients data to a specific individual time window. With controls we can match the time window per control or pull their 'full' data for the global time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_filter_arguments = {\n",
    "    \"acc\": 0.6,  # base concept accuracy\n",
    "    # umls list of types for medcat filter\n",
    "    #'types': ['qualifier value', 'procedure', 'substance', 'finding', 'environment', 'disorder', 'observable entity'],\n",
    "    \"types\": [\n",
    "        \"qualifier value\",\n",
    "        \"procedure\",\n",
    "        \"substance\",\n",
    "        \"finding\",\n",
    "        \"environment\",\n",
    "        \"disorder\",\n",
    "        \"observable entity\",\n",
    "        \"organism\",\n",
    "        \"phenomenon\",\n",
    "        \"anatomy\",\n",
    "        \"conceptual entity\",\n",
    "        \"physical object\",\n",
    "        \"intellectual product\",\n",
    "        \"occupation or discipline\",\n",
    "        \"mental or behavioral dysfunction\",\n",
    "        \"geographic area\",\n",
    "        \"population group\",\n",
    "        \"biomedical or dental material\",\n",
    "        \"medical device\",\n",
    "        \"classification\",\n",
    "        \"regulation or law\",\n",
    "        \"health care activity\",\n",
    "        \"health care related organization\",\n",
    "        \"professional or occupational group\",\n",
    "        \"group\",\n",
    "        \"attribute\",\n",
    "        \"individual behavior\",\n",
    "    ],\n",
    "    # Specify the values you want to include in a list. Must be defined in medcat model.\n",
    "    \"Time_Value\": [\"Recent\", \"Past\"],\n",
    "    \"Time_Confidence\": 0.6,  # Specify the confidence threshold as a float\n",
    "    # Specify the values you want to include in a list\n",
    "    \"Presence_Value\": [\"True\"],\n",
    "    \"Presence_Confidence\": 0.6,  # Specify the confidence threshold as a float\n",
    "    # Specify the values you want to include in a list\n",
    "    \"Subject_Value\": [\"Patient\"],\n",
    "    \"Subject_Confidence\": 0.6,  # Specify the confidence threshold as a float\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\n",
    "    f\"new_project/current_pat_document_batches/{pat2vec_obj.all_patient_list[1]}.csv\"\n",
    ").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.post_processing_build_ipw_dataframe import build_ipw_dataframe\n",
    "\n",
    "build_ipw_dataframe(\n",
    "    annot_filter_arguments=annot_filter_arguments,\n",
    "    config_obj=pat2vec_obj.config_obj,\n",
    "    filter_codes=[\n",
    "        38341003,\n",
    "        274640006,\n",
    "        886731000000109,\n",
    "        268910001,\n",
    "        62315008,\n",
    "        55822004,\n",
    "        49727002,\n",
    "        22232009,\n",
    "    ],\n",
    "    mode=\"latest\",\n",
    "    include_mct=True,\n",
    "    include_textual_obs=False,\n",
    ")  # '62315008', '55822004', '268910001',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.post_processing_build_ipw_dataframe import build_ipw_dataframe\n",
    "\n",
    "build_ipw_dataframe(\n",
    "    annot_filter_arguments=annot_filter_arguments,\n",
    "    config_obj=pat2vec_obj.config_obj,\n",
    "    filter_codes=[\n",
    "        38341003,\n",
    "        274640006,\n",
    "        268910001,\n",
    "        62315008,\n",
    "        55822004,\n",
    "        49727002,\n",
    "        248153007,\n",
    "    ],\n",
    "    mode=\"earliest\",\n",
    "    include_mct=True,\n",
    "    include_textual_obs=False,\n",
    ")  # '62315008', '55822004', '268910001',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine and screen the patient client_idcode list for malformed entries\n",
    "\n",
    "from pat2vec.pat2vec_pat_list.get_patient_treatment_list import analyze_client_codes\n",
    "\n",
    "# valid_codes, invalid_codes, clusters = analyze_client_codes(pat2vec_obj.all_patient_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(treatment_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_docs = pd.read_csv(\"test_files/treatment_docs.csv\")\n",
    "# assert len(treatment_docs) == 23\n",
    "logger.info(f\"Length of treatment_docs is 23: {len(treatment_docs)==23}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert treatment_docs['basicobs_itemname_analysed'].iloc[21] == 'Parathyroid Hormone (PTH)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Body analysed (row 0): {treatment_docs['body_analysed'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"acrylic head\" in str(treatment_docs[\"body_analysed\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_example_annot = pd.read_csv(\n",
    "    \"new_project/current_pat_documents_annotations_batches/P0IFD0TV.csv\"\n",
    ")\n",
    "\n",
    "pat_example_annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert pat_example_annot['cui'].iloc[0] == 38341003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPW demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build IPW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_filter_arguments = {\n",
    "    \"acc\": 0.1,  # base concept accuracy\n",
    "    # umls list of types for medcat filter\n",
    "    \"types\": [\n",
    "        \"qualifier value\",\n",
    "        \"procedure\",\n",
    "        \"substance\",\n",
    "        \"finding\",\n",
    "        \"environment\",\n",
    "        \"disorder\",\n",
    "        \"observable entity\",\n",
    "    ],\n",
    "    # 'types': ['qualifier value', 'procedure', 'substance', 'finding', 'environment', 'disorder', 'observable entity', 'organism', 'phenomenon', 'anatomy', 'conceptual entity', 'physical object', 'intellectual product', 'occupation or discipline', 'mental or behavioral dysfunction', 'geographic area', 'population group', 'biomedical or dental material', 'medical device', 'classification', 'regulation or law', 'health care activity', 'health care related organization', 'professional or occupational group', 'group', 'attribute', 'individual behavior']\n",
    "    # Specify the values you want to include in a list. Must be defined in medcat model.\n",
    "    \"Time_Value\": [\"Recent\", \"Past\"],\n",
    "    \"Time_Confidence\": 0.1,  # Specify the confidence threshold as a float\n",
    "    # Specify the values you want to include in a list\n",
    "    \"Presence_Value\": [\"True\"],\n",
    "    \"Presence_Confidence\": 0.1,  # Specify the confidence threshold as a float\n",
    "    # Specify the values you want to include in a list\n",
    "    \"Subject_Value\": [\"Patient\"],\n",
    "    \"Subject_Confidence\": 0.1,  # Specify the confidence threshold as a float\n",
    "}\n",
    "\n",
    "pd.read_csv(\n",
    "    f\"new_project/current_pat_document_batches/{pat2vec_obj.all_patient_list[1]}.csv\"\n",
    ").head()\n",
    "len(pat2vec_obj.all_patient_list)\n",
    "pd.read_csv(\n",
    "    f\"new_project/current_pat_documents_annotations_batches/{pat2vec_obj.all_patient_list[1]}.csv\"\n",
    ").head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select two cui to simulate condition\n",
    "\n",
    "dfa_s = pd.read_csv(\"new_project/merged_batches/annots_mct_epr.csv\")\n",
    "\n",
    "dfa_s.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa_s[dfa_s[\"client_idcode\"] == \"V5LXO6QJ\"].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using these two cui codes as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "# Group the data so we have a set of CUIs for each client\n",
    "client_cui_map = dfa_s.groupby(\"client_idcode\")[\"cui\"].apply(set)\n",
    "\n",
    "# Create all unique pairs of CUIs\n",
    "all_cuis = pd.Series(list(itertools.chain.from_iterable(client_cui_map))).unique()\n",
    "pairs = itertools.combinations(all_cuis, 2)\n",
    "\n",
    "# Dictionary to map each pair to a list of clients that have it\n",
    "pair_to_clients = {}\n",
    "\n",
    "for cui1, cui2 in pairs:\n",
    "    current_pair = {cui1, cui2}\n",
    "    # Create a list of clients that have both CUIs\n",
    "    clients_with_pair = [\n",
    "        client_id\n",
    "        for client_id, cui_set in client_cui_map.items()\n",
    "        if current_pair.issubset(cui_set)\n",
    "    ]\n",
    "\n",
    "    # If the list is not empty, add it to our dictionary\n",
    "    if clients_with_pair:\n",
    "        pair_to_clients[(cui1, cui2)] = clients_with_pair\n",
    "\n",
    "# Find the pair with the most clients by checking the length of the lists\n",
    "most_common_pair = max(pair_to_clients, key=lambda pair: len(pair_to_clients[pair]))\n",
    "\n",
    "# Get the list of clients and the count for that most common pair\n",
    "clients_list = pair_to_clients[most_common_pair]\n",
    "max_count = len(clients_list)\n",
    "\n",
    "logger.info(\n",
    "    f\"Most common co-occurring pair: {most_common_pair} with {max_count} clients having both.\"\n",
    ")\n",
    "logger.info(\"Clients with this pair:\")\n",
    "# Log each client from the list\n",
    "logger.info(clients_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_A_pretty_name = dfa[dfa[\"cui\"] == int(most_common_pair[0])][\"pretty_name\"].iloc[\n",
    "    0\n",
    "]\n",
    "\n",
    "concept_B_pretty_name = dfa[dfa[\"cui\"] == int(most_common_pair[1])][\"pretty_name\"].iloc[\n",
    "    0\n",
    "]\n",
    "\n",
    "concept_A_pretty_name, concept_B_pretty_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_A_filter_codes = [int(most_common_pair[0])]\n",
    "concept_B_filter_codes = [int(most_common_pair[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_obj.verbosity = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the earliest occurrence of any CUI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa_s[\n",
    "    (dfa_s[\"client_idcode\"] == \"V5LXO6QJ\")\n",
    "    & (dfa_s[\"cui\"].isin(concept_A_filter_codes + concept_B_filter_codes))\n",
    "].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.post_processing_build_ipw_dataframe import build_ipw_dataframe\n",
    "\n",
    "file_path = \"ipw_dataframe.csv\"\n",
    "overwrite = True\n",
    "skip_ipw_build = False\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    if overwrite:\n",
    "        pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "        # n.b this needs filter annot arguments...\n",
    "        ipw_dataframe = build_ipw_dataframe(\n",
    "            annot_filter_arguments=annot_filter_arguments,\n",
    "            config_obj=pat2vec_obj.config_obj,\n",
    "            filter_codes=concept_A_filter_codes + concept_B_filter_codes,\n",
    "            mode=\"earliest\",\n",
    "            include_mct=False,\n",
    "            include_textual_obs=False,\n",
    "        )  # '62315008', '55822004', '268910001',\n",
    "        ipw_dataframe.to_csv(file_path)\n",
    "        ipw_dataframe\n",
    "        # Proceed with overwriting the file\n",
    "        logger.info(\"File exists and will be overwritten.\")\n",
    "    else:\n",
    "        # Skip or handle the existing file\n",
    "        ipw_dataframe = pd.read_csv(\"ipw_dataframe.csv\")\n",
    "        logger.info(\"File exists and will NOT be overwritten.\")\n",
    "else:\n",
    "    # File does not exist, safe to proceed\n",
    "    logger.info(\"File does not exist, safe to proceed.\")\n",
    "\n",
    "    pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "    # n.b this needs filter annot arguments...\n",
    "    ipw_dataframe = build_ipw_dataframe(\n",
    "        annot_filter_arguments=annot_filter_arguments,\n",
    "        config_obj=pat2vec_obj.config_obj,\n",
    "        filter_codes=concept_A_filter_codes + concept_B_filter_codes,\n",
    "        mode=\"earliest\",\n",
    "        include_mct=False,\n",
    "        include_textual_obs=False,\n",
    "    )  # '62315008', '55822004', '268910001',\n",
    "    ipw_dataframe.to_csv(file_path)\n",
    "    ipw_dataframe\n",
    "\n",
    "ipw_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additionally filter by only those who had both of the cui coocurring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pat2vec.util.post_processing import filter_annot_dataframe2\n",
    "\n",
    "annot_batch_file_path = \"new_project/merged_batches/annots_mct_epr.csv\"\n",
    "\n",
    "# Assume concept_A_filter_codes and concept_B_filter_codes are defined as sets for efficiency\n",
    "concept_A_filter_codes_set = set(concept_A_filter_codes)\n",
    "concept_B_filter_codes_set = set(concept_B_filter_codes)\n",
    "# Assume annot_filter_arguments is defined\n",
    "\n",
    "if not skip_ipw_build:\n",
    "    # Initialize two empty sets to store client IDs for each condition\n",
    "    clients_with_concept_A = set()\n",
    "    clients_with_concept_B = set()\n",
    "\n",
    "    # Process the file in chunks \n",
    "    for chunk in pd.read_csv(annot_batch_file_path, chunksize=100000):\n",
    "\n",
    "        # 1. Filter annotations by earlier annotation filter arguments first\n",
    "        chunk = filter_annot_dataframe2(chunk, annot_filter_arguments)\n",
    "\n",
    "        # 2. Find clients in this chunk with a Concept A code and update the set\n",
    "        A_in_chunk = chunk[chunk[\"cui\"].isin(concept_A_filter_codes)][\n",
    "            \"client_idcode\"\n",
    "        ].unique()\n",
    "        clients_with_concept_A.update(A_in_chunk)\n",
    "\n",
    "        # 3. Find clients in this chunk with a Concept B code and update the set\n",
    "        B_in_chunk = chunk[chunk[\"cui\"].isin(concept_B_filter_codes)][\n",
    "            \"client_idcode\"\n",
    "        ].unique()\n",
    "        clients_with_concept_B.update(B_in_chunk)\n",
    "\n",
    "    # 4. The final list is the intersection of the two sets \n",
    "    true_clients = list(clients_with_concept_A.intersection(clients_with_concept_B))\n",
    "\n",
    "    logger.info(f\"Found {len(true_clients)} patients with both concept A and concept B\")\n",
    "    logger.info(f\"Clients with both concepts: {true_clients}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally filter the IPW by the true clients with concept_A_filter_codes and concept_B_filter_codes\n",
    "if not skip_ipw_build:\n",
    "    ipw_dataframe = ipw_dataframe[ipw_dataframe[\"client_idcode\"].isin(true_clients)]\n",
    "\n",
    "    ipw_dataframe\n",
    "if not skip_ipw_build:\n",
    "    ipw_dataframe.reset_index(drop=True, inplace=True)\n",
    "if not skip_ipw_build:\n",
    "    ipw_dataframe.to_csv(\"ipw_dataframe.csv\")\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "if not skip_ipw_build:\n",
    "    # Convert to datetime and ensure all values are timezone-aware in UTC\n",
    "    ipw_dataframe[\"updatetime\"] = pd.to_datetime(\n",
    "        ipw_dataframe[\"updatetime\"], utc=True  # format='ISO8601',\n",
    "    )\n",
    "\n",
    "    # We need to compute individual start and end dates for each patient in the IPW dataframe.\n",
    "    # We will use the 'updatetime' column as the basis for this calculation.\n",
    "    # We will create two new columns: 'updatetime_offset' and 'updatetime_end_date'.\n",
    "    # These will be used to create the patient_dict for pat2vec processing.\n",
    "    # We add a buffer of 3 months to the 'updatetime' to create 'updatetime_offset' to avoid information leakage.\n",
    "\n",
    "    # add 3 months using pd.DateOffset, this is a buffer between the first mention of the concept and our new individual patient start time/ time window.\n",
    "    ipw_dataframe[\"updatetime_offset\"] = ipw_dataframe[\"updatetime\"] + pd.DateOffset(\n",
    "        months=3\n",
    "    )\n",
    "\n",
    "    ipw_dataframe[\"updatetime_offset\"] = pd.to_datetime(\n",
    "        ipw_dataframe[\"updatetime_offset\"], format=\"ISO8601\", utc=True\n",
    "    )\n",
    "\n",
    "    # Now add the time delta to create the individual patient window end date from the offset date\n",
    "\n",
    "    ipw_dataframe[\"updatetime_end_date\"] = ipw_dataframe[\"updatetime_offset\"].apply(\n",
    "        lambda dt: dt + pat2vec_obj.config_obj.time_window_interval_delta\n",
    "    )\n",
    "\n",
    "    ipw_dataframe.to_csv(\"ipw_dataframe.csv\")\n",
    "\n",
    "    ipw_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipw_dataframe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.config_pat2vec import config_class\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from pat2vec.util.post_processing_process_csv_files import process_csv_files\n",
    "from pat2vec.util.post_processing import extract_datetime_to_column\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "# Configuration dictionary for main options in a medical application\n",
    "main_options_dict = {\n",
    "    # Enable demographic information (Ethnicity mapped to UK census, age, death)\n",
    "    \"demo\": True,\n",
    "    \"bmi\": True,  # Enable BMI (Body Mass Index) tracking\n",
    "    \"bloods\": True,  # Enable blood-related information\n",
    "    \"drugs\": True,  # Enable drug-related information\n",
    "    \"diagnostics\": True,  # Enable diagnostic information\n",
    "    \"core_02\": True,  # Enable core_02 information\n",
    "    \"bed\": True,  # Enable bed n information\n",
    "    \"vte_status\": True,  # Enable VTE () status tracking\n",
    "    \"hosp_site\": True,  # Enable hospital site information\n",
    "    \"core_resus\": True,  # Enable core resuscitation information\n",
    "    \"news\": True,  # Enable NEWS (National Early Warning Score) tracking\n",
    "    \"smoking\": True,  # Enable smoking-related information\n",
    "    \"annotations\": True,  # Enable EPR annotations\n",
    "    # Enable MRC (Additional clinical note observations index) annotations\n",
    "    \"annotations_mrc\": True,\n",
    "    # Enable or disable negated presence annotations\n",
    "    \"negated_presence_annotations\": False,\n",
    "    \"appointments\": True,  # Enable appointments\n",
    "    \"annotations_reports\": False,  # Enable reports\n",
    "    \"textual_obs\": True,  # Enable textual observations (basic_observations index)\n",
    "}\n",
    "\n",
    "\n",
    "annot_filter_arguments = {\n",
    "    \"acc\": 0.8,  # base concept accuracy\n",
    "    # umls list of types for medcat filter\n",
    "    \"types\": [\n",
    "        \"qualifier value\",\n",
    "        \"procedure\",\n",
    "        \"substance\",\n",
    "        \"finding\",\n",
    "        \"environment\",\n",
    "        \"disorder\",\n",
    "        \"observable entity\",\n",
    "    ],\n",
    "    # 'types': ['qualifier value', 'procedure', 'substance', 'finding', 'environment', 'disorder', 'observable entity', 'organism', 'phenomenon', 'anatomy', 'conceptual entity', 'physical object', 'intellectual product', 'occupation or discipline', 'mental or behavioral dysfunction', 'geographic area', 'population group', 'biomedical or dental material', 'medical device', 'classification', 'regulation or law', 'health care activity', 'health care related organization', 'professional or occupational group', 'group', 'attribute', 'individual behavior']\n",
    "    # Specify the values you want to include in a list. Must be defined in medcat model.\n",
    "    \"Time_Value\": [\"Recent\", \"Past\"],\n",
    "    \"Time_Confidence\": 0.8,  # Specify the confidence threshold as a float\n",
    "    # Specify the values you want to include in a list\n",
    "    \"Presence_Value\": [\"True\"],\n",
    "    \"Presence_Confidence\": 0.8,  # Specify the confidence threshold as a float\n",
    "    # Specify the values you want to include in a list\n",
    "    \"Subject_Value\": [\"Patient\"],\n",
    "    \"Subject_Confidence\": 0.8,  # Specify the confidence threshold as a float\n",
    "}\n",
    "\n",
    "# Filter data batches by terms before processing.\n",
    "\n",
    "epr_docs_term_regex: Optional[Union[str, None]] = None\n",
    "mct_docs_term_regex: Optional[Union[str, None]] = None\n",
    "\n",
    "# Example bloods_filter_term_list: Optional[Union[List[str], None]] = ['wbc']\n",
    "bloods_filter_term_list: Optional[Union[List[str], None]] = None\n",
    "\n",
    "# Example mct_docs_document_type_filter_list: Optional[Union[List[str], None]] = ['KHMDC Integrated report']\n",
    "\n",
    "mct_docs_document_type_filter_list: Optional[Union[List[str], None]] = None\n",
    "epr_docs_document_type_filter_list: Optional[Union[List[str], None]] = None\n",
    "\n",
    "data_type_filter_dict: Dict[str, any] = {\n",
    "    \"filter_term_lists\": {\n",
    "        \"epr_docs\": epr_docs_document_type_filter_list,\n",
    "        \"mct_docs\": mct_docs_document_type_filter_list,\n",
    "        \"bloods\": bloods_filter_term_list,\n",
    "    },\n",
    "    \"epr_docs_term_regex\": epr_docs_term_regex,\n",
    "    \"mct_docs_term_regex\": mct_docs_term_regex,\n",
    "}\n",
    "\n",
    "# Example date settings:\n",
    "# start_date=(datetime(2020, 1, 1)) Start date for processing\n",
    "\n",
    "# Define the length of the time window, example 1 year and 15 days, only data within this window will be processed.\n",
    "# years=1,      # Number of years to add to the start date\n",
    "# months=0,  # Number of months to add to the start date\n",
    "# days=15,  # Number of days to add to the start date\n",
    "\n",
    "# Define the interval between time windows. Example 1 year. Each vector/row output will be based on this interval.\n",
    "# time_window_interval_delta = relativedelta(years=1)\n",
    "\n",
    "# lookback = True #This determines the direction of the time length window. True = backward, False = forward. Our time window (+1 years, 15 days) is therefore 2020, 1, 1 - 2021, 1, 15.\n",
    "\n",
    "# IPW settings:\n",
    "\n",
    "# Init config obj\n",
    "\n",
    "# Creating a configuration object for a specific task or project\n",
    "config_obj = config_class(\n",
    "    remote_dump=False,  # Flag for remote data dumping. partially deprecated.\n",
    "    suffix=\"\",  # Suffix for file names\n",
    "    # Filename for treatment documentation\n",
    "    treatment_doc_filename=\"treatment_docs.csv\",\n",
    "    treatment_control_ratio_n=1,  # Ratio for treatment to control\n",
    "    # Project name. patient data batches and vectors stored here.\n",
    "    proj_name=\"new_project_ipw\",\n",
    "    current_path_dir=\"\",  # Current path directory\n",
    "    main_options=main_options_dict,  # Dictionary for main options\n",
    "    start_date=(datetime(1995, 1, 1)),  # Starting date for processing\n",
    "    # Number of years to add to the start date. Set the duration of the time window. Window is defined as the start date + years/months/days set here.\n",
    "    years=30,\n",
    "    months=0,  # Number of months to add to the start date\n",
    "    days=0,  # Number of days to add to the start date\n",
    "    batch_mode=True,  # Flag for batch processing mode. only functioning mode.\n",
    "    store_annot=True,  # Flag to store annotations. partially deprecated.\n",
    "    share_sftp=True,  # Flag for sharing via SFTP. partially deprecated\n",
    "    multi_process=False,  # Flag for multi-process execution. deprecated.\n",
    "    # annot_first=False,  # Flag for annotation priority. deprecated.\n",
    "    # Flag for stripping lists, will check for completed patients before starting to avoid redundancy.\n",
    "    strip_list=True,\n",
    "    verbosity=0,  # Verbosity level 0-9 printing debug messages\n",
    "    random_seed_val=random_seed_value,  # Random seed value for reproducibility of controls.\n",
    "    testing=True,  # Flag for testing mode\n",
    "    dummy_medcat_model=True,  # Flag for dummy MedCAT model, used if testing == True\n",
    "    # Flag for using controls. #will add desired ratio of controls at random from global pool.\n",
    "    use_controls=False,\n",
    "    # Flag for MedCAT processing. #will load medcat into memory and use for annotating.\n",
    "    medcat=False,\n",
    "    # Current timestamp as the start time for logging and progress bar\n",
    "    start_time=datetime.now(),\n",
    "    # Column name for patient ID, auto will try to find it. Example \"client_idcode\"\n",
    "    patient_id_column_name=\"client_idcode\",\n",
    "    annot_filter_options=annot_filter_arguments,  # Annotation filtering options\n",
    "    # Global start year. #set the limits of the time window data can be drawn from. Start should not precede start date set above.\n",
    "    global_start_year=1995,  # Global dates are overwritten by individual patient windows to match patient window. # Ensure that global start year/month/day is before end year/month/day\n",
    "    global_start_month=1,  # Global start month\n",
    "    global_end_year=2023,  # Global end year\n",
    "    global_end_month=1,  # Global end month\n",
    "    global_start_day=1,\n",
    "    global_end_day=1,\n",
    "    individual_patient_window=True,\n",
    "    individual_patient_window_df=pd.read_csv(\"ipw_dataframe.csv\"),\n",
    "    individual_patient_window_start_column_name=\"updatetime\",  # _offset , this will look for your start column name + '_offset'\n",
    "    individual_patient_id_column_name=\"client_idcode\",\n",
    "    individual_patient_window_controls_method=\"full\",\n",
    "    shuffle_pat_list=False,  # Flag for shuffling patient list\n",
    "    time_window_interval_delta=relativedelta(\n",
    "        years=31\n",
    "    ),  # specify the time window to collapse each feature vector into, years=1 is one vector per year within the global time window\n",
    "    split_clinical_notes=True,  # will split clinical notes by date and treat as individual documents with extracted dates. Requires note splitter module.\n",
    "    lookback=False,  # when calculating individual patient window from table of start dates, will calculate backwards in time if true. Else Forwards. When calculating from global start date, will calculate backwards or forwards respectively.\n",
    "    add_icd10=False,  # append icd 10 codes to annot batches. Can be found under current_pat_documents_annotations/%client_idcode%.csv.\n",
    "    add_opc4s=False,  # needs icd10 true also. Can be found under current_pat_documents_annotations/%client_idcode%.csv\n",
    "    override_medcat_model_path=path_to_medcat_model_pack,  # Force medcat model path, if None uses defaults for env. #Can be set in paths.py with medcat_path = %path to medcat model pack.zip\"\n",
    "    data_type_filter_dict=None,  # Dictionary for data type filter, see examples above.\n",
    "    filter_split_notes=True,  # If enabled, will reapply global time window filter post clinical note splitting. Recommended to enable if split notes enabled.\n",
    "    calculate_vectors=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_obj.individual_patient_window_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj = main(\n",
    "    cogstack=True,\n",
    "    use_filter=False,\n",
    "    json_filter_path=None,\n",
    "    random_seed_val=42,\n",
    "    hostname=None,\n",
    "    config_obj=config_obj,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_obj.patient_dict  # These are the individual patient time windows for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj.config_obj.date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_obj.verbosity = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj.config_obj.date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj.pat_maker(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the maximum number of retries\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "# Iterate through the patient list starting from index 0\n",
    "for i in tqdm(range(0, len(pat2vec_obj.all_patient_list))):\n",
    "    retries = 0\n",
    "    success = False\n",
    "\n",
    "    while retries < MAX_RETRIES and not success:\n",
    "        try:\n",
    "            # Try to process the patient\n",
    "            pat2vec_obj.pat_maker(i)\n",
    "            success = True  # Mark as successful if no exception is raised\n",
    "\n",
    "        except KeyError as e:\n",
    "            # Handle specific exception\n",
    "            logger.warning(\n",
    "                f\"KeyError at index {i} for patient {pat2vec_obj.all_patient_list[i]}: {e}. Retrying after removal...\"\n",
    "            )\n",
    "            remove_file_from_paths(pat2vec_obj.all_patient_list[i])\n",
    "            retries += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            # Handle generic exceptions\n",
    "            logger.error(\n",
    "                f\"Exception at index {i} for patient {pat2vec_obj.all_patient_list[i]}: {e}. Skipping this patient...\"\n",
    "            )\n",
    "            raise e\n",
    "            break  # Break the retry loop for non-retryable exceptions\n",
    "\n",
    "        finally:\n",
    "            pat2vec_obj.t.update(1)  # Update progress\n",
    "\n",
    "    if not success:\n",
    "        logger.error(\n",
    "            f\"Failed to process index {i} for patient {pat2vec_obj.all_patient_list[i]} after {MAX_RETRIES} retries.\"\n",
    "        )\n",
    "\n",
    "pat2vec_obj.t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj.config_obj.date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = f\"{pat2vec_obj.proj_name}/current_pat_lines_parts\"  # Patient vectors are stored individually in this directory.\n",
    "output_csv_file = \"output_file\"\n",
    "\n",
    "# Specify the directory where you want to create the file\n",
    "directory = pat2vec_obj.proj_name + \"/output_directory\"\n",
    "\n",
    "# Check if the directory exists, if not, create it\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# We will join the individual patient vectors into a single output file. This is useful for filtering.\n",
    "output_csv_file_filename = process_csv_files(\n",
    "    input_directory,\n",
    "    out_folder=directory,\n",
    "    output_filename_suffix=output_csv_file,\n",
    "    part_size=336,\n",
    ")\n",
    "df = pd.read_csv(output_csv_file_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.post_processing import save_missing_values_pickle\n",
    "\n",
    "# Save with the same name as the final imputed dataset\n",
    "save_missing_values_pickle(df, \"my_project_outputfile_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"client_idcode\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"client_idcode\"] == \"V1IBLJH7\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"client_idcode\"] == list(pat2vec_obj.config_obj.patient_dict.keys())[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"extracted_datetime_stamp\"] = extract_datetime_to_column(df)[\n",
    "    \"extracted_datetime_stamp\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_processed = list(pat2vec_obj.config_obj.patient_dict.keys())\n",
    "logger.info(f\"Number of patients processed: {len(patients_processed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter df by patients_processed list on client_idcode\n",
    "\n",
    "df_filtered = df[df[\"client_idcode\"].isin(patients_processed)]\n",
    "\n",
    "df_filtered[[\"client_idcode\", \"extracted_datetime_stamp\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj.config_obj.patient_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Create DataFrame and Find Earliest Date\n",
    "df_dict = pd.DataFrame.from_dict(\n",
    "    pat2vec_obj.config_obj.patient_dict, orient=\"index\", columns=[\"date1\", \"date2\"]\n",
    ")\n",
    "df_dict.index.name = \"patient_id\"\n",
    "df_dict.reset_index(inplace=True)\n",
    "date1_ts = pd.to_datetime(df_dict[\"date1\"])\n",
    "date2_ts = pd.to_datetime(df_dict[\"date2\"])\n",
    "df_dict[\"earliest_date_to_check\"] = np.minimum(date1_ts, date2_ts)\n",
    "\n",
    "# 2. Handle Duplicates in the Main DataFrame\n",
    "if df_filtered[\"client_idcode\"].duplicated().any():\n",
    "    logger.warning(\n",
    "        f\"Found and dropped {df_filtered.duplicated(subset=['client_idcode']).sum()} duplicate patient IDs.\"\n",
    "    )\n",
    "    df_filtered_unique = df_filtered.drop_duplicates(\n",
    "        subset=[\"client_idcode\"], keep=\"first\"\n",
    "    )\n",
    "else:\n",
    "    df_filtered_unique = df_filtered\n",
    "\n",
    "# --- NEW: Diagnostic Checks ---\n",
    "logger.info(\"\\n## Diagnostic Info\")\n",
    "logger.info(\"--------------------------------------------------\")\n",
    "\n",
    "# Check if the data sources are empty\n",
    "logger.info(f\"1. Size of data from dictionary: {len(df_dict)} rows\")\n",
    "logger.info(f\"2. Size of data from df_filtered: {len(df_filtered_unique)} rows\")\n",
    "\n",
    "if len(df_dict) > 0 and len(df_filtered_unique) > 0:\n",
    "    # Check the data types of the keys\n",
    "    logger.info(\n",
    "        f\"3. Data type of 'patient_id' (from dict): {df_dict['patient_id'].dtype}\"\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"4. Data type of 'client_idcode' (from df): {df_filtered_unique['client_idcode'].dtype}\"\n",
    "    )\n",
    "\n",
    "    # Show a sample of the keys to visually inspect for whitespace/casing\n",
    "    logger.info(\n",
    "        f\"5. Sample keys from dictionary: {df_dict['patient_id'].head(3).to_list()}\"\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"6. Sample keys from df_filtered: {df_filtered_unique['client_idcode'].head(3).to_list()}\"\n",
    "    )\n",
    "\n",
    "    # Programmatically find the exact number of overlapping IDs\n",
    "    set_dict = set(df_dict[\"patient_id\"].astype(str).str.strip())\n",
    "    set_df = set(df_filtered_unique[\"client_idcode\"].astype(str).str.strip())\n",
    "    overlap = set_dict.intersection(set_df)\n",
    "    logger.info(\n",
    "        f\"7. Found {len(overlap)} common IDs between the two sources after cleaning whitespace.\"\n",
    "    )\n",
    "    if len(overlap) < 5 and len(overlap) > 0:\n",
    "        logger.info(f\"   -> Common IDs are: {list(overlap)}\")\n",
    "else:\n",
    "    logger.warning(\"One or both data sources are empty. Cannot perform merge.\")\n",
    "\n",
    "logger.info(\"--------------------------------------------------\")\n",
    "\n",
    "\n",
    "merged_df = pd.merge(\n",
    "    left=df_dict,\n",
    "    right=df_filtered_unique,\n",
    "    left_on=\"patient_id\",\n",
    "    right_on=\"client_idcode\",\n",
    "    how=\"inner\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj.all_patient_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_obj.individual_patient_window_df[\n",
    "    config_obj.individual_patient_window_df[\"client_idcode\"]\n",
    "    == pat2vec_obj.all_patient_list[0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pat2vec_obj.all_patient_list), len(pat2vec_obj.config_obj.patient_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"client_idcode\"] == pat2vec_obj.all_patient_list[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Restrict to only clients in df\n",
    "dfa_filtered = dfa[dfa[\"client_idcode\"].isin(df[\"client_idcode\"])]\n",
    "\n",
    "# cui  set of client_idcodes that have it\n",
    "cui_to_clients = dfa_filtered.groupby(\"cui\")[\"client_idcode\"].apply(set).to_dict()\n",
    "\n",
    "# cuis present for some but not all df clients\n",
    "eligible_cuis = [\n",
    "    cui\n",
    "    for cui, clients in cui_to_clients.items()\n",
    "    if 0 < len(clients) < df[\"client_idcode\"].nunique()\n",
    "]\n",
    "\n",
    "# Randomly pick one\n",
    "random_cui = random.choice(eligible_cuis) if eligible_cuis else None\n",
    "# Which client_idcodes have that random_cui\n",
    "clients_with_random_cui = set(dfa.loc[dfa[\"cui\"] == random_cui, \"client_idcode\"])\n",
    "\n",
    "# Flag in df\n",
    "df[\"outcome_var_1\"] = df[\"client_idcode\"].isin(clients_with_random_cui).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"outcome_var_1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.impute_data_for_pipe import mean_impute_dataframe\n",
    "\n",
    "\n",
    "df_imputed = mean_impute_dataframe(df, \"outcome_var_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pat2vec_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
