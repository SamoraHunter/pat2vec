{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `example_usage` script handles the end-to-end workflow for extracting and transforming patient data. The process is divided into two main sections: defining the patient cohort and performing the data extraction/transformation.\n",
    "\n",
    "## 1. Define Cohort (Optional)\n",
    "The first section of the script is responsible for identifying the cohort—the specific group of patients whose data you wish to process.\n",
    "\n",
    "**When to run this step:**\n",
    "*   You do not have a pre-existing list of patients.\n",
    "*   You need to query the datalake to generate a new list of Patient Identifiers based on specific criteria (e.g., diagnosis codes, date ranges).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from elasticsearch.exceptions import AuthenticationException\n",
    "\n",
    "# Fix the random seed for reproducibility in unit testing\n",
    "\n",
    "random_seed_value = 42\n",
    "\n",
    "np.random.seed(random_seed_value)\n",
    "\n",
    "random.seed(random_seed_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Print the current working directory\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "# 2. Print Python's sys.path\n",
    "print(\"Python Path:\", sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove dir\n",
    "clear_previous_outputs = True\n",
    "\n",
    "if(clear_previous_outputs):\n",
    "\n",
    "    shutil.rmtree('new_project', ignore_errors=True)\n",
    "\n",
    "    shutil.rmtree('new_project_ipw', ignore_errors=True)\n",
    "\n",
    "    shutil.rmtree('treatment_doc_extract', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure dependencies are on path\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Define relative paths from the current working directory\n",
    "path_to_medcat_model_pack = os.path.abspath(os.path.join(current_dir, '..', '..', 'medcat_models', 'medcat_model_pack_422d1d38fc58f158.zip'))\n",
    "\n",
    "path_to_snomed_ct_file = os.path.abspath(os.path.join(current_dir, '..', '..', 'snomed', 'SnomedCT_InternationalRF2_PRODUCTION_20231101T120000Z', 'SnomedCT_InternationalRF2_PRODUCTION_20231101T120000Z', 'Full', 'Terminology', 'sct2_StatedRelationship_Full_INT_20231101.txt'))\n",
    "\n",
    " # Define the relative path\n",
    "path_to_gloabl_files = '../../'\n",
    "\n",
    "additional_path_to_pat2vec = 'pat2vec'\n",
    "\n",
    "additional_path_to_pat2vec = os.path.abspath(os.path.join(path_to_gloabl_files, additional_path_to_pat2vec))\n",
    "\n",
    "# Get the absolute path of the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Combine the current directory with the relative path\n",
    "absolute_path = os.path.abspath(os.path.join(current_dir, path_to_gloabl_files))\n",
    "\n",
    "# Usage examples\n",
    "print(path_to_medcat_model_pack)\n",
    "print(path_to_snomed_ct_file)\n",
    "print(path_to_gloabl_files)\n",
    "print(additional_path_to_pat2vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, path_to_gloabl_files)\n",
    "sys.path.insert(0, additional_path_to_pat2vec)\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Add the grandparent directory of the current directory to the Python path\n",
    "grandparent_dir = os.path.dirname(parent_dir)\n",
    "sys.path.append(grandparent_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.logger_setup import setup_logger\n",
    "\n",
    "# Get the logger\n",
    "logger = setup_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get treatment_docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.config_pat2vec import config_class\n",
    "\n",
    "config_obj = config_class(medcat=False, # Load medcat, ensure model pack is in gloabl_files/medcat_models/ ..examplemodelpack.zip\n",
    "                          override_medcat_model_path = path_to_medcat_model_pack,\n",
    "                          proj_name='treatment_doc_extract',\n",
    "                          verbosity=0,\n",
    "                          global_start_year=1995, # Set the start date, this will extract data between these dates.\n",
    "                          global_end_year=2024,\n",
    "                          global_start_month=1,\n",
    "                          global_end_month=12,\n",
    "                          global_start_day=1,\n",
    "                          global_end_day=31,\n",
    "                          lookback=False, # Set to True if you want to look back at the previous year and month\n",
    "                          testing=True, # Set to True if you want to run in testing mode, this will use dummy data for testing.\n",
    "                          credentials_path='../util/credentials.py'\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.main_pat2vec import main\n",
    "\n",
    "pat2vec_obj = main( cogstack=True, use_filter=False,\n",
    "             json_filter_path = None, random_seed_val=random_seed_value, \n",
    "             hostname =None, config_obj= config_obj, ) # initialize the pat2vec object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pat2vec_obj.config_obj.testing:\n",
    "    print(\"Testing mode is enabled, skipping authentication check.\")\n",
    "else:\n",
    "    # Check if the Elasticsearch client is authenticated # advise user to check credentials\n",
    "    try:\n",
    "        pat2vec_obj.cs.elastic.info()\n",
    "    except AuthenticationException as e:\n",
    "        print(f\"Authentication failed: {e.info['error']['reason']}\")\n",
    "        print(\"Please check your Elasticsearch credentials in the configuration file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while checking Elasticsearch authentication: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL example illustration of how we can find related snomed codes from a code of interest. \n",
    "# Requires installing snomed_methods and setting the path.\n",
    "# This may be useful to find all the related codes for a given snomed CT concept.\n",
    "\n",
    "snomed_example = False\n",
    "\n",
    "if(snomed_example):\n",
    "    \n",
    "    from snomed_methods import snomed_methods_v1\n",
    "\n",
    "    path_to_sct2 = path_to_snomed_ct_file\n",
    "\n",
    "    medcat_path = path_to_medcat_model_pack\n",
    "\n",
    "    snomed_relations_obj = snomed_methods_v1.snomed_relations(medcat=True, snomed_rf2_full_path=path_to_sct2,\n",
    "                                                            medcat_path = medcat_path)\n",
    "\n",
    "    outcome_variable_cui_for_filter = '109989006'  # myeloma\n",
    "\n",
    "    print(outcome_variable_cui_for_filter)\n",
    "\n",
    "    filter_root_cui = outcome_variable_cui_for_filter\n",
    "    print(filter_root_cui)\n",
    "\n",
    "    retrieved_codes_snomed_tree, retrieved_names_snomed_tree = snomed_relations_obj.recursive_code_expansion(filter_root_cui, n_recursion = 3, debug=False)\n",
    "\n",
    "    print(retrieved_codes_snomed_tree[0:5], len(retrieved_codes_snomed_tree), len(retrieved_names_snomed_tree))\n",
    "\n",
    "\n",
    "    retrieved_codes_medcat_cdb, retrieved_names_medcat_cdb = (\n",
    "    snomed_relations_obj.get_medcat_cdb_most_similar(\n",
    "        filter_root_cui, context_type=\"xxxlong\", type_id_filter=[], topn=50\n",
    "    )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add terms to search the document indicies for. \n",
    "# In the next cell each term will be searched across the document indicies with fuzzy matching.\n",
    "# For more complex cohort searching it's recommended to expose the elastic api directly.\n",
    "\n",
    "term_list = ['myeloma', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.pre_processing import get_treatment_docs_by_iterative_multi_term_cohort_searcher_no_terms_fuzzy\n",
    "\n",
    "# Example getting a patient cohort by the presence of terms in their clinical documents\n",
    "\n",
    "# We start by extracting the documents across textual document sources with fuzzy string matching \n",
    "\n",
    "treatment_docs = get_treatment_docs_by_iterative_multi_term_cohort_searcher_no_terms_fuzzy(pat2vec_obj=pat2vec_obj,\n",
    "                                                                          term_list=term_list, # List of terms to search for\n",
    "                                                                          overwrite=True, # overwrite existing treatment_docs.csv else append results\n",
    "                                                                          append=False, # Append results to existing treatment_docs.csv\n",
    "                                                                          verbose=9, # Adjust verbosity for logging\n",
    "                                                                          mct=True, # Include clinical notes text sources, this will search an additional document index\n",
    "                                                                          textual_obs=True, # Include observations index text sources, this will search an additional document index\n",
    "                                                                          additional_filters=None, # Add additional filters to the search such as document type. \n",
    "                                                                          all_fields=False # Return all fields from indicies instead of just a typical subset. \n",
    "                                                                          )\n",
    "\n",
    "treatment_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example get cohort by drug treatment for cross reference etc\n",
    "\n",
    "# Example, I want to get a cohort of patients who have drug orders to check against their diagnosis status from the previous step.\n",
    "\n",
    "from pat2vec.util.pre_get_drug_treatment_docs import iterative_drug_treatment_search\n",
    "import pandas as pd\n",
    "\n",
    "retrieve_cohort_by_drug_treatment = False\n",
    "\n",
    "if retrieve_cohort_by_drug_treatment:\n",
    "\n",
    "    search_terms_list = [\"asprin\", \"ibuprofen\", \"Emtricitabine\", \"Mepacrine\"]\n",
    "    output_file_path = \"drug_treatment_records.csv\"\n",
    "\n",
    "    iterative_drug_treatment_search(\n",
    "        pat2vec_obj=pat2vec_obj,\n",
    "        search_terms=search_terms_list,\n",
    "        output_file_path=output_file_path,\n",
    "        verbose=5,  # Adjust verbosity for logging\n",
    "        drop_duplicates=True, # Search terms can produce duplicates, remove by order guid.\n",
    "        overwrite = True # Overwrite initial output file\n",
    "    )\n",
    "    \n",
    "    # Load the csv file\n",
    "    df_drug_treatment_cohort = pd.read_csv(output_file_path)\n",
    "    df_drug_treatment_cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Data and Create Patient Vectors\n",
    "\n",
    "This is the main execution block of the workflow. If you already have your cohort defined (e.g., a file named `treatment_docs.csv`), you should skip Section 1 and start directly here.\n",
    "\n",
    "This step iterates through the patient identifiers provided in your cohort file, extracts their relevant clinical data, and transforms that data into patient vectors (the final dataset) based on your configuration settings.\n",
    "\n",
    "### Code Structure & Explanation\n",
    "\n",
    "The code provided in the following cells performs three main actions: **Configuration**, **Initialization**, and **Execution**.\n",
    "\n",
    "#### 1. Configuration (`config_obj`)\n",
    "The first cell defines a highly detailed configuration object. This tells the system *what* to extract and *how* to structure it. Key parameters include:\n",
    "\n",
    "*   **`main_options_dict`**: Acts as a switchboard for data domains. Setting keys like `'bloods': True` or `'drugs': True` tells the extractor to query those specific tables.\n",
    "*   **`annot_filter_arguments`**: Controls the Natural Language Processing (MedCAT). It filters extracted concepts based on confidence scores (e.g., `acc: 0.8`) and semantic types (e.g., only keeping 'disorders' or 'substances').\n",
    "*   **Time Windows**:\n",
    "    *   `start_date` & `years`: Defines the total observation window for a patient (e.g., 30 years starting from 1995).\n",
    "    *   `time_window_interval_delta`: Defines the resolution of the output vectors. If set to `years=1`, the patient's data is aggregated into one vector per year.\n",
    "*   **`treatment_doc_filename`**: The path to your cohort file (the list of patients to process).\n",
    "\n",
    "#### 2. Initialization (`pat2vec_obj`)\n",
    "The second cell instantiates the `main()` class with your configuration. This prepares the environment, sets up file paths, and loads necessary models (like MedCAT) if enabled.\n",
    "\n",
    "#### 3. The Processing Loop (`pat_maker`)\n",
    "The third cell contains the execution logic. It iterates through every patient in the cohort list (`pat2vec_obj.all_patient_list`) and calls `pat_maker(i)`.\n",
    "\n",
    "*   **Robustness**: It includes a `try-except` block with a retry mechanism (`MAX_RETRIES`).\n",
    "*   **Error Handling**: It specifically looks for `KeyError`, which often indicates a file access collision or data inconsistency. If found, it attempts to clean up (`remove_file_from_paths`) and retry.\n",
    "\n",
    "### How to Run\n",
    "1.  **Prerequisites**: Ensure your cohort file exists at `test_files/treatment_docs.csv` (or update the path in the configuration cell) and that the `pat2vec` library is installed.\n",
    "2.  **Configure**: Update the variables in the **Configuration** cell to match your desired data sources and time windows.\n",
    "3.  **Execute**: Run the following cells in order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.config_pat2vec import config_class\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from pat2vec.util.post_processing_process_csv_files import process_csv_files \n",
    "from pat2vec.util.post_processing import extract_datetime_to_column\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "# Configuration dictionary for main options in pat2vec\n",
    "main_options_dict = {\n",
    "    \n",
    "    'demo': True, # Enable demographic information (Ethnicity mapped to UK census categories, age, death).\n",
    "    'bmi': True,  # Enable BMI (Body Mass Index) information.\n",
    "    'bloods': True,  # Enable blood-related information\n",
    "    'drugs': True,  # Enable drug-related information\n",
    "    'diagnostics': True,  # Enable diagnostic information\n",
    "\n",
    "    'core_02': True,  # Enable core_02 information\n",
    "    'bed': True,  # Enable bed n information\n",
    "    'vte_status': True,  # Enable VTE () status information\n",
    "    'hosp_site': True,  # Enable hospital site information\n",
    "    'core_resus': True,  # Enable core resuscitation information\n",
    "    'news': True,  # Enable NEWS (National Early Warning Score) information\n",
    "\n",
    "    'smoking': True,  # Enable smoking-related information\n",
    "    'annotations': True,  # Enable EPR documents annotations via MedCat\n",
    "    'annotations_mrc': True,# Enable MRC (Additional clinical note observations index) annotations via MedCat\n",
    "    'negated_presence_annotations': False, # Enable or disable negated presence annotations\n",
    "    'appointments': False,  # Enable appointments information\n",
    "    'annotations_reports': False,  # Enable reports information\n",
    "    'textual_obs': False,  # Enable textual observations (basic_observations index) annotations via MedCat\n",
    "}\n",
    "\n",
    "# Configuration dictionary for annotation filtering, only base annotations meeting this threshold will be included.\n",
    "annot_filter_arguments = {\n",
    "    'acc': 0.8,  # base concept accuracy\n",
    "    'types': ['qualifier value', 'procedure', 'substance', 'finding', 'environment', 'disorder', 'observable entity'], # umls list of types for medcat filter\n",
    "    # 'types': ['qualifier value', 'procedure', 'substance', 'finding', 'environment', 'disorder', 'observable entity', 'organism', 'phenomenon', 'anatomy', 'conceptual entity', 'physical object', 'intellectual product', 'occupation or discipline', 'mental or behavioral dysfunction', 'geographic area', 'population group', 'biomedical or dental material', 'medical device', 'classification', 'regulation or law', 'health care activity', 'health care related organization', 'professional or occupational group', 'group', 'attribute', 'individual behavior']\n",
    "    \n",
    "    'Time_Value': ['Recent', 'Past'], # Specify the values you want to include in a list. Must be defined in medcat model. # Example ['Recent', 'Past', 'Subject/Experiencer']\n",
    "    'Time_Confidence': 0.8,  # Specify the confidence threshold as a float\n",
    "    'Presence_Value': ['True'], # Specify the values you want to include in a list\n",
    "    'Presence_Confidence': 0.8,  # Specify the confidence threshold as a float\n",
    "    'Subject_Value': ['Patient'], # Specify the values you want to include in a list\n",
    "    'Subject_Confidence': 0.8  # Specify the confidence threshold as a float\n",
    "}\n",
    "\n",
    "# Filter data batches by terms before processing. \n",
    "\n",
    "epr_docs_term_regex: Optional[Union[str, None]] = None\n",
    "mct_docs_term_regex: Optional[Union[str, None]] = None\n",
    "\n",
    "# Example bloods_filter_term_list: Optional[Union[List[str], None]] = ['wbc'] # This will only include basic observations with this item name analysed.\n",
    "bloods_filter_term_list: Optional[Union[List[str], None]] = None\n",
    "\n",
    "# Example mct_docs_document_type_filter_list: Optional[Union[List[str], None]] = ['KHMDC Integrated report'] # This will only include documents with this document type field value.\n",
    "\n",
    "mct_docs_document_type_filter_list: Optional[Union[List[str], None]] = None\n",
    "epr_docs_document_type_filter_list: Optional[Union[List[str], None]] = None\n",
    "\n",
    "data_type_filter_dict: Dict[str, any] = {\n",
    "    'filter_term_lists': {\n",
    "        'epr_docs': epr_docs_document_type_filter_list,\n",
    "        'mct_docs': mct_docs_document_type_filter_list,\n",
    "        'bloods': bloods_filter_term_list\n",
    "    },\n",
    "    'epr_docs_term_regex': epr_docs_term_regex,\n",
    "    'mct_docs_term_regex': mct_docs_term_regex,\n",
    "}\n",
    "\n",
    "#Example date settings:\n",
    "#start_date=(datetime(2020, 1, 1)) Start date for processing\n",
    "\n",
    "# Define the length of the time window, example 1 year and 15 days, only data within this window will be processed.\n",
    "# years=1,      # Number of years to add to the start date \n",
    "# months=0,  # Number of months to add to the start date\n",
    "# days=15,  # Number of days to add to the start date\n",
    "\n",
    "# Define the interval between time windows. Example 1 year. Each vector/row output will be based on this interval.\n",
    "# time_window_interval_delta = relativedelta(years=1)\n",
    "\n",
    "# lookback = True #This determines the direction of the time length window. True = backward, False = forward. Our time window (+1 years, 15 days) is therefore 2020, 1, 1 - 2021, 1, 15. \n",
    "\n",
    "# IPW settings:\n",
    "\n",
    "# Init config obj\n",
    "\n",
    "# Hypothetical date config_obj configuration:\n",
    "# I want all patients data between Feb 2015 and Jul 2020. This date window will extract and create the batched patient data for this time window.\n",
    "\n",
    "# global_start_year=2015, \n",
    "# global_start_month=2,  \n",
    "# global_end_year=2020,  \n",
    "# global_end_month=6, \n",
    "# global_start_day = 1, \n",
    "# global_end_day = 1, \n",
    "\n",
    "# I want patient vectors starting from Feb 2019 to Feb 2020 as I would like to see if X medical event is recorded on those taking medication Y\n",
    "# start_date=(datetime(2019, 2, 1)),  \n",
    "# years=1, \n",
    "# months=0,  \n",
    "# days=0, \n",
    "# lookback = False # 2019 to 2020 is forward in time.\n",
    "# I would like a single vector for each patient\n",
    "# time_window_interval_delta = relativedelta(years=1) \n",
    "# I would like 1 vector per month per patient for the 1 year time window\n",
    "# time_window_interval_delta = relativedelta(months=1)\n",
    "\n",
    "# Creating a configuration object for a specific task or project\n",
    "config_obj = config_class(\n",
    "    remote_dump=False,  # Flag for remote data dumping. partially deprecated.\n",
    "    suffix='',  # Suffix for file names\n",
    "    treatment_doc_filename='test_files/treatment_docs.csv', # Filename for treatment documentation\n",
    "    treatment_control_ratio_n=1,  # Ratio for treatment to control\n",
    "    proj_name='new_project', # Project name. patient data batches and vectors stored here.\n",
    "    current_path_dir=\"\",  # Current path directory\n",
    "    main_options=main_options_dict,  # Dictionary for main options\n",
    "    start_date=(datetime(1995, 1, 1)),  # Starting date for processing\n",
    "    years=30, # Number of years to add to the start date. Set the duration of the time window. Window is defined as the start date + years/months/days set here.\n",
    "    months=0,  # Number of months to add to the start date\n",
    "    days=0,  # Number of days to add to the start date\n",
    "    batch_mode=True,  # Flag for batch processing mode. Only functioning mode.\n",
    "    store_annot=True,  # Flag to store annotations. partially deprecated.\n",
    "    share_sftp=True,  # Flag for sharing via SFTP. partially deprecated\n",
    "    multi_process=False,  # Flag for multi-process execution. deprecated.\n",
    "    strip_list=True, # Flag for stripping lists, this will check for completed patients before starting to avoid redundancy.\n",
    "    verbosity=0,  # Verbosity level 0-9 printing debug messages\n",
    "    random_seed_val=random_seed_value,  # Random seed value for reproducibility of controls.\n",
    "    testing=True,  # Flag for testing mode. Will use dummy data.\n",
    "    dummy_medcat_model=True,  # Flag for dummy MedCAT model, used if testing == True, this will simulate a MedCAT model.\n",
    "    use_controls=False, # If true this will add desired ratio of controls at random from global pool, requires configuring with a master list of patients.\n",
    "    medcat=False, # Flag for MedCAT processing. #will load medcat into memory and use for annotating.\n",
    "    start_time=datetime.now(), # Current timestamp as the start time for logging and progress bar\n",
    "    patient_id_column_name='auto', # Column name for patient ID, auto will try to find it. Example \"client_idcode\"\n",
    "    annot_filter_options=annot_filter_arguments,  # Annotation filtering options\n",
    "    \n",
    "    # Global start year. #set the limits of the time window data can be drawn from. Start should not precede start date set above.\n",
    "    global_start_year=1995, # Global dates are overwritten by individual patient windows to match patient window. # Ensure that global start year/month/day is before end year/month/day\n",
    "    global_start_month=1,  # Global start month\n",
    "    global_end_year=2025,  # Global end year\n",
    "    global_end_month=1, # Global end month\n",
    "    global_start_day = 1, \n",
    "    global_end_day = 1, \n",
    "    ## Use these if each patient has their own individual time window. Requires preparing a table of start dates.\n",
    "    # individual_patient_window = True,\n",
    "    # individual_patient_window_df = pd.read_csv('ipw_overlap.csv'),\n",
    "    # individual_patient_window_start_column_name = 'updatetime_manual_offset',\n",
    "    # individual_patient_id_column_name = 'client_idcode',\n",
    "    # individual_patient_window_controls_method = 'full',\n",
    "    shuffle_pat_list=False,  # Flag for shuffling patient list\n",
    "    time_window_interval_delta = relativedelta(years=31), #specify the time window to collapse each feature vector into, years=1 is one vector per year within the global time window\n",
    "    split_clinical_notes=True, #will split clinical notes by date and treat as individual documents with extracted dates. Requires note splitter module. \n",
    "    lookback = False, # when calculating individual patient window from table of start dates, will calculate backwards in time if true. Else Forwards. When calculating from global start date, will calculate backwards or forwards respectively. \n",
    "    add_icd10 = False, #append icd 10 codes to annot batches. Can be found under current_pat_documents_annotations/%client_idcode%.csv.\n",
    "    add_opc4s=False, # needs icd10 true also. Can be found under current_pat_documents_annotations/%client_idcode%.csv\n",
    "    override_medcat_model_path = path_to_medcat_model_pack, #Force medcat model path, if None uses defaults for env. #Can be set in paths.py with medcat_path = %path to medcat model pack.zip\"\n",
    "    data_type_filter_dict = None, # Dictionary for data type filter, see examples above. \n",
    "    filter_split_notes = True, # If enabled, will reapply global time window filter post clinical note splitting. Recommended to enable if split notes enabled.\n",
    "    prefetch_pat_batches = False, # If enabled, will fetch batches for entire patient list and pre poulate batch folders with individual pat batches. Out of memory issues.\n",
    "    sample_treatment_docs=5 # If int > 0, will sample treatment documents from the treatment_docs.csv file. This is useful for testing and debugging / pilot run purposes.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.main_pat2vec import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj = main( cogstack=True, use_filter=False,\n",
    "             json_filter_path = None, random_seed_val=42, \n",
    "             hostname =None, config_obj= config_obj, )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View patient list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj.all_patient_list[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj.config_obj.date_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make pat vectors for pat 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj.pat_maker(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove specific patient raw documents and annotations:\n",
    "from pat2vec.util.post_processing import remove_file_from_paths\n",
    "\n",
    "# remove_file_from_paths(pat2vec_obj.all_patient_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the maximum number of retries\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "# Iterate through the patient list starting from index 0\n",
    "for i in tqdm(range(0, len(pat2vec_obj.all_patient_list))):\n",
    "    retries = 0\n",
    "    success = False\n",
    "    \n",
    "    while retries < MAX_RETRIES and not success:\n",
    "        try:\n",
    "            # Try to process the patient\n",
    "            pat2vec_obj.pat_maker(i)\n",
    "            success = True  # Mark as successful if no exception is raised\n",
    "            \n",
    "        except KeyError as e:\n",
    "            # Handle specific exception\n",
    "            print(f\"KeyError at index {i}: {e}. Retrying after removal...\")\n",
    "            remove_file_from_paths(pat2vec_obj.all_patient_list[i])\n",
    "            retries += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Handle generic exceptions\n",
    "            print(f\"Exception at index {i}: {e}. Skipping this patient...\")\n",
    "            break  # Break the retry loop for non-retryable exceptions\n",
    "            \n",
    "        finally:\n",
    "            pat2vec_obj.t.update(1)  # Update progress\n",
    "\n",
    "    if not success:\n",
    "        print(f\"Failed to process index {i} after {MAX_RETRIES} retries.\")\n",
    "\n",
    "pat2vec_obj.t.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Build Output Datasets\n",
    "\n",
    "The extraction process in Section 2 generates individual data files for each patient. This section consolidates those individual files into unified datasets for analysis.\n",
    "\n",
    "Run the following cells to perform these aggregations.\n",
    "\n",
    "### 1. Create Patient Vector Dataset\n",
    "The first cell concatenates all individual patient vectors (from `current_pat_lines_parts`) into a single CSV file. This is your primary dataset for downstream analysis or modeling.\n",
    "\n",
    "### 2. Merge Raw Data Sources (Optional)\n",
    "The subsequent cells allow you to merge raw data sources, such as:\n",
    "*   **Clinical Documents & Annotations:** Combines all EPR and MedCAT documents.\n",
    "*   **Medications:** Combines all drug records.\n",
    "*   **Blood Results:** Combines all blood test observations.\n",
    "\n",
    "> **⚠️ WARNING: LARGE FILES**\n",
    "> Merging raw data sources (especially clinical notes and annotations) can result in **extremely large files** (potentially exceeding available RAM or disk space).\n",
    "> *   Only run these cells if you specifically need the raw, aggregated data for filtering or inspection.\n",
    "> *   Ensure you have sufficient storage space before proceeding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = f'{pat2vec_obj.proj_name}/current_pat_lines_parts' # Patient vectors are stored individually in this directory. \n",
    "output_csv_file = 'output_file'\n",
    "\n",
    "# Specify the directory where you want to create the file\n",
    "directory = pat2vec_obj.proj_name + '/output_directory'\n",
    "\n",
    "# Check if the directory exists, if not, create it\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# We will join the individual patient vectors into a single output file. This is useful for filtering.\n",
    "output_csv_file_filename = process_csv_files(input_directory, out_folder=directory, output_filename_suffix=output_csv_file, part_size=336)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_csv_file_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_datetime_to_column(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build all document batches dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will merge all document source batches into a single file. This is useful for filtering. May produce a large file.\n",
    "\n",
    "from pat2vec.util.post_processing_build_methods import build_merged_epr_mct_doc_df\n",
    "\n",
    "all_pat_list = pat2vec_obj.all_patient_list\n",
    "\n",
    "dfd = build_merged_epr_mct_doc_df(all_pat_list, pat2vec_obj.config_obj, overwrite=True)\n",
    "\n",
    "#dfd = pd.read_csv(dfd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build all annotation batches dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will merge all annotation source batches into a single file. This is useful for filtering. May produce a large file.\n",
    "\n",
    "from pat2vec.util.post_processing_build_methods import build_merged_epr_mct_annot_df\n",
    "\n",
    "all_pat_list = pat2vec_obj.all_patient_list\n",
    "\n",
    "dfa = build_merged_epr_mct_annot_df(all_pat_list, pat2vec_obj.config_obj, overwrite=True)\n",
    "\n",
    "dfa = pd.read_csv(dfa)\n",
    "\n",
    "dfa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build additional batches from individual patient data batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will merge all drug source batches into a single file. This is useful for filtering. May produce a large file.\n",
    "\n",
    "from pat2vec.util.post_processing_build_methods import merge_drugs_csv\n",
    "\n",
    "all_pat_list = pat2vec_obj.all_patient_list\n",
    "\n",
    "merged_drugs_path = merge_drugs_csv(all_pat_list, pat2vec_obj.config_obj, overwrite=True)\n",
    "\n",
    "merged_drugs = pd.read_csv(merged_drugs_path)\n",
    "merged_drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfmdi = pd.read_csv('new_project/merged_input_pat_batches/merged_drugs_batches.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in dfmdi.select_dtypes(exclude=[np.number]).columns:\n",
    "#     assert dfmdi[col].astype(str).equals(merged_drugs[col].astype(str)), f\"Mismatch in column: {col}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will merge all diagnostics source batches into a single file. This is useful for filtering. May produce a large file.\n",
    "\n",
    "from pat2vec.util.post_processing_build_methods import merge_diagnostics_csv\n",
    "\n",
    "all_pat_list = pat2vec_obj.all_patient_list\n",
    "\n",
    "merged_diagnostics_path = merge_diagnostics_csv(all_pat_list, pat2vec_obj.config_obj, overwrite=True)\n",
    "\n",
    "merged_diagnostics = pd.read_csv(merged_diagnostics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.post_processing_build_methods import merge_news_csv\n",
    "\n",
    "all_pat_list = pat2vec_obj.all_patient_list\n",
    "\n",
    "merged_news_path = merge_news_csv(all_pat_list, pat2vec_obj.config_obj, overwrite=True)\n",
    "\n",
    "#merged_news = pd.read_csv(merged_news_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.post_processing_build_methods import merge_bmi_csv\n",
    "\n",
    "all_pat_list = pat2vec_obj.all_patient_list\n",
    "\n",
    "merged_bmi_path = merge_bmi_csv(all_pat_list, pat2vec_obj.config_obj, overwrite=True)\n",
    "\n",
    "#merged_bmi = pd.read_csv(merged_bmi_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.post_processing_build_methods import build_merged_bloods\n",
    "\n",
    "all_pat_list = pat2vec_obj.all_patient_list\n",
    "\n",
    "merged_bloods_path = build_merged_bloods(all_pat_list, pat2vec_obj.config_obj, overwrite=True)\n",
    "\n",
    "merged_bloods = pd.read_csv(merged_bloods_path)\n",
    "merged_bloods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_csv('new_project/merged_input_pat_batches/merged_bloods_batches.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.post_processing_build_methods import merge_demographics_csv\n",
    "\n",
    "all_pat_list = pat2vec_obj.all_patient_list\n",
    "\n",
    "merged_demographics_path = merge_demographics_csv(all_pat_list, pat2vec_obj.config_obj, overwrite=True)\n",
    "\n",
    "merged_demographics = pd.read_csv(merged_demographics_path)\n",
    "\n",
    "merged_demographics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the annotation batches by a snomed cui and its related codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snomed_methods_example = False\n",
    "# Optional example to filter annotation batches by snomed codes. \n",
    "\n",
    "if(snomed_methods_example):\n",
    "    from snomed_methods import snomed_methods_v1\n",
    "\n",
    "    path_to_sct2 = path_to_snomed_ct_file\n",
    "\n",
    "    medcat_path = path_to_medcat_model_pack\n",
    "\n",
    "    snomed_relations_obj = snomed_methods_v1.snomed_relations(medcat=True, snomed_rf2_full_path=path_to_sct2,\n",
    "                                                            medcat_path = medcat_path)\n",
    "    outcome_variable_cui_for_filter = '40733004'  # infection\n",
    "\n",
    "    print(outcome_variable_cui_for_filter)\n",
    "\n",
    "    filter_root_cui = outcome_variable_cui_for_filter\n",
    "    print(filter_root_cui)\n",
    "\n",
    "    retrieved_codes_snomed_tree, retrieved_names_snomed_tree = snomed_relations_obj.recursive_code_expansion(filter_root_cui, n_recursion = 3, debug=False)\n",
    "\n",
    "    print(retrieved_codes_snomed_tree[0:5], len(retrieved_codes_snomed_tree), len(retrieved_names_snomed_tree))\n",
    "\n",
    "    print(retrieved_names_snomed_tree[0:10])\n",
    "    \n",
    "    retrieved_codes_medcat_cdb, retrieved_names_medcat_cdb  = snomed_relations_obj.get_medcat_cdb_most_similar(filter_root_cui, context_type = 'xxxlong', type_id_filter=[], topn=25)\n",
    "    \n",
    "    print(retrieved_names_medcat_cdb[0:10])\n",
    "    \n",
    "    all_names_list = list(set(retrieved_names_medcat_cdb + retrieved_names_snomed_tree))\n",
    "\n",
    "    all_codes_list = list(set(retrieved_codes_medcat_cdb + retrieved_codes_snomed_tree))\n",
    "\n",
    "    print(len(all_names_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply misc methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example call methods from the pat2vec API which exposes most individual functions available in the package.\n",
    "\n",
    "#from pat2vec.all_methods import pat2vec_methods\n",
    "\n",
    "# p2v = pat2vec_methods()\n",
    "\n",
    "# p2v.produce_filtered_annotation_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build IPW dataframe\n",
    "\n",
    "\n",
    "Find the latest/earliest record for one of [268910001, 62315008, 55822004, 49727002]\n",
    "\n",
    "We can use this in another main block with:\n",
    "\n",
    "individual_patient_window = True,\n",
    "\n",
    "individual_patient_window_df = pd.read_csv('ipw_overlap.csv'),\n",
    "\n",
    "individual_patient_window_start_column_name = 'updatetime_manual_offset',\n",
    "\n",
    "individual_patient_id_column_name = 'client_idcode',\n",
    "\n",
    "individual_patient_window_controls_method = 'full', \n",
    "\n",
    "To limit each patients data to a specific individual time window. With controls we can match the time window per control or pull their 'full' data for the global time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_filter_arguments = {\n",
    "    'acc': 0.6,  # base concept accuracy\n",
    "    # umls list of types for medcat filter\n",
    "    #'types': ['qualifier value', 'procedure', 'substance', 'finding', 'environment', 'disorder', 'observable entity'],\n",
    "     'types': ['qualifier value', 'procedure', 'substance', 'finding', 'environment', 'disorder', 'observable entity', 'organism', 'phenomenon', 'anatomy', 'conceptual entity', 'physical object', 'intellectual product', 'occupation or discipline', 'mental or behavioral dysfunction', 'geographic area', 'population group', 'biomedical or dental material', 'medical device', 'classification', 'regulation or law', 'health care activity', 'health care related organization', 'professional or occupational group', 'group', 'attribute', 'individual behavior'],\n",
    "    # Specify the values you want to include in a list. Must be defined in medcat model.\n",
    "    'Time_Value': ['Recent', 'Past'],\n",
    "    'Time_Confidence': 0.6,  # Specify the confidence threshold as a float\n",
    "    # Specify the values you want to include in a list\n",
    "    'Presence_Value': ['True'],\n",
    "    'Presence_Confidence': 0.6,  # Specify the confidence threshold as a float\n",
    "    # Specify the values you want to include in a list\n",
    "    'Subject_Value': ['Patient'],\n",
    "    'Subject_Confidence': 0.6  # Specify the confidence threshold as a float\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(f'new_project/current_pat_document_batches/{pat2vec_obj.all_patient_list[1]}.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.post_processing_build_ipw_dataframe import build_ipw_dataframe\n",
    "\n",
    "build_ipw_dataframe(annot_filter_arguments=annot_filter_arguments, config_obj=pat2vec_obj.config_obj, filter_codes=[38341003, 274640006, 886731000000109,268910001, 62315008, 55822004, 49727002, 22232009], mode='latest', include_mct=True, include_textual_obs=False) # '62315008', '55822004', '268910001',\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.post_processing_build_ipw_dataframe import build_ipw_dataframe\n",
    "\n",
    "build_ipw_dataframe(annot_filter_arguments=annot_filter_arguments, config_obj=pat2vec_obj.config_obj, filter_codes=[38341003, 274640006, 268910001, 62315008, 55822004, 49727002, 248153007], mode='earliest' , include_mct=True, include_textual_obs=False) # '62315008', '55822004', '268910001',\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine and screen the patient client_idcode list for malformed entries \n",
    "\n",
    "from pat2vec.pat2vec_pat_list.get_patient_treatment_list import analyze_client_codes\n",
    "\n",
    "#valid_codes, invalid_codes, clusters = analyze_client_codes(pat2vec_obj.all_patient_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(treatment_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_docs = pd.read_csv('test_files/treatment_docs.csv')\n",
    "#assert len(treatment_docs) == 23\n",
    "print(len(treatment_docs)==23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert treatment_docs['basicobs_itemname_analysed'].iloc[21] == 'Parathyroid Hormone (PTH)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(treatment_docs['body_analysed'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'acrylic head' in str(treatment_docs['body_analysed'].iloc[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_example_annot = pd.read_csv('new_project/current_pat_documents_annotations_batches/P0IFD0TV.csv')\n",
    "\n",
    "pat_example_annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert pat_example_annot['cui'].iloc[0] == 38341003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Individual Patient Windows (IPW) (Optional)\n",
    "\n",
    "While Section 2 extracts data based on a global calendar range (e.g., \"Jan 2020 to Jan 2021\" for all patients), **Section 3** allows you to define a unique time window for each patient.\n",
    "\n",
    "**Use Case:**\n",
    "This is essential for study designs centered around a specific event, such as:\n",
    "*   \"Extract data for 1 year prior to **each patient's** diagnosis of Diabetes.\"\n",
    "*   \"Extract data for 6 months following **each patient's** first prescription of Drug X.\"\n",
    "\n",
    "### 3.1 Build the IPW Dataframe\n",
    "The first step is to define the start date (anchor date) for each patient. You can do this in two ways:\n",
    "\n",
    "1.  **Automatic Generation:** Use the code below to scan patient annotations for specific medical concepts (SNOMED codes). The script will find the \"earliest\" or \"latest\" instance of that concept to serve as the anchor date.\n",
    "2.  **Manual Loading:** If you already have a CSV file containing `client_idcode` and a specific start date column, you can load that directly and skip the build step.\n",
    "\n",
    "#### Configuration & Execution\n",
    "The following cells configure the concept filters (accuracy, semantic types) and run the `build_ipw_dataframe` function.\n",
    "\n",
    "*   **`filter_codes`**: A list of SNOMED CT codes used to identify the anchor event (e.g., diagnosis codes).\n",
    "*   **`mode`**: Set to `'earliest'` (first occurrence) or `'latest'` (most recent occurrence).\n",
    "*   **`annot_filter_arguments`**: Ensures only high-confidence annotations are used to set the date.\n",
    "\n",
    "**Output:**\n",
    "This process generates a file (e.g., `ipw_dataframe.csv`) containing the calculated start dates for every patient found with the specified concepts.\n",
    "\n",
    "### 3.2 Run Extraction with IPW\n",
    "Once the `ipw_dataframe` is created (or loaded), you must re-run the extraction process (similar to Section 2).\n",
    "\n",
    "**Key Differences in Configuration:**\n",
    "You must update the `config_obj` to prioritize the individual windows over the global dates:\n",
    "\n",
    "1.  Set `individual_patient_window = True`.\n",
    "2.  Pass the dataframe to `individual_patient_window_df`.\n",
    "3.  Set `lookback`:\n",
    "    *   `True`: The window extends **backwards** from the anchor date (e.g., \"History *before* diagnosis\").\n",
    "    *   `False`: The window extends **forwards** from the anchor date (e.g., \"Outcomes *after* treatment\").\n",
    "\n",
    "After updating the configuration, run the below `pat_maker` loop again to generate the new, relative-time patient vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPW demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build IPW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_filter_arguments = {\n",
    "    'acc': 0.1,  # base concept accuracy\n",
    "    # umls list of types for medcat filter\n",
    "    'types': ['qualifier value', 'procedure', 'substance', 'finding', 'environment', 'disorder', 'observable entity'],\n",
    "    # 'types': ['qualifier value', 'procedure', 'substance', 'finding', 'environment', 'disorder', 'observable entity', 'organism', 'phenomenon', 'anatomy', 'conceptual entity', 'physical object', 'intellectual product', 'occupation or discipline', 'mental or behavioral dysfunction', 'geographic area', 'population group', 'biomedical or dental material', 'medical device', 'classification', 'regulation or law', 'health care activity', 'health care related organization', 'professional or occupational group', 'group', 'attribute', 'individual behavior']\n",
    "    # Specify the values you want to include in a list. Must be defined in medcat model.\n",
    "    'Time_Value': ['Recent', 'Past'],\n",
    "    'Time_Confidence': 0.1,  # Specify the confidence threshold as a float\n",
    "    # Specify the values you want to include in a list\n",
    "    'Presence_Value': ['True'],\n",
    "    'Presence_Confidence': 0.1,  # Specify the confidence threshold as a float\n",
    "    # Specify the values you want to include in a list\n",
    "    'Subject_Value': ['Patient'],\n",
    "    'Subject_Confidence': 0.1  # Specify the confidence threshold as a float\n",
    "}\n",
    "\n",
    "pd.read_csv(f'new_project/current_pat_document_batches/{pat2vec_obj.all_patient_list[1]}.csv').head()\n",
    "len(pat2vec_obj.all_patient_list)\n",
    "pd.read_csv(f'new_project/current_pat_documents_annotations_batches/{pat2vec_obj.all_patient_list[1]}.csv').head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select two cui to simulate condition\n",
    "\n",
    "dfa_s = pd.read_csv('new_project/merged_batches/annots_mct_epr.csv')\n",
    "\n",
    "dfa_s.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa_s[dfa_s['client_idcode'] == 'V5LXO6QJ'].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using these two cui codes as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "# Group the data so we have a set of CUIs for each client\n",
    "client_cui_map = (\n",
    "    dfa_s.groupby('client_idcode')['cui']\n",
    "    .apply(set)\n",
    ")\n",
    "\n",
    "# Create all unique pairs of CUIs\n",
    "all_cuis = pd.Series(list(itertools.chain.from_iterable(client_cui_map))).unique()\n",
    "pairs = itertools.combinations(all_cuis, 2)\n",
    "\n",
    "# Dictionary to map each pair to a list of clients that have it\n",
    "pair_to_clients = {}\n",
    "\n",
    "for cui1, cui2 in pairs:\n",
    "    current_pair = {cui1, cui2}\n",
    "    # Create a list of clients that have both CUIs\n",
    "    clients_with_pair = [\n",
    "        client_id for client_id, cui_set in client_cui_map.items()\n",
    "        if current_pair.issubset(cui_set)\n",
    "    ]\n",
    "    \n",
    "    # If the list is not empty, add it to our dictionary\n",
    "    if clients_with_pair:\n",
    "        pair_to_clients[(cui1, cui2)] = clients_with_pair\n",
    "\n",
    "# Find the pair with the most clients by checking the length of the lists\n",
    "most_common_pair = max(pair_to_clients, key=lambda pair: len(pair_to_clients[pair]))\n",
    "\n",
    "# Get the list of clients and the count for that most common pair\n",
    "clients_list = pair_to_clients[most_common_pair]\n",
    "max_count = len(clients_list)\n",
    "\n",
    "print(f\"Most common co-occurring pair: {most_common_pair} with {max_count} clients having both.\")\n",
    "print(\"Clients with this pair:\")\n",
    "# Print each client from the list\n",
    "print(clients_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_A_pretty_name = dfa[dfa['cui']==int(most_common_pair[0])]['pretty_name'].iloc[0]\n",
    "\n",
    "concept_B_pretty_name = dfa[dfa['cui']==int(most_common_pair[1])]['pretty_name'].iloc[0]\n",
    "\n",
    "concept_A_pretty_name, concept_B_pretty_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_A_filter_codes = [int(most_common_pair[0])]\n",
    "concept_B_filter_codes = [int(most_common_pair[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_obj.verbosity = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the earliest occurrence of any CUI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa_s[(dfa_s['client_idcode'] == 'V5LXO6QJ') & (dfa_s['cui'].isin(concept_A_filter_codes + concept_B_filter_codes))].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.post_processing_build_ipw_dataframe import build_ipw_dataframe\n",
    "\n",
    "file_path = 'ipw_dataframe.csv'\n",
    "overwrite = True  \n",
    "skip_ipw_build = False\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    if overwrite:\n",
    "        pd.set_option('display.max_columns',None)\n",
    "\n",
    "        #n.b this needs filter annot arguments...\n",
    "        ipw_dataframe = build_ipw_dataframe(annot_filter_arguments=annot_filter_arguments, config_obj=pat2vec_obj.config_obj, filter_codes=concept_A_filter_codes + concept_B_filter_codes, mode='earliest', include_mct=False, include_textual_obs=False) # '62315008', '55822004', '268910001',\n",
    "        ipw_dataframe.to_csv(file_path)\n",
    "        ipw_dataframe\n",
    "        # Proceed with overwriting the file\n",
    "        print(\"File exists and will be overwritten.\")\n",
    "    else:\n",
    "        # Skip or handle the existing file\n",
    "        ipw_dataframe = pd.read_csv('ipw_dataframe.csv')\n",
    "        print(\"File exists and will NOT be overwritten.\")\n",
    "else:\n",
    "    # File does not exist, safe to proceed\n",
    "    print(\"File does not exist, safe to proceed.\")\n",
    "    \n",
    "    pd.set_option('display.max_columns',None)\n",
    "\n",
    "    #n.b this needs filter annot arguments...\n",
    "    ipw_dataframe = build_ipw_dataframe(annot_filter_arguments=annot_filter_arguments, config_obj=pat2vec_obj.config_obj, filter_codes=concept_A_filter_codes + concept_B_filter_codes, mode='earliest', include_mct=False, include_textual_obs=False) # '62315008', '55822004', '268910001',\n",
    "    ipw_dataframe.to_csv(file_path)\n",
    "    ipw_dataframe\n",
    "\n",
    "ipw_dataframe.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additionally filter by only those who had both of the cui coocurring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pat2vec.util.post_processing import filter_annot_dataframe2\n",
    "\n",
    "annot_batch_file_path = 'new_project/merged_batches/annots_mct_epr.csv'\n",
    "\n",
    "# Assume concept_A_filter_codes and concept_B_filter_codes are defined as sets for efficiency\n",
    "concept_A_filter_codes_set = set(concept_A_filter_codes)\n",
    "concept_B_filter_codes_set = set(concept_B_filter_codes)\n",
    "# Assume annot_filter_arguments is defined\n",
    "\n",
    "if not skip_ipw_build:\n",
    "    # Initialize two empty sets to store client IDs for each condition\n",
    "    clients_with_concept_A = set()\n",
    "    clients_with_concept_B = set()\n",
    "\n",
    "    # Process the file in chunks ⚙️\n",
    "    for chunk in pd.read_csv(annot_batch_file_path, chunksize=100000):\n",
    "        \n",
    "        # 1. Filter annotations by earlier annotation filter arguments first\n",
    "        chunk = filter_annot_dataframe2(chunk, annot_filter_arguments)\n",
    "\n",
    "        # 2. Find clients in this chunk with a Concept A code and update the set\n",
    "        A_in_chunk = chunk[chunk['cui'].isin(concept_A_filter_codes)]['client_idcode'].unique()\n",
    "        clients_with_concept_A.update(A_in_chunk)\n",
    "        \n",
    "        # 3. Find clients in this chunk with a Concept B code and update the set\n",
    "        B_in_chunk = chunk[chunk['cui'].isin(concept_B_filter_codes)]['client_idcode'].unique()\n",
    "        clients_with_concept_B.update(B_in_chunk)\n",
    "\n",
    "    # 4. The final list is the intersection of the two sets ✅\n",
    "    true_clients = list(clients_with_concept_A.intersection(clients_with_concept_B))\n",
    "\n",
    "    print(f\"Found {len(true_clients)} patients with both concept A and concept B\")\n",
    "    print(true_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "# finally filter the IPW by the true clients with concept_A_filter_codes and concept_B_filter_codes\n",
    "if not skip_ipw_build:\n",
    "    ipw_dataframe = ipw_dataframe[ipw_dataframe['client_idcode'].isin(true_clients)]\n",
    "\n",
    "    ipw_dataframe\n",
    "if not skip_ipw_build:\n",
    "    ipw_dataframe.reset_index(drop=True, inplace=True)\n",
    "if not skip_ipw_build:\n",
    "    ipw_dataframe.to_csv('ipw_dataframe.csv')\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "if not skip_ipw_build:\n",
    "    # Convert to datetime and ensure all values are timezone-aware in UTC\n",
    "    ipw_dataframe['updatetime'] = pd.to_datetime(\n",
    "    ipw_dataframe['updatetime'], #format='ISO8601',\n",
    "    utc=True\n",
    ")\n",
    "\n",
    "    # We need to compute individual start and end dates for each patient in the IPW dataframe.\n",
    "    # We will use the 'updatetime' column as the basis for this calculation.\n",
    "    # We will create two new columns: 'updatetime_offset' and 'updatetime_end_date'.\n",
    "    # These will be used to create the patient_dict for pat2vec processing.\n",
    "    # We add a buffer of 3 months to the 'updatetime' to create 'updatetime_offset' to avoid information leakage.\n",
    "\n",
    "    # add 3 months using pd.DateOffset, this is a buffer between the first mention of the concept and our new individual patient start time/ time window. \n",
    "    ipw_dataframe['updatetime_offset'] = ipw_dataframe['updatetime'] + pd.DateOffset(months=3)\n",
    "\n",
    "    ipw_dataframe['updatetime_offset'] = pd.to_datetime(ipw_dataframe['updatetime_offset'], format='ISO8601', utc=True)\n",
    "\n",
    "    # Now add the time delta to create the individual patient window end date from the offset date\n",
    "\n",
    "    ipw_dataframe['updatetime_end_date'] = ipw_dataframe['updatetime_offset'].apply(lambda dt: dt + pat2vec_obj.config_obj.time_window_interval_delta)\n",
    "\n",
    "    ipw_dataframe.to_csv('ipw_dataframe.csv')\n",
    "\n",
    "    ipw_dataframe\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipw_dataframe.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Run Extraction with Individual Patient Windows\n",
    "\n",
    "Now that you have defined your Individual Patient Windows (IPW) in Section 3.1 (either by building them from annotations or loading a pre-existing CSV), you must re-run the extraction process.\n",
    "\n",
    "This step is similar to Section 2, but the configuration is updated to prioritize patient-specific time windows over global calendar dates.\n",
    "\n",
    "### Configuration Updates\n",
    "\n",
    "The code below initializes a new configuration object with specific IPW settings:\n",
    "\n",
    "*   **`individual_patient_window = True`**: This is the master switch. It tells the system to ignore the global `start_date` and instead use the specific dates found in your dataframe.\n",
    "*   **`individual_patient_window_df`**: Loads the dataframe containing the anchor dates (e.g., `ipw_dataframe.csv`).\n",
    "*   **`individual_patient_window_start_column_name`**: Specifies the column name in the dataframe that contains the anchor date (e.g., `'updatetime'`).\n",
    "*   **`lookback`**: Determines the direction of the time window relative to the anchor date:\n",
    "    *   `False` (Forward): Extracts data *after* the anchor date (e.g., outcomes following a treatment).\n",
    "    *   `True` (Backward): Extracts data *before* the anchor date (e.g., history prior to a diagnosis).\n",
    "*   **`proj_name`**: We recommend changing this (e.g., to `'new_project_ipw'`) to ensure these results are stored in a separate output directory from your previous runs.\n",
    "\n",
    "### Execution\n",
    "\n",
    "Run the following cells to:\n",
    "1.  **Configure**: Apply the IPW settings.\n",
    "2.  **Initialize**: Create the `pat2vec` object.\n",
    "3.  **Execute**: Run the `pat_maker` loop to generate the patient vectors based on their unique time windows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "from pat2vec.util.config_pat2vec import config_class\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from pat2vec.util.post_processing_process_csv_files import process_csv_files\n",
    "from pat2vec.util.post_processing import extract_datetime_to_column\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "# Configuration dictionary for main options in a medical application\n",
    "main_options_dict = {\n",
    "    # Enable demographic information (Ethnicity mapped to UK census, age, death)\n",
    "    'demo': True,\n",
    "    'bmi': True,  # Enable BMI (Body Mass Index) tracking\n",
    "    'bloods': True,  # Enable blood-related information\n",
    "    'drugs': True,  # Enable drug-related information\n",
    "    'diagnostics': True,  # Enable diagnostic information\n",
    "\n",
    "    'core_02': True,  # Enable core_02 information\n",
    "    'bed': True,  # Enable bed n information\n",
    "    'vte_status': True,  # Enable VTE () status tracking\n",
    "    'hosp_site': True,  # Enable hospital site information\n",
    "    'core_resus': True,  # Enable core resuscitation information\n",
    "    'news': True,  # Enable NEWS (National Early Warning Score) tracking\n",
    "\n",
    "    'smoking': True,  # Enable smoking-related information\n",
    "    'annotations': True,  # Enable EPR annotations\n",
    "    # Enable MRC (Additional clinical note observations index) annotations\n",
    "    'annotations_mrc': True,\n",
    "    # Enable or disable negated presence annotations\n",
    "    'negated_presence_annotations': False,\n",
    "    'appointments': True,  # Enable appointments\n",
    "    'annotations_reports': False,  # Enable reports\n",
    "    'textual_obs': True,  # Enable textual observations (basic_observations index)\n",
    "}\n",
    "\n",
    "\n",
    "annot_filter_arguments = {\n",
    "    'acc': 0.8,  # base concept accuracy\n",
    "    # umls list of types for medcat filter\n",
    "    'types': ['qualifier value', 'procedure', 'substance', 'finding', 'environment', 'disorder', 'observable entity'],\n",
    "    # 'types': ['qualifier value', 'procedure', 'substance', 'finding', 'environment', 'disorder', 'observable entity', 'organism', 'phenomenon', 'anatomy', 'conceptual entity', 'physical object', 'intellectual product', 'occupation or discipline', 'mental or behavioral dysfunction', 'geographic area', 'population group', 'biomedical or dental material', 'medical device', 'classification', 'regulation or law', 'health care activity', 'health care related organization', 'professional or occupational group', 'group', 'attribute', 'individual behavior']\n",
    "    # Specify the values you want to include in a list. Must be defined in medcat model.\n",
    "    'Time_Value': ['Recent', 'Past'],\n",
    "    'Time_Confidence': 0.8,  # Specify the confidence threshold as a float\n",
    "    # Specify the values you want to include in a list\n",
    "    'Presence_Value': ['True'],\n",
    "    'Presence_Confidence': 0.8,  # Specify the confidence threshold as a float\n",
    "    # Specify the values you want to include in a list\n",
    "    'Subject_Value': ['Patient'],\n",
    "    'Subject_Confidence': 0.8  # Specify the confidence threshold as a float\n",
    "}\n",
    "\n",
    "# Filter data batches by terms before processing. \n",
    "\n",
    "epr_docs_term_regex: Optional[Union[str, None]] = None\n",
    "mct_docs_term_regex: Optional[Union[str, None]] = None\n",
    "\n",
    "# Example bloods_filter_term_list: Optional[Union[List[str], None]] = ['wbc']\n",
    "bloods_filter_term_list: Optional[Union[List[str], None]] = None\n",
    "\n",
    "# Example mct_docs_document_type_filter_list: Optional[Union[List[str], None]] = ['KHMDC Integrated report']\n",
    "\n",
    "mct_docs_document_type_filter_list: Optional[Union[List[str], None]] = None\n",
    "epr_docs_document_type_filter_list: Optional[Union[List[str], None]] = None\n",
    "\n",
    "data_type_filter_dict: Dict[str, any] = {\n",
    "    'filter_term_lists': {\n",
    "        'epr_docs': epr_docs_document_type_filter_list,\n",
    "        'mct_docs': mct_docs_document_type_filter_list,\n",
    "        'bloods': bloods_filter_term_list\n",
    "    },\n",
    "    'epr_docs_term_regex': epr_docs_term_regex,\n",
    "    'mct_docs_term_regex': mct_docs_term_regex,\n",
    "}\n",
    "\n",
    "#Example date settings:\n",
    "#start_date=(datetime(2020, 1, 1)) Start date for processing\n",
    "\n",
    "# Define the length of the time window, example 1 year and 15 days, only data within this window will be processed.\n",
    "# years=1,      # Number of years to add to the start date \n",
    "# months=0,  # Number of months to add to the start date\n",
    "# days=15,  # Number of days to add to the start date\n",
    "\n",
    "# Define the interval between time windows. Example 1 year. Each vector/row output will be based on this interval.\n",
    "# time_window_interval_delta = relativedelta(years=1)\n",
    "\n",
    "# lookback = True #This determines the direction of the time length window. True = backward, False = forward. Our time window (+1 years, 15 days) is therefore 2020, 1, 1 - 2021, 1, 15. \n",
    "\n",
    "# IPW settings:\n",
    "\n",
    "# Init config obj\n",
    "\n",
    "# Creating a configuration object for a specific task or project\n",
    "config_obj = config_class(\n",
    "    remote_dump=False,  # Flag for remote data dumping. partially deprecated.\n",
    "    suffix='',  # Suffix for file names\n",
    "    # Filename for treatment documentation\n",
    "    treatment_doc_filename='treatment_docs.csv',\n",
    "    treatment_control_ratio_n=1,  # Ratio for treatment to control\n",
    "    # Project name. patient data batches and vectors stored here.\n",
    "    proj_name='new_project_ipw',\n",
    "    current_path_dir=\"\",  # Current path directory\n",
    "    main_options=main_options_dict,  # Dictionary for main options\n",
    "    start_date=(datetime(1995, 1, 1)),  # Starting date for processing\n",
    "    # Number of years to add to the start date. Set the duration of the time window. Window is defined as the start date + years/months/days set here.\n",
    "    years=30,\n",
    "    months=0,  # Number of months to add to the start date\n",
    "    days=0,  # Number of days to add to the start date\n",
    "    batch_mode=True,  # Flag for batch processing mode. only functioning mode.\n",
    "    store_annot=True,  # Flag to store annotations. partially deprecated.\n",
    "    share_sftp=True,  # Flag for sharing via SFTP. partially deprecated\n",
    "    multi_process=False,  # Flag for multi-process execution. deprecated.\n",
    "    #annot_first=False,  # Flag for annotation priority. deprecated.\n",
    "    # Flag for stripping lists, will check for completed patients before starting to avoid redundancy.\n",
    "    strip_list=True,\n",
    "    verbosity=0,  # Verbosity level 0-9 printing debug messages\n",
    "    random_seed_val=random_seed_value,  # Random seed value for reproducibility of controls.\n",
    "    testing=True,  # Flag for testing mode\n",
    "    dummy_medcat_model=True,  # Flag for dummy MedCAT model, used if testing == True\n",
    "    # Flag for using controls. #will add desired ratio of controls at random from global pool.\n",
    "    use_controls=False,\n",
    "    # Flag for MedCAT processing. #will load medcat into memory and use for annotating.\n",
    "    medcat=False,\n",
    "    # Current timestamp as the start time for logging and progress bar\n",
    "    start_time=datetime.now(),\n",
    "    # Column name for patient ID, auto will try to find it. Example \"client_idcode\"\n",
    "    patient_id_column_name='client_idcode',\n",
    "    annot_filter_options=annot_filter_arguments,  # Annotation filtering options\n",
    "    # Global start year. #set the limits of the time window data can be drawn from. Start should not precede start date set above.\n",
    "    global_start_year=1995, # Global dates are overwritten by individual patient windows to match patient window. # Ensure that global start year/month/day is before end year/month/day\n",
    "    global_start_month=1,  # Global start month\n",
    "    global_end_year=2023,  # Global end year\n",
    "    global_end_month=1, # Global end month\n",
    "    global_start_day = 1, \n",
    "    global_end_day = 1, \n",
    "    individual_patient_window = True,\n",
    "    individual_patient_window_df = pd.read_csv('ipw_dataframe.csv'),\n",
    "    individual_patient_window_start_column_name = 'updatetime', #_offset , this will look for your start column name + '_offset'\n",
    "    individual_patient_id_column_name = 'client_idcode',\n",
    "    individual_patient_window_controls_method = 'full',\n",
    "    shuffle_pat_list=False,  # Flag for shuffling patient list\n",
    "    time_window_interval_delta = relativedelta(years=31), #specify the time window to collapse each feature vector into, years=1 is one vector per year within the global time window\n",
    "    split_clinical_notes=True, #will split clinical notes by date and treat as individual documents with extracted dates. Requires note splitter module. \n",
    "    lookback = False, # when calculating individual patient window from table of start dates, will calculate backwards in time if true. Else Forwards. When calculating from global start date, will calculate backwards or forwards respectively. \n",
    "    add_icd10 = False, #append icd 10 codes to annot batches. Can be found under current_pat_documents_annotations/%client_idcode%.csv.\n",
    "    add_opc4s=False, # needs icd10 true also. Can be found under current_pat_documents_annotations/%client_idcode%.csv\n",
    "    override_medcat_model_path = path_to_medcat_model_pack, #Force medcat model path, if None uses defaults for env. #Can be set in paths.py with medcat_path = %path to medcat model pack.zip\"\n",
    "    data_type_filter_dict = None, # Dictionary for data type filter, see examples above. \n",
    "    filter_split_notes = True, # If enabled, will reapply global time window filter post clinical note splitting. Recommended to enable if split notes enabled.\n",
    "    calculate_vectors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_obj.individual_patient_window_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj = main( cogstack=True, use_filter=False,\n",
    "             json_filter_path = None, random_seed_val=42, \n",
    "             hostname =None, config_obj= config_obj, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_obj.patient_dict # These are the individual patient time windows for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj.config_obj.date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_obj.verbosity = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj.config_obj.date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj.pat_maker(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the maximum number of retries\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "# Iterate through the patient list starting from index 0\n",
    "for i in tqdm(range(0, len(pat2vec_obj.all_patient_list))):\n",
    "    retries = 0\n",
    "    success = False\n",
    "    \n",
    "    while retries < MAX_RETRIES and not success:\n",
    "        try:\n",
    "            # Try to process the patient\n",
    "            pat2vec_obj.pat_maker(i)\n",
    "            success = True  # Mark as successful if no exception is raised\n",
    "            \n",
    "        except KeyError as e:\n",
    "            # Handle specific exception\n",
    "            print(f\"KeyError at index {i}: {e}. Retrying after removal...\")\n",
    "            remove_file_from_paths(pat2vec_obj.all_patient_list[i])\n",
    "            retries += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Handle generic exceptions\n",
    "            print(f\"Exception at index {i}: {e}. Skipping this patient...\")\n",
    "            raise e\n",
    "            break  # Break the retry loop for non-retryable exceptions\n",
    "            \n",
    "        finally:\n",
    "            pat2vec_obj.t.update(1)  # Update progress\n",
    "\n",
    "    if not success:\n",
    "        print(f\"Failed to process index {i} after {MAX_RETRIES} retries.\")\n",
    "\n",
    "pat2vec_obj.t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj.config_obj.date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = f'{pat2vec_obj.proj_name}/current_pat_lines_parts' # Patient vectors are stored individually in this directory. \n",
    "output_csv_file = 'output_file'\n",
    "\n",
    "# Specify the directory where you want to create the file\n",
    "directory = pat2vec_obj.proj_name + '/output_directory'\n",
    "\n",
    "# Check if the directory exists, if not, create it\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# We will join the individual patient vectors into a single output file. This is useful for filtering.\n",
    "output_csv_file_filename = process_csv_files(input_directory, out_folder=directory, output_filename_suffix=output_csv_file, part_size=336)\n",
    "df = pd.read_csv(output_csv_file_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['client_idcode']=='V1IBLJH7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['client_idcode']==list(pat2vec_obj.config_obj.patient_dict.keys())[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['extracted_datetime_stamp'] = extract_datetime_to_column(df)['extracted_datetime_stamp']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_processed = list(pat2vec_obj.config_obj.patient_dict.keys())\n",
    "print(len(patients_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter df by patients_processed list on client_idcode\n",
    "\n",
    "df_filtered = df[df['client_idcode'].isin(patients_processed)]\n",
    "\n",
    "df_filtered[['client_idcode','extracted_datetime_stamp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj.config_obj.patient_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Create DataFrame and Find Earliest Date\n",
    "df_dict = pd.DataFrame.from_dict(\n",
    "    pat2vec_obj.config_obj.patient_dict,\n",
    "    orient='index',\n",
    "    columns=['date1', 'date2']\n",
    ")\n",
    "df_dict.index.name = 'patient_id'\n",
    "df_dict.reset_index(inplace=True)\n",
    "date1_ts = pd.to_datetime(df_dict['date1'])\n",
    "date2_ts = pd.to_datetime(df_dict['date2'])\n",
    "df_dict['earliest_date_to_check'] = np.minimum(date1_ts, date2_ts)\n",
    "\n",
    "# 2. Handle Duplicates in the Main DataFrame\n",
    "if df_filtered['client_idcode'].duplicated().any():\n",
    "    print(f\"⚠️ Warning: Found and dropped {df_filtered.duplicated(subset=['client_idcode']).sum()} duplicate patient IDs.\")\n",
    "    df_filtered_unique = df_filtered.drop_duplicates(subset=['client_idcode'], keep='first')\n",
    "else:\n",
    "    df_filtered_unique = df_filtered\n",
    "\n",
    "# --- NEW: Diagnostic Checks ---\n",
    "print(\"\\n## Diagnostic Info\")\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "# Check if the data sources are empty\n",
    "print(f\"1. Size of data from dictionary: {len(df_dict)} rows\")\n",
    "print(f\"2. Size of data from df_filtered: {len(df_filtered_unique)} rows\")\n",
    "\n",
    "if len(df_dict) > 0 and len(df_filtered_unique) > 0:\n",
    "    # Check the data types of the keys\n",
    "    print(f\"\\n3. Data type of 'patient_id' (from dict): {df_dict['patient_id'].dtype}\")\n",
    "    print(f\"4. Data type of 'client_idcode' (from df): {df_filtered_unique['client_idcode'].dtype}\")\n",
    "\n",
    "    # Show a sample of the keys to visually inspect for whitespace/casing\n",
    "    print(\"\\n5. Sample keys from dictionary:\\n\", df_dict['patient_id'].head(3).to_list())\n",
    "    print(\"\\n6. Sample keys from df_filtered:\\n\", df_filtered_unique['client_idcode'].head(3).to_list())\n",
    "\n",
    "    # Programmatically find the exact number of overlapping IDs\n",
    "    set_dict = set(df_dict['patient_id'].astype(str).str.strip())\n",
    "    set_df = set(df_filtered_unique['client_idcode'].astype(str).str.strip())\n",
    "    overlap = set_dict.intersection(set_df)\n",
    "    print(f\"\\n7. Found {len(overlap)} common IDs between the two sources after cleaning whitespace.\")\n",
    "    if len(overlap) < 5 and len(overlap) > 0:\n",
    "         print(f\"   -> Common IDs are: {list(overlap)}\")\n",
    "else:\n",
    "    print(\"\\nOne or both data sources are empty. Cannot perform merge.\")\n",
    "\n",
    "print(\"--------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "merged_df = pd.merge(\n",
    "    left=df_dict,\n",
    "    right=df_filtered_unique,\n",
    "    left_on='patient_id',\n",
    "    right_on='client_idcode',\n",
    "    how='inner'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj.all_patient_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_obj.individual_patient_window_df[config_obj.individual_patient_window_df['client_idcode']==pat2vec_obj.all_patient_list[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pat2vec_obj.all_patient_list), len(pat2vec_obj.config_obj.patient_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['client_idcode']==pat2vec_obj.all_patient_list[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pat2vec_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
