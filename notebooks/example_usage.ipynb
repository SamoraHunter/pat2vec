{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from elasticsearch.exceptions import AuthenticationException\n",
    "\n",
    "# Fix the random seed for reproducibility in unit testing\n",
    "\n",
    "random_seed_value = 42\n",
    "\n",
    "np.random.seed(random_seed_value)\n",
    "\n",
    "random.seed(random_seed_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Print the current working directory\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "# 2. Print Python's sys.path\n",
    "print(\"Python Path:\", sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove dir\n",
    "clear_previous_outputs = True\n",
    "\n",
    "if(clear_previous_outputs):\n",
    "\n",
    "    shutil.rmtree('new_project', ignore_errors=True)\n",
    "\n",
    "    shutil.rmtree('new_project_ipw', ignore_errors=True)\n",
    "\n",
    "    shutil.rmtree('treatment_doc_extract', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure dependencies are on path\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Define relative paths from the current working directory\n",
    "path_to_medcat_model_pack = os.path.abspath(os.path.join(current_dir, '..', '..', 'medcat_models', 'medcat_model_pack_422d1d38fc58f158.zip'))\n",
    "\n",
    "path_to_snomed_ct_file = os.path.abspath(os.path.join(current_dir, '..', '..', 'snomed', 'SnomedCT_InternationalRF2_PRODUCTION_20231101T120000Z', 'SnomedCT_InternationalRF2_PRODUCTION_20231101T120000Z', 'Full', 'Terminology', 'sct2_StatedRelationship_Full_INT_20231101.txt'))\n",
    "\n",
    " # Define the relative path\n",
    "path_to_gloabl_files = '../../'\n",
    "\n",
    "additional_path_to_pat2vec = 'pat2vec'\n",
    "\n",
    "additional_path_to_pat2vec = os.path.abspath(os.path.join(path_to_gloabl_files, additional_path_to_pat2vec))\n",
    "\n",
    "# Get the absolute path of the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Combine the current directory with the relative path\n",
    "absolute_path = os.path.abspath(os.path.join(current_dir, path_to_gloabl_files))\n",
    "\n",
    "# Usage examples\n",
    "print(path_to_medcat_model_pack)\n",
    "print(path_to_snomed_ct_file)\n",
    "print(path_to_gloabl_files)\n",
    "print(additional_path_to_pat2vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, path_to_gloabl_files)\n",
    "sys.path.insert(0, additional_path_to_pat2vec)\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Add the grandparent directory of the current directory to the Python path\n",
    "grandparent_dir = os.path.dirname(parent_dir)\n",
    "sys.path.append(grandparent_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.logger_setup import setup_logger\n",
    "\n",
    "# Get the logger\n",
    "logger = setup_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get treatment_docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.config_pat2vec import config_class\n",
    "\n",
    "config_obj = config_class(medcat=False, # Load medcat, ensure model pack is in gloabl_files/medcat_models/ ..examplemodelpack.zip\n",
    "                          override_medcat_model_path = path_to_medcat_model_pack,\n",
    "                          proj_name='treatment_doc_extract',\n",
    "                          verbosity=0,\n",
    "                          global_start_year=1995, # Set the start date, this will extract data between these dates.\n",
    "                          global_end_year=2024,\n",
    "                          global_start_month=1,\n",
    "                          global_end_month=12,\n",
    "                          global_start_day=1,\n",
    "                          global_end_day=31,\n",
    "                          lookback=False, # Set to True if you want to look back at the previous year and month\n",
    "                          testing=True # Set to True if you want to run in testing mode, this will use dummy data for testing.\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.main_pat2vec import main\n",
    "\n",
    "pat2vec_obj = main( cogstack=True, use_filter=False,\n",
    "             json_filter_path = None, random_seed_val=random_seed_value, \n",
    "             hostname =None, config_obj= config_obj, ) # initialize the pat2vec object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pat2vec_obj.config_obj.testing:\n",
    "    print(\"Testing mode is enabled, skipping authentication check.\")\n",
    "else:\n",
    "    # Check if the Elasticsearch client is authenticated # advise user to check credentials\n",
    "    try:\n",
    "        pat2vec_obj.cs.elastic.info()\n",
    "    except AuthenticationException as e:\n",
    "        print(f\"Authentication failed: {e.info['error']['reason']}\")\n",
    "        print(\"Please check your Elasticsearch credentials in the configuration file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while checking Elasticsearch authentication: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snomed_example = False\n",
    "\n",
    "if(snomed_example):\n",
    "    \n",
    "    from snomed_methods import snomed_methods_v1\n",
    "\n",
    "    path_to_sct2 = path_to_snomed_ct_file\n",
    "\n",
    "    medcat_path = path_to_medcat_model_pack\n",
    "\n",
    "    snomed_relations_obj = snomed_methods_v1.snomed_relations(medcat=True, snomed_rf2_full_path=path_to_sct2,\n",
    "                                                            medcat_path = medcat_path)\n",
    "\n",
    "    outcome_variable_cui_for_filter = '109989006'  # myeloma\n",
    "\n",
    "    print(outcome_variable_cui_for_filter)\n",
    "\n",
    "    filter_root_cui = outcome_variable_cui_for_filter\n",
    "    print(filter_root_cui)\n",
    "\n",
    "    retrieved_codes_snomed_tree, retrieved_names_snomed_tree = snomed_relations_obj.recursive_code_expansion(filter_root_cui, n_recursion = 3, debug=False)\n",
    "\n",
    "    print(retrieved_codes_snomed_tree[0:5], len(retrieved_codes_snomed_tree), len(retrieved_names_snomed_tree))\n",
    "\n",
    "\n",
    "    retrieved_codes_medcat_cdb, retrieved_names_medcat_cdb = (\n",
    "    snomed_relations_obj.get_medcat_cdb_most_similar(\n",
    "        filter_root_cui, context_type=\"xxxlong\", type_id_filter=[], topn=50\n",
    "    )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add terms to search the document indicies for\n",
    "\n",
    "term_list = ['myeloma', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.pre_processing import get_treatment_docs_by_iterative_multi_term_cohort_searcher_no_terms_fuzzy\n",
    "\n",
    "# Example getting a patient cohort by the presence of terms in their clinical documents\n",
    "\n",
    "# We start by extracting the documents across textual document sources with fuzzy string matching \n",
    "\n",
    "treatment_docs = get_treatment_docs_by_iterative_multi_term_cohort_searcher_no_terms_fuzzy(pat2vec_obj=pat2vec_obj,\n",
    "                                                                          term_list=term_list, # List of terms to search for\n",
    "                                                                          overwrite=True, # overwrite existing treatment_docs.csv else append results\n",
    "                                                                          append=False, # Append results to existing treatment_docs.csv\n",
    "                                                                          verbose=9, # Adjust verbosity for logging\n",
    "                                                                          mct=True, # Include clinical notes text sources, this will search an additional document index\n",
    "                                                                          textual_obs=True, # Include observations index text sources, this will search an additional document index\n",
    "                                                                          additional_filters=None, # Add additional filters to the search such as document type. \n",
    "                                                                          all_fields=False # Return all fields from indicies instead of just a typical subset. \n",
    "                                                                          )\n",
    "\n",
    "treatment_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example get cohort by drug treatment for cross reference etc\n",
    "\n",
    "# Example, I want to get a cohort of patients who have drug orders to check against their diagnosis status from the previous step.\n",
    "\n",
    "from pat2vec.util.pre_get_drug_treatment_docs import iterative_drug_treatment_search\n",
    "import pandas as pd\n",
    "\n",
    "retrieve_cohort_by_drug_treatment = False\n",
    "\n",
    "if retrieve_cohort_by_drug_treatment:\n",
    "\n",
    "    search_terms_list = [\"asprin\", \"ibuprofen\", \"Emtricitabine\", \"Mepacrine\"]\n",
    "    output_file_path = \"drug_treatment_records.csv\"\n",
    "\n",
    "    iterative_drug_treatment_search(\n",
    "        pat2vec_obj=pat2vec_obj,\n",
    "        search_terms=search_terms_list,\n",
    "        output_file_path=output_file_path,\n",
    "        verbose=5,  # Adjust verbosity for logging\n",
    "        drop_duplicates=True, # Search terms can produce duplicates, remove by order guid.\n",
    "        overwrite = True # Overwrite initial output file\n",
    "    )\n",
    "    \n",
    "    # Load the csv file\n",
    "    df_drug_treatment_cohort = pd.read_csv(output_file_path)\n",
    "    df_drug_treatment_cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.config_pat2vec import config_class\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from pat2vec.util.post_processing import process_csv_files\n",
    "from pat2vec.util.post_processing import extract_datetime_to_column\n",
    "from pat2vec.pat2vec_pat_list.get_patient_treatment_list import get_all_patients_list\n",
    "from pat2vec.util.post_processing import produce_filtered_annotation_dataframe\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "# Configuration dictionary for main options in pat2vec\n",
    "main_options_dict = {\n",
    "    \n",
    "    'demo': True, # Enable demographic information (Ethnicity mapped to UK census categories, age, death).\n",
    "    'bmi': True,  # Enable BMI (Body Mass Index) information.\n",
    "    'bloods': True,  # Enable blood-related information\n",
    "    'drugs': True,  # Enable drug-related information\n",
    "    'diagnostics': True,  # Enable diagnostic information\n",
    "\n",
    "    'core_02': True,  # Enable core_02 information\n",
    "    'bed': True,  # Enable bed n information\n",
    "    'vte_status': True,  # Enable VTE () status information\n",
    "    'hosp_site': True,  # Enable hospital site information\n",
    "    'core_resus': True,  # Enable core resuscitation information\n",
    "    'news': True,  # Enable NEWS (National Early Warning Score) information\n",
    "\n",
    "    'smoking': True,  # Enable smoking-related information\n",
    "    'annotations': True,  # Enable EPR documents annotations via MedCat\n",
    "    'annotations_mrc': True,# Enable MRC (Additional clinical note observations index) annotations via MedCat\n",
    "    'negated_presence_annotations': False, # Enable or disable negated presence annotations\n",
    "    'appointments': False,  # Enable appointments information\n",
    "    'annotations_reports': False,  # Enable reports information\n",
    "    'textual_obs': False,  # Enable textual observations (basic_observations index) annotations via MedCat\n",
    "}\n",
    "\n",
    "# Configuration dictionary for annotation filtering, only base annotations meeting this threshold will be included.\n",
    "annot_filter_arguments = {\n",
    "    'acc': 0.8,  # base concept accuracy\n",
    "    'types': ['qualifier value', 'procedure', 'substance', 'finding', 'environment', 'disorder', 'observable entity'], # umls list of types for medcat filter\n",
    "    # 'types': ['qualifier value', 'procedure', 'substance', 'finding', 'environment', 'disorder', 'observable entity', 'organism', 'phenomenon', 'anatomy', 'conceptual entity', 'physical object', 'intellectual product', 'occupation or discipline', 'mental or behavioral dysfunction', 'geographic area', 'population group', 'biomedical or dental material', 'medical device', 'classification', 'regulation or law', 'health care activity', 'health care related organization', 'professional or occupational group', 'group', 'attribute', 'individual behavior']\n",
    "    \n",
    "    'Time_Value': ['Recent', 'Past'], # Specify the values you want to include in a list. Must be defined in medcat model. # Example ['Recent', 'Past', 'Subject/Experiencer']\n",
    "    'Time_Confidence': 0.8,  # Specify the confidence threshold as a float\n",
    "    'Presence_Value': ['True'], # Specify the values you want to include in a list\n",
    "    'Presence_Confidence': 0.8,  # Specify the confidence threshold as a float\n",
    "    'Subject_Value': ['Patient'], # Specify the values you want to include in a list\n",
    "    'Subject_Confidence': 0.8  # Specify the confidence threshold as a float\n",
    "}\n",
    "\n",
    "# Filter data batches by terms before processing. \n",
    "\n",
    "epr_docs_term_regex: Optional[Union[str, None]] = None\n",
    "mct_docs_term_regex: Optional[Union[str, None]] = None\n",
    "\n",
    "# Example bloods_filter_term_list: Optional[Union[List[str], None]] = ['wbc'] # This will only include basic observations with this item name analysed.\n",
    "bloods_filter_term_list: Optional[Union[List[str], None]] = None\n",
    "\n",
    "# Example mct_docs_document_type_filter_list: Optional[Union[List[str], None]] = ['KHMDC Integrated report'] # This will only include documents with this document type field value.\n",
    "\n",
    "mct_docs_document_type_filter_list: Optional[Union[List[str], None]] = None\n",
    "epr_docs_document_type_filter_list: Optional[Union[List[str], None]] = None\n",
    "\n",
    "data_type_filter_dict: Dict[str, any] = {\n",
    "    'filter_term_lists': {\n",
    "        'epr_docs': epr_docs_document_type_filter_list,\n",
    "        'mct_docs': mct_docs_document_type_filter_list,\n",
    "        'bloods': bloods_filter_term_list\n",
    "    },\n",
    "    'epr_docs_term_regex': epr_docs_term_regex,\n",
    "    'mct_docs_term_regex': mct_docs_term_regex,\n",
    "}\n",
    "\n",
    "#Example date settings:\n",
    "#start_date=(datetime(2020, 1, 1)) Start date for processing\n",
    "\n",
    "# Define the length of the time window, example 1 year and 15 days, only data within this window will be processed.\n",
    "# years=1,      # Number of years to add to the start date \n",
    "# months=0,  # Number of months to add to the start date\n",
    "# days=15,  # Number of days to add to the start date\n",
    "\n",
    "# Define the interval between time windows. Example 1 year. Each vector/row output will be based on this interval.\n",
    "# time_window_interval_delta = relativedelta(years=1)\n",
    "\n",
    "# lookback = True #This determines the direction of the time length window. True = backward, False = forward. Our time window (+1 years, 15 days) is therefore 2020, 1, 1 - 2021, 1, 15. \n",
    "\n",
    "# IPW settings:\n",
    "\n",
    "# Init config obj\n",
    "\n",
    "# Hypothetical date config_obj configuration:\n",
    "# I want all patients data between Feb 2015 and Jul 2020. This date window will extract and create the batched patient data for this time window.\n",
    "\n",
    "# global_start_year=2015, \n",
    "# global_start_month=2,  \n",
    "# global_end_year=2020,  \n",
    "# global_end_month=6, \n",
    "# global_start_day = 1, \n",
    "# global_end_day = 1, \n",
    "\n",
    "# I want patient vectors starting from Feb 2019 to Feb 2020 as I would like to see if X medical event is recorded on those taking medication Y\n",
    "# start_date=(datetime(2019, 2, 1)),  \n",
    "# years=1, \n",
    "# months=0,  \n",
    "# days=0, \n",
    "# lookback = False # 2019 to 2020 is forward in time.\n",
    "# I would like a single vector for each patient\n",
    "# time_window_interval_delta = relativedelta(years=1) \n",
    "# I would like 1 vector per month per patient for the 1 year time window\n",
    "# time_window_interval_delta = relativedelta(months=1)\n",
    "\n",
    "# Creating a configuration object for a specific task or project\n",
    "config_obj = config_class(\n",
    "    remote_dump=False,  # Flag for remote data dumping. partially deprecated.\n",
    "    suffix='',  # Suffix for file names\n",
    "    treatment_doc_filename='test_files/treatment_docs.csv', # Filename for treatment documentation\n",
    "    treatment_control_ratio_n=1,  # Ratio for treatment to control\n",
    "    proj_name='new_project', # Project name. patient data batches and vectors stored here.\n",
    "    current_path_dir=\"\",  # Current path directory\n",
    "    main_options=main_options_dict,  # Dictionary for main options\n",
    "    start_date=(datetime(1995, 1, 1)),  # Starting date for processing\n",
    "    years=30, # Number of years to add to the start date. Set the duration of the time window. Window is defined as the start date + years/months/days set here.\n",
    "    months=0,  # Number of months to add to the start date\n",
    "    days=0,  # Number of days to add to the start date\n",
    "    batch_mode=True,  # Flag for batch processing mode. Only functioning mode.\n",
    "    store_annot=True,  # Flag to store annotations. partially deprecated.\n",
    "    share_sftp=True,  # Flag for sharing via SFTP. partially deprecated\n",
    "    multi_process=False,  # Flag for multi-process execution. deprecated.\n",
    "    strip_list=True, # Flag for stripping lists, this will check for completed patients before starting to avoid redundancy.\n",
    "    verbosity=9,  # Verbosity level 0-9 printing debug messages\n",
    "    random_seed_val=random_seed_value,  # Random seed value for reproducibility of controls.\n",
    "    testing=True,  # Flag for testing mode. Will use dummy data.\n",
    "    dummy_medcat_model=True,  # Flag for dummy MedCAT model, used if testing == True, this will simulate a MedCAT model.\n",
    "    use_controls=False, # If true this will add desired ratio of controls at random from global pool, requires configuring with a master list of patients.\n",
    "    medcat=False, # Flag for MedCAT processing. #will load medcat into memory and use for annotating.\n",
    "    start_time=datetime.now(), # Current timestamp as the start time for logging and progress bar\n",
    "    patient_id_column_name='auto', # Column name for patient ID, auto will try to find it. Example \"client_idcode\"\n",
    "    annot_filter_options=annot_filter_arguments,  # Annotation filtering options\n",
    "    \n",
    "    # Global start year. #set the limits of the time window data can be drawn from. Start should not precede start date set above.\n",
    "    global_start_year=1995, # Global dates are overwritten by individual patient windows to match patient window. # Ensure that global start year/month/day is before end year/month/day\n",
    "    global_start_month=1,  # Global start month\n",
    "    global_end_year=2025,  # Global end year\n",
    "    global_end_month=1, # Global end month\n",
    "    global_start_day = 1, \n",
    "    global_end_day = 1, \n",
    "    ## Use these if each patient has their own individual time window. Requires preparing a table of start dates.\n",
    "    # individual_patient_window = True,\n",
    "    # individual_patient_window_df = pd.read_csv('ipw_overlap.csv'),\n",
    "    # individual_patient_window_start_column_name = 'updatetime_manual_offset',\n",
    "    # individual_patient_id_column_name = 'client_idcode',\n",
    "    # individual_patient_window_controls_method = 'full',\n",
    "    shuffle_pat_list=False,  # Flag for shuffling patient list\n",
    "    time_window_interval_delta = relativedelta(years=30), #specify the time window to collapse each feature vector into, years=1 is one vector per year within the global time window\n",
    "    split_clinical_notes=True, #will split clinical notes by date and treat as individual documents with extracted dates. Requires note splitter module. \n",
    "    lookback = False, # when calculating individual patient window from table of start dates, will calculate backwards in time if true. Else Forwards. When calculating from global start date, will calculate backwards or forwards respectively. \n",
    "    add_icd10 = False, #append icd 10 codes to annot batches. Can be found under current_pat_documents_annotations/%client_idcode%.csv.\n",
    "    add_opc4s=False, # needs icd10 true also. Can be found under current_pat_documents_annotations/%client_idcode%.csv\n",
    "    override_medcat_model_path = path_to_medcat_model_pack, #Force medcat model path, if None uses defaults for env. #Can be set in paths.py with medcat_path = %path to medcat model pack.zip\"\n",
    "    data_type_filter_dict = None, # Dictionary for data type filter, see examples above. \n",
    "    filter_split_notes = True, # If enabled, will reapply global time window filter post clinical note splitting. Recommended to enable if split notes enabled.\n",
    "    prefetch_pat_batches = False, # If enabled, will fetch batches for entire patient list and pre poulate batch folders with individual pat batches. Out of memory issues.\n",
    "    sample_treatment_docs=5 # If int > 0, will sample treatment documents from the treatment_docs.csv file. This is useful for testing and debugging / pilot run purposes.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.main_pat2vec import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj = main( cogstack=True, use_filter=False,\n",
    "             json_filter_path = None, random_seed_val=42, \n",
    "             hostname =None, config_obj= config_obj, )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View patient list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj.all_patient_list[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj.config_obj.date_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make pat vectors for pat 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj.pat_maker(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove specific patient raw documents and annotations:\n",
    "from pat2vec.util.post_processing import remove_file_from_paths\n",
    "\n",
    "# remove_file_from_paths(pat2vec_obj.all_patient_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the maximum number of retries\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "# Iterate through the patient list starting from index 0\n",
    "for i in tqdm(range(0, len(pat2vec_obj.all_patient_list))):\n",
    "    retries = 0\n",
    "    success = False\n",
    "    \n",
    "    while retries < MAX_RETRIES and not success:\n",
    "        try:\n",
    "            # Try to process the patient\n",
    "            pat2vec_obj.pat_maker(i)\n",
    "            success = True  # Mark as successful if no exception is raised\n",
    "            \n",
    "        except KeyError as e:\n",
    "            # Handle specific exception\n",
    "            print(f\"KeyError at index {i}: {e}. Retrying after removal...\")\n",
    "            remove_file_from_paths(pat2vec_obj.all_patient_list[i])\n",
    "            retries += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Handle generic exceptions\n",
    "            print(f\"Exception at index {i}: {e}. Skipping this patient...\")\n",
    "            break  # Break the retry loop for non-retryable exceptions\n",
    "            \n",
    "        finally:\n",
    "            pat2vec_obj.t.update(1)  # Update progress\n",
    "\n",
    "    if not success:\n",
    "        print(f\"Failed to process index {i} after {MAX_RETRIES} retries.\")\n",
    "\n",
    "pat2vec_obj.t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = f'{pat2vec_obj.proj_name}/current_pat_lines_parts' # Patient vectors are stored individually in this directory. \n",
    "output_csv_file = 'output_file.csv'\n",
    "\n",
    "# Specify the directory where you want to create the file\n",
    "directory = pat2vec_obj.proj_name + '/output_directory'\n",
    "\n",
    "# Check if the directory exists, if not, create it\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# We will join the individual patient vectors into a single output file. This is useful for filtering.\n",
    "output_csv_file_filename = process_csv_files(input_directory, out_folder=directory, output_filename_suffix=output_csv_file, part_size=336)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_csv_file_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_datetime_to_column(df)['extracted_datetime_stamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_datetime_to_column(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build all document batches dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will merge all document source batches into a single file. This is useful for filtering. May produce a large file.\n",
    "\n",
    "from pat2vec.util.post_processing_build_methods import build_merged_epr_mct_doc_df\n",
    "\n",
    "all_pat_list = pat2vec_obj.all_patient_list\n",
    "\n",
    "dfd = build_merged_epr_mct_doc_df(all_pat_list, pat2vec_obj.config_obj, overwrite=True)\n",
    "\n",
    "#dfd = pd.read_csv(dfd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build all annotation batches dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will merge all annotation source batches into a single file. This is useful for filtering. May produce a large file.\n",
    "\n",
    "from pat2vec.util.post_processing_build_methods import build_merged_epr_mct_annot_df\n",
    "\n",
    "all_pat_list = pat2vec_obj.all_patient_list\n",
    "\n",
    "dfa = build_merged_epr_mct_annot_df(all_pat_list, pat2vec_obj.config_obj, overwrite=True)\n",
    "\n",
    "dfa = pd.read_csv(dfa)\n",
    "\n",
    "dfa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build additional batches from individual patient data batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will merge all drug source batches into a single file. This is useful for filtering. May produce a large file.\n",
    "\n",
    "from pat2vec.util.post_processing_build_methods import merge_drugs_csv\n",
    "\n",
    "all_pat_list = pat2vec_obj.all_patient_list\n",
    "\n",
    "merged_drugs_path = merge_drugs_csv(all_pat_list, pat2vec_obj.config_obj, overwrite=True)\n",
    "\n",
    "merged_drugs = pd.read_csv(merged_drugs_path)\n",
    "merged_drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfmdi = pd.read_csv('new_project/merged_input_pat_batches/merged_drugs_batches.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in dfmdi.select_dtypes(exclude=[np.number]).columns:\n",
    "#     assert dfmdi[col].astype(str).equals(merged_drugs[col].astype(str)), f\"Mismatch in column: {col}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will merge all diagnostics source batches into a single file. This is useful for filtering. May produce a large file.\n",
    "\n",
    "from pat2vec.util.post_processing_build_methods import merge_diagnostics_csv\n",
    "\n",
    "all_pat_list = pat2vec_obj.all_patient_list\n",
    "\n",
    "merged_diagnostics_path = merge_diagnostics_csv(all_pat_list, pat2vec_obj.config_obj, overwrite=True)\n",
    "\n",
    "merged_diagnostics = pd.read_csv(merged_diagnostics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.post_processing_build_methods import merge_news_csv\n",
    "\n",
    "all_pat_list = pat2vec_obj.all_patient_list\n",
    "\n",
    "merged_news_path = merge_news_csv(all_pat_list, pat2vec_obj.config_obj, overwrite=True)\n",
    "\n",
    "#merged_news = pd.read_csv(merged_news_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.post_processing_build_methods import merge_bmi_csv\n",
    "\n",
    "all_pat_list = pat2vec_obj.all_patient_list\n",
    "\n",
    "merged_bmi_path = merge_bmi_csv(all_pat_list, pat2vec_obj.config_obj, overwrite=True)\n",
    "\n",
    "#merged_bmi = pd.read_csv(merged_bmi_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.post_processing_build_methods import build_merged_bloods\n",
    "\n",
    "all_pat_list = pat2vec_obj.all_patient_list\n",
    "\n",
    "merged_bloods_path = build_merged_bloods(all_pat_list, pat2vec_obj.config_obj, overwrite=True)\n",
    "\n",
    "merged_bloods = pd.read_csv(merged_bloods_path)\n",
    "merged_bloods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_csv('new_project/merged_input_pat_batches/merged_bloods_batches.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.post_processing_build_methods import merge_demographics_csv\n",
    "\n",
    "all_pat_list = pat2vec_obj.all_patient_list\n",
    "\n",
    "merged_demographics_path = merge_demographics_csv(all_pat_list, pat2vec_obj.config_obj, overwrite=True)\n",
    "\n",
    "merged_demographics = pd.read_csv(merged_demographics_path)\n",
    "\n",
    "merged_demographics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the annotation batches by a snomed cui and its related codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snomed_methods import snomed_methods_v1\n",
    "\n",
    "snomed_methods_example = False\n",
    "\n",
    "if(snomed_methods_example):\n",
    "\n",
    "    path_to_sct2 = path_to_snomed_ct_file\n",
    "\n",
    "    medcat_path = path_to_medcat_model_pack\n",
    "\n",
    "    snomed_relations_obj = snomed_methods_v1.snomed_relations(medcat=True, snomed_rf2_full_path=path_to_sct2,\n",
    "                                                            medcat_path = medcat_path)\n",
    "    outcome_variable_cui_for_filter = '40733004'  # infection\n",
    "\n",
    "    print(outcome_variable_cui_for_filter)\n",
    "\n",
    "    filter_root_cui = outcome_variable_cui_for_filter\n",
    "    print(filter_root_cui)\n",
    "\n",
    "    retrieved_codes_snomed_tree, retrieved_names_snomed_tree = snomed_relations_obj.recursive_code_expansion(filter_root_cui, n_recursion = 3, debug=False)\n",
    "\n",
    "    print(retrieved_codes_snomed_tree[0:5], len(retrieved_codes_snomed_tree), len(retrieved_names_snomed_tree))\n",
    "\n",
    "    print(retrieved_names_snomed_tree[0:10])\n",
    "    \n",
    "    retrieved_codes_medcat_cdb, retrieved_names_medcat_cdb  = snomed_relations_obj.get_medcat_cdb_most_similar(filter_root_cui, context_type = 'xxxlong', type_id_filter=[], topn=25)\n",
    "    \n",
    "    print(retrieved_names_medcat_cdb[0:10])\n",
    "    \n",
    "    all_names_list = list(set(retrieved_names_medcat_cdb + retrieved_names_snomed_tree))\n",
    "\n",
    "    all_codes_list = list(set(retrieved_codes_medcat_cdb + retrieved_codes_snomed_tree))\n",
    "\n",
    "    print(len(all_names_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply misc methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pat2vec.all_methods import pat2vec_methods\n",
    "\n",
    "# p2v = pat2vec_methods()\n",
    "\n",
    "# p2v.produce_filtered_annotation_dataframe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build IPW dataframe\n",
    "\n",
    "\n",
    "Find the latest/earliest record for one of [268910001, 62315008, 55822004, 49727002]\n",
    "\n",
    "We can use this in another main block with:\n",
    "\n",
    "individual_patient_window = True,\n",
    "\n",
    "individual_patient_window_df = pd.read_csv('ipw_overlap.csv'),\n",
    "\n",
    "individual_patient_window_start_column_name = 'updatetime_manual_offset',\n",
    "\n",
    "individual_patient_id_column_name = 'client_idcode',\n",
    "\n",
    "individual_patient_window_controls_method = 'full', \n",
    "\n",
    "To limit each patients data to a specific individual time window. With controls we can match the time window per control or pull their 'full' data for the global time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_filter_arguments = {\n",
    "    'acc': 0.6,  # base concept accuracy\n",
    "    # umls list of types for medcat filter\n",
    "    #'types': ['qualifier value', 'procedure', 'substance', 'finding', 'environment', 'disorder', 'observable entity'],\n",
    "     'types': ['qualifier value', 'procedure', 'substance', 'finding', 'environment', 'disorder', 'observable entity', 'organism', 'phenomenon', 'anatomy', 'conceptual entity', 'physical object', 'intellectual product', 'occupation or discipline', 'mental or behavioral dysfunction', 'geographic area', 'population group', 'biomedical or dental material', 'medical device', 'classification', 'regulation or law', 'health care activity', 'health care related organization', 'professional or occupational group', 'group', 'attribute', 'individual behavior'],\n",
    "    # Specify the values you want to include in a list. Must be defined in medcat model.\n",
    "    'Time_Value': ['Recent', 'Past'],\n",
    "    'Time_Confidence': 0.6,  # Specify the confidence threshold as a float\n",
    "    # Specify the values you want to include in a list\n",
    "    'Presence_Value': ['True'],\n",
    "    'Presence_Confidence': 0.6,  # Specify the confidence threshold as a float\n",
    "    # Specify the values you want to include in a list\n",
    "    'Subject_Value': ['Patient'],\n",
    "    'Subject_Confidence': 0.6  # Specify the confidence threshold as a float\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(f'new_project/current_pat_document_batches/{pat2vec_obj.all_patient_list[0]}.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pat2vec_obj.all_patient_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(f'new_project/current_pat_documents_annotations_batches/{pat2vec_obj.all_patient_list[0]}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.post_processing import build_ipw_dataframe\n",
    "\n",
    "build_ipw_dataframe(annot_filter_arguments=annot_filter_arguments, config_obj=pat2vec_obj.config_obj, filter_codes=[38341003, 274640006, 886731000000109,268910001, 62315008, 55822004, 49727002, 22232009], mode='latest', include_mct=True, include_textual_obs=False) # '62315008', '55822004', '268910001',\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.post_processing import build_ipw_dataframe\n",
    "\n",
    "build_ipw_dataframe(annot_filter_arguments=annot_filter_arguments, config_obj=pat2vec_obj.config_obj, filter_codes=[38341003, 274640006, 268910001, 62315008, 55822004, 49727002, 248153007], mode='earliest' , include_mct=True, include_textual_obs=False) # '62315008', '55822004', '268910001',\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine and screen the patient client_idcode list for malformed entries \n",
    "\n",
    "from pat2vec.pat2vec_pat_list.get_patient_treatment_list import analyze_client_codes\n",
    "\n",
    "#valid_codes, invalid_codes, clusters = analyze_client_codes(pat2vec_obj.all_patient_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic integrity checks:\n",
    "\n",
    "example_pat_res = pd.read_csv(f'new_project/current_pat_documents_annotations_batches/{pat2vec_obj.all_patient_list[0]}.csv')\n",
    "\n",
    "example_pat_res.shape == (4, 26), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_pat_res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert example_pat_res.shape == (1, 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(treatment_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_docs = pd.read_csv('test_files/treatment_docs.csv')\n",
    "#assert len(treatment_docs) == 23\n",
    "print(len(treatment_docs)==23)\n",
    "treatment_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert treatment_docs['basicobs_itemname_analysed'].iloc[21] == 'Parathyroid Hormone (PTH)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(treatment_docs['body_analysed'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'acrylic head' in str(treatment_docs['body_analysed'].iloc[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_example_annot = pd.read_csv('new_project/current_pat_documents_annotations_batches/P0IFD0TV.csv')\n",
    "\n",
    "pat_example_annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert pat_example_annot['cui'].iloc[0] == 38341003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test push protection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test push protection 3x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPW demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build IPW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_filter_arguments = {\n",
    "    'acc': 0.1,  # base concept accuracy\n",
    "    # umls list of types for medcat filter\n",
    "    'types': ['qualifier value', 'procedure', 'substance', 'finding', 'environment', 'disorder', 'observable entity'],\n",
    "    # 'types': ['qualifier value', 'procedure', 'substance', 'finding', 'environment', 'disorder', 'observable entity', 'organism', 'phenomenon', 'anatomy', 'conceptual entity', 'physical object', 'intellectual product', 'occupation or discipline', 'mental or behavioral dysfunction', 'geographic area', 'population group', 'biomedical or dental material', 'medical device', 'classification', 'regulation or law', 'health care activity', 'health care related organization', 'professional or occupational group', 'group', 'attribute', 'individual behavior']\n",
    "    # Specify the values you want to include in a list. Must be defined in medcat model.\n",
    "    'Time_Value': ['Recent', 'Past'],\n",
    "    'Time_Confidence': 0.1,  # Specify the confidence threshold as a float\n",
    "    # Specify the values you want to include in a list\n",
    "    'Presence_Value': ['True'],\n",
    "    'Presence_Confidence': 0.1,  # Specify the confidence threshold as a float\n",
    "    # Specify the values you want to include in a list\n",
    "    'Subject_Value': ['Patient'],\n",
    "    'Subject_Confidence': 0.1  # Specify the confidence threshold as a float\n",
    "}\n",
    "\n",
    "pd.read_csv(f'new_project/current_pat_document_batches/{pat2vec_obj.all_patient_list[0]}.csv').head()\n",
    "len(pat2vec_obj.all_patient_list)\n",
    "pd.read_csv(f'new_project/current_pat_documents_annotations_batches/{pat2vec_obj.all_patient_list[0]}.csv').head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select two cui to simulate condition\n",
    "\n",
    "dfa_s = pd.read_csv('new_project/merged_batches/annots_mct_epr.csv')\n",
    "\n",
    "dfa_s.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using these two cui codes as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "# Group the data so we have a set of CUIs for each client\n",
    "client_cui_map = (\n",
    "    dfa_s.groupby('client_idcode')['cui']\n",
    "    .apply(set)\n",
    ")\n",
    "\n",
    "# Create all unique pairs of CUIs\n",
    "all_cuis = pd.Series(itertools.chain.from_iterable(client_cui_map)).unique()\n",
    "pairs = itertools.combinations(all_cuis, 2)\n",
    "\n",
    "# Count how many clients have both CUIs for each pair\n",
    "pair_counts = []\n",
    "for cui1, cui2 in pairs:\n",
    "    count = sum({cui1, cui2}.issubset(cui_set) for cui_set in client_cui_map)\n",
    "    pair_counts.append(((cui1, cui2), count))\n",
    "\n",
    "# Find the pair with the maximum co-occurrence\n",
    "most_common_pair, max_count = max(pair_counts, key=lambda x: x[1])\n",
    "\n",
    "print(f\"Most common co-occurring pair: {most_common_pair} with {max_count} clients having both.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_A_pretty_name = dfa[dfa['cui']==int(most_common_pair[0])]['pretty_name'].iloc[0]\n",
    "\n",
    "concept_B_pretty_name = dfa[dfa['cui']==int(most_common_pair[1])]['pretty_name'].iloc[0]\n",
    "\n",
    "concept_A_pretty_name, concept_B_pretty_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_A_filter_codes = [int(most_common_pair[0])]\n",
    "concept_B_filter_codes = [int(most_common_pair[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_obj.verbosity = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the earliest occurrence of any CUI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.post_processing import build_ipw_dataframe\n",
    "\n",
    "file_path = 'ipw_dataframe.csv'\n",
    "overwrite = True  \n",
    "skip_ipw_build = False\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    if overwrite:\n",
    "        pd.set_option('display.max_columns',None)\n",
    "\n",
    "        #n.b this needs filter annot arguments...\n",
    "        ipw_dataframe = build_ipw_dataframe(annot_filter_arguments=annot_filter_arguments, config_obj=pat2vec_obj.config_obj, filter_codes=concept_A_filter_codes + concept_B_filter_codes, mode='earliest', include_mct=False, include_textual_obs=False) # '62315008', '55822004', '268910001',\n",
    "        ipw_dataframe.to_csv(file_path)\n",
    "        ipw_dataframe\n",
    "        # Proceed with overwriting the file\n",
    "        print(\"File exists and will be overwritten.\")\n",
    "    else:\n",
    "        # Skip or handle the existing file\n",
    "        ipw_dataframe = pd.read_csv('ipw_dataframe.csv')\n",
    "        print(\"File exists and will NOT be overwritten.\")\n",
    "else:\n",
    "    # File does not exist, safe to proceed\n",
    "    print(\"File does not exist, safe to proceed.\")\n",
    "    \n",
    "    pd.set_option('display.max_columns',None)\n",
    "\n",
    "    #n.b this needs filter annot arguments...\n",
    "    ipw_dataframe = build_ipw_dataframe(annot_filter_arguments=annot_filter_arguments, config_obj=pat2vec_obj.config_obj, filter_codes=concept_A_filter_codes + concept_B_filter_codes, mode='earliest', include_mct=False, include_textual_obs=False) # '62315008', '55822004', '268910001',\n",
    "    ipw_dataframe.to_csv(file_path)\n",
    "    ipw_dataframe\n",
    "\n",
    "ipw_dataframe.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additionally filter by only those who had both of the cui coocurring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pat2vec.util.post_processing import filter_annot_dataframe2\n",
    "\n",
    "\n",
    "annot_batch_file_path = 'new_project/merged_batches/annots_mct_epr.csv'\n",
    "\n",
    "if not skip_ipw_build:\n",
    "    # Initialize an empty set to store client_idcodes that meet both conditions\n",
    "    true_clients_set = set()\n",
    "\n",
    "    # Process the DataFrame in chunks\n",
    "    for chunk in pd.read_csv(annot_batch_file_path, chunksize=100000):\n",
    "\n",
    "        # filter annotations by earlier annotation filter arguments\n",
    "        chunk = filter_annot_dataframe2(chunk, annot_filter_arguments)\n",
    "\n",
    "        # Group the chunk by client_idcode and check conditions\n",
    "        grouped_chunk = chunk.groupby('client_idcode')['cui'].agg(\n",
    "            lambda x: any(x.isin(concept_A_filter_codes)) and any(x.isin(concept_B_filter_codes)))\n",
    "        \n",
    "        # Add clients that meet both conditions in this chunk to our set\n",
    "        true_in_chunk = grouped_chunk[grouped_chunk].index.tolist()\n",
    "        true_clients_set.update(true_in_chunk)\n",
    "\n",
    "    # Convert the set to a list\n",
    "    true_clients = list(true_clients_set)\n",
    "\n",
    "    print(f\"Found {len(true_clients)} patients with both {concept_A_filter_codes} and {concept_B_filter_codes}\")\n",
    "    print(true_clients)\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "# finally filter the IPW by the true clients with concept_A_filter_codes and concept_B_filter_codes\n",
    "if not skip_ipw_build:\n",
    "    ipw_dataframe = ipw_dataframe[ipw_dataframe['client_idcode'].isin(true_clients)]\n",
    "\n",
    "    ipw_dataframe\n",
    "if not skip_ipw_build:\n",
    "    ipw_dataframe.reset_index(drop=True, inplace=True)\n",
    "if not skip_ipw_build:\n",
    "    ipw_dataframe.to_csv('ipw_dataframe.csv')\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "if not skip_ipw_build:\n",
    "    # Convert to datetime and ensure all values are timezone-aware in UTC\n",
    "    ipw_dataframe['updatetime'] = pd.to_datetime(\n",
    "    ipw_dataframe['updatetime'], #format='ISO8601',\n",
    "    utc=True\n",
    ")\n",
    "\n",
    "\n",
    "    # Subtract 3 months using pd.DateOffset, this is a buffer between the first mention of the concept and our new individual patient start time/ time window. \n",
    "    ipw_dataframe['updatetime_offset'] = ipw_dataframe['updatetime'] - pd.DateOffset(months=3)\n",
    "\n",
    "    ipw_dataframe['updatetime_offset'] = pd.to_datetime(ipw_dataframe['updatetime_offset'], format='ISO8601', utc=True)\n",
    "\n",
    "\n",
    "    ipw_dataframe.to_csv('ipw_dataframe.csv')\n",
    "\n",
    "    ipw_dataframe\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipw_dataframe.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "from pat2vec.util.config_pat2vec import config_class\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from pat2vec.util.post_processing import process_csv_files\n",
    "from pat2vec.util.post_processing import extract_datetime_to_column\n",
    "from pat2vec.pat2vec_pat_list.get_patient_treatment_list import get_all_patients_list\n",
    "from pat2vec.util.post_processing import produce_filtered_annotation_dataframe\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "# Configuration dictionary for main options in a medical application\n",
    "main_options_dict = {\n",
    "    # Enable demographic information (Ethnicity mapped to UK census, age, death)\n",
    "    'demo': True,\n",
    "    'bmi': True,  # Enable BMI (Body Mass Index) tracking\n",
    "    'bloods': True,  # Enable blood-related information\n",
    "    'drugs': True,  # Enable drug-related information\n",
    "    'diagnostics': True,  # Enable diagnostic information\n",
    "\n",
    "    'core_02': True,  # Enable core_02 information\n",
    "    'bed': True,  # Enable bed n information\n",
    "    'vte_status': True,  # Enable VTE () status tracking\n",
    "    'hosp_site': True,  # Enable hospital site information\n",
    "    'core_resus': True,  # Enable core resuscitation information\n",
    "    'news': True,  # Enable NEWS (National Early Warning Score) tracking\n",
    "\n",
    "    'smoking': True,  # Enable smoking-related information\n",
    "    'annotations': True,  # Enable EPR annotations\n",
    "    # Enable MRC (Additional clinical note observations index) annotations\n",
    "    'annotations_mrc': True,\n",
    "    # Enable or disable negated presence annotations\n",
    "    'negated_presence_annotations': False,\n",
    "    'appointments': True,  # Enable appointments\n",
    "    'annotations_reports': False,  # Enable reports\n",
    "    'textual_obs': True,  # Enable textual observations (basic_observations index)\n",
    "}\n",
    "\n",
    "\n",
    "annot_filter_arguments = {\n",
    "    'acc': 0.8,  # base concept accuracy\n",
    "    # umls list of types for medcat filter\n",
    "    'types': ['qualifier value', 'procedure', 'substance', 'finding', 'environment', 'disorder', 'observable entity'],\n",
    "    # 'types': ['qualifier value', 'procedure', 'substance', 'finding', 'environment', 'disorder', 'observable entity', 'organism', 'phenomenon', 'anatomy', 'conceptual entity', 'physical object', 'intellectual product', 'occupation or discipline', 'mental or behavioral dysfunction', 'geographic area', 'population group', 'biomedical or dental material', 'medical device', 'classification', 'regulation or law', 'health care activity', 'health care related organization', 'professional or occupational group', 'group', 'attribute', 'individual behavior']\n",
    "    # Specify the values you want to include in a list. Must be defined in medcat model.\n",
    "    'Time_Value': ['Recent', 'Past'],\n",
    "    'Time_Confidence': 0.8,  # Specify the confidence threshold as a float\n",
    "    # Specify the values you want to include in a list\n",
    "    'Presence_Value': ['True'],\n",
    "    'Presence_Confidence': 0.8,  # Specify the confidence threshold as a float\n",
    "    # Specify the values you want to include in a list\n",
    "    'Subject_Value': ['Patient'],\n",
    "    'Subject_Confidence': 0.8  # Specify the confidence threshold as a float\n",
    "}\n",
    "\n",
    "# Filter data batches by terms before processing. \n",
    "\n",
    "epr_docs_term_regex: Optional[Union[str, None]] = None\n",
    "mct_docs_term_regex: Optional[Union[str, None]] = None\n",
    "\n",
    "# Example bloods_filter_term_list: Optional[Union[List[str], None]] = ['wbc']\n",
    "bloods_filter_term_list: Optional[Union[List[str], None]] = None\n",
    "\n",
    "# Example mct_docs_document_type_filter_list: Optional[Union[List[str], None]] = ['KHMDC Integrated report']\n",
    "\n",
    "mct_docs_document_type_filter_list: Optional[Union[List[str], None]] = None\n",
    "epr_docs_document_type_filter_list: Optional[Union[List[str], None]] = None\n",
    "\n",
    "data_type_filter_dict: Dict[str, any] = {\n",
    "    'filter_term_lists': {\n",
    "        'epr_docs': epr_docs_document_type_filter_list,\n",
    "        'mct_docs': mct_docs_document_type_filter_list,\n",
    "        'bloods': bloods_filter_term_list\n",
    "    },\n",
    "    'epr_docs_term_regex': epr_docs_term_regex,\n",
    "    'mct_docs_term_regex': mct_docs_term_regex,\n",
    "}\n",
    "\n",
    "#Example date settings:\n",
    "#start_date=(datetime(2020, 1, 1)) Start date for processing\n",
    "\n",
    "# Define the length of the time window, example 1 year and 15 days, only data within this window will be processed.\n",
    "# years=1,      # Number of years to add to the start date \n",
    "# months=0,  # Number of months to add to the start date\n",
    "# days=15,  # Number of days to add to the start date\n",
    "\n",
    "# Define the interval between time windows. Example 1 year. Each vector/row output will be based on this interval.\n",
    "# time_window_interval_delta = relativedelta(years=1)\n",
    "\n",
    "# lookback = True #This determines the direction of the time length window. True = backward, False = forward. Our time window (+1 years, 15 days) is therefore 2020, 1, 1 - 2021, 1, 15. \n",
    "\n",
    "# IPW settings:\n",
    "\n",
    "# Init config obj\n",
    "\n",
    "# Creating a configuration object for a specific task or project\n",
    "config_obj = config_class(\n",
    "    remote_dump=False,  # Flag for remote data dumping. partially deprecated.\n",
    "    suffix='',  # Suffix for file names\n",
    "    # Filename for treatment documentation\n",
    "    treatment_doc_filename='treatment_docs.csv',\n",
    "    treatment_control_ratio_n=1,  # Ratio for treatment to control\n",
    "    # Project name. patient data batches and vectors stored here.\n",
    "    proj_name='new_project_ipw',\n",
    "    current_path_dir=\"\",  # Current path directory\n",
    "    main_options=main_options_dict,  # Dictionary for main options\n",
    "    start_date=(datetime(1995, 1, 1)),  # Starting date for processing\n",
    "    # Number of years to add to the start date. Set the duration of the time window. Window is defined as the start date + years/months/days set here.\n",
    "    years=30,\n",
    "    months=0,  # Number of months to add to the start date\n",
    "    days=0,  # Number of days to add to the start date\n",
    "    batch_mode=True,  # Flag for batch processing mode. only functioning mode.\n",
    "    store_annot=True,  # Flag to store annotations. partially deprecated.\n",
    "    share_sftp=True,  # Flag for sharing via SFTP. partially deprecated\n",
    "    multi_process=False,  # Flag for multi-process execution. deprecated.\n",
    "    #annot_first=False,  # Flag for annotation priority. deprecated.\n",
    "    # Flag for stripping lists, will check for completed patients before starting to avoid redundancy.\n",
    "    strip_list=True,\n",
    "    verbosity=0,  # Verbosity level 0-9 printing debug messages\n",
    "    random_seed_val=random_seed_value,  # Random seed value for reproducibility of controls.\n",
    "    testing=True,  # Flag for testing mode\n",
    "    dummy_medcat_model=True,  # Flag for dummy MedCAT model, used if testing == True\n",
    "    # Flag for using controls. #will add desired ratio of controls at random from global pool.\n",
    "    use_controls=False,\n",
    "    # Flag for MedCAT processing. #will load medcat into memory and use for annotating.\n",
    "    medcat=False,\n",
    "    # Current timestamp as the start time for logging and progress bar\n",
    "    start_time=datetime.now(),\n",
    "    # Column name for patient ID, auto will try to find it. Example \"client_idcode\"\n",
    "    patient_id_column_name='client_idcode',\n",
    "    annot_filter_options=annot_filter_arguments,  # Annotation filtering options\n",
    "    # Global start year. #set the limits of the time window data can be drawn from. Start should not precede start date set above.\n",
    "    global_start_year=1995, # Global dates are overwritten by individual patient windows to match patient window. # Ensure that global start year/month/day is before end year/month/day\n",
    "    global_start_month=1,  # Global start month\n",
    "    global_end_year=2023,  # Global end year\n",
    "    global_end_month=1, # Global end month\n",
    "    global_start_day = 1, \n",
    "    global_end_day = 1, \n",
    "    individual_patient_window = True,\n",
    "    individual_patient_window_df = pd.read_csv('ipw_dataframe.csv'),\n",
    "    individual_patient_window_start_column_name = 'updatetime', #_offset , this will look for your start column name + '_offset'\n",
    "    individual_patient_id_column_name = 'client_idcode',\n",
    "    individual_patient_window_controls_method = 'full',\n",
    "    shuffle_pat_list=False,  # Flag for shuffling patient list\n",
    "    time_window_interval_delta = relativedelta(years=30), #specify the time window to collapse each feature vector into, years=1 is one vector per year within the global time window\n",
    "    split_clinical_notes=True, #will split clinical notes by date and treat as individual documents with extracted dates. Requires note splitter module. \n",
    "    lookback = True, # when calculating individual patient window from table of start dates, will calculate backwards in time if true. Else Forwards. When calculating from global start date, will calculate backwards or forwards respectively. \n",
    "    add_icd10 = False, #append icd 10 codes to annot batches. Can be found under current_pat_documents_annotations/%client_idcode%.csv.\n",
    "    add_opc4s=False, # needs icd10 true also. Can be found under current_pat_documents_annotations/%client_idcode%.csv\n",
    "    override_medcat_model_path = path_to_medcat_model_pack, #Force medcat model path, if None uses defaults for env. #Can be set in paths.py with medcat_path = %path to medcat model pack.zip\"\n",
    "    data_type_filter_dict = None, # Dictionary for data type filter, see examples above. \n",
    "    filter_split_notes = True # If enabled, will reapply global time window filter post clinical note splitting. Recommended to enable if split notes enabled.\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_obj.individual_patient_window_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_obj.time_window_interval_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_obj.patient_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj = main( cogstack=True, use_filter=False,\n",
    "             json_filter_path = None, random_seed_val=42, \n",
    "             hostname =None, config_obj= config_obj, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the maximum number of retries\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "# Iterate through the patient list starting from index 0\n",
    "for i in tqdm(range(0, len(pat2vec_obj.all_patient_list))):\n",
    "    retries = 0\n",
    "    success = False\n",
    "    \n",
    "    while retries < MAX_RETRIES and not success:\n",
    "        try:\n",
    "            # Try to process the patient\n",
    "            pat2vec_obj.pat_maker(i)\n",
    "            success = True  # Mark as successful if no exception is raised\n",
    "            \n",
    "        except KeyError as e:\n",
    "            # Handle specific exception\n",
    "            print(f\"KeyError at index {i}: {e}. Retrying after removal...\")\n",
    "            remove_file_from_paths(pat2vec_obj.all_patient_list[i])\n",
    "            retries += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Handle generic exceptions\n",
    "            print(f\"Exception at index {i}: {e}. Skipping this patient...\")\n",
    "            raise e\n",
    "            break  # Break the retry loop for non-retryable exceptions\n",
    "            \n",
    "        finally:\n",
    "            pat2vec_obj.t.update(1)  # Update progress\n",
    "\n",
    "    if not success:\n",
    "        print(f\"Failed to process index {i} after {MAX_RETRIES} retries.\")\n",
    "\n",
    "pat2vec_obj.t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(pat2vec_obj.config_obj.global_start_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(pat2vec_obj.config_obj.global_end_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj.config_obj.global_start_day, pat2vec_obj.config_obj.global_end_day, pat2vec_obj.config_obj.global_start_month, pat2vec_obj.config_obj.global_end_month, pat2vec_obj.config_obj.global_start_year, pat2vec_obj.config_obj.global_end_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = f'{pat2vec_obj.proj_name}/current_pat_lines_parts' # Patient vectors are stored individually in this directory. \n",
    "output_csv_file = 'output_file.csv'\n",
    "\n",
    "# Specify the directory where you want to create the file\n",
    "directory = pat2vec_obj.proj_name + '/output_directory'\n",
    "\n",
    "# Check if the directory exists, if not, create it\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# We will join the individual patient vectors into a single output file. This is useful for filtering.\n",
    "output_csv_file_filename = process_csv_files(input_directory, out_folder=directory, output_filename_suffix=output_csv_file, part_size=336)\n",
    "df = pd.read_csv(output_csv_file_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2vec_obj.all_patient_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_obj.individual_patient_window_df[config_obj.individual_patient_window_df['client_idcode']==pat2vec_obj.all_patient_list[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pat2vec_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
