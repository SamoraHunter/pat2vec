{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aliencat = False\n",
    "dgx = True\n",
    "\n",
    "\n",
    "batch_mode = True\n",
    "remote_dump = True\n",
    "\n",
    "negated_presence_annotations = True\n",
    "store_annot = True\n",
    "\n",
    "share_sftp = True\n",
    "multi_process = True\n",
    "\n",
    "annot_first = True\n",
    "strip_list = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# time.sleep(86000 * 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "#%%javascript\n",
    "#IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install 'paramiko-3.1.0-py3-none-any.whl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import paramiko\n",
    "from os.path import exists\n",
    "import random\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# nb_full_path = os.path.join(os.getcwd(), nb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_name = 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not aliencat or dgx):\n",
    "\n",
    "    import sys\n",
    "    import logging\n",
    "\n",
    "    nblog = open(f\"{nb_name}.log\", \"w\")\n",
    "    nblog = open(f\"{nb_name}.log\", \"a+\")\n",
    "    sys.stdout.echo = nblog\n",
    "    sys.stderr.echo = nblog\n",
    "\n",
    "    get_ipython().log.handlers[0].stream = nblog\n",
    "    get_ipython().log.setLevel(logging.INFO)\n",
    "\n",
    "    get_ipython().run_line_magic('autosave', '5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import trange\n",
    "from colorama import Fore, Style\n",
    "\n",
    "color_bars = [\n",
    "    Fore.RED,\n",
    "    Fore.GREEN,\n",
    "    Fore.BLUE,\n",
    "    Fore.MAGENTA,\n",
    "    Fore.YELLOW,\n",
    "    Fore.CYAN,\n",
    "    Fore.WHITE,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.utils import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# import tqdm\n",
    "import sys\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"/home/aliencat/samora/gloabl_files\")\n",
    "sys.path.insert(0, \"/data/AS/Samora/gloabl_files\")\n",
    "# from cogstack_v8 import *\n",
    "from cogstack_v8_lite import *\n",
    "from credentials import *\n",
    "\n",
    "# import cogstack_v8\n",
    "from COGStats import *\n",
    "from scipy import stats\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "\n",
    "def convert_date(date_string):\n",
    "    date_string = date_string.split(\"T\")[0]\n",
    "    date_object = datetime.strptime(date_string, \"%Y-%m-%d\")\n",
    "    return date_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_free_gpu():\n",
    "    gpu_stats = subprocess.check_output(\n",
    "        [\"nvidia-smi\", \"--format=csv\", \"--query-gpu=memory.used,memory.free\"]\n",
    "    )\n",
    "    gpu_df = pd.read_csv(\n",
    "        StringIO(gpu_stats.decode(\"utf-8\")),\n",
    "        names=[\"memory.used\", \"memory.free\"],\n",
    "        skiprows=1,\n",
    "    )\n",
    "    print(\"GPU usage:\\n{}\".format(gpu_df))\n",
    "    gpu_df[\"memory.free\"] = gpu_df[\"memory.free\"].map(lambda x: x.rstrip(\" [MiB]\"))\n",
    "    idx = gpu_df[\"memory.free\"].astype(int).idxmax()\n",
    "    print(\n",
    "        \"Returning GPU{} with {} free MiB\".format(idx, gpu_df.iloc[idx][\"memory.free\"])\n",
    "    )\n",
    "    return int(idx), gpu_df.iloc[idx][\"memory.free\"]\n",
    "\n",
    "\n",
    "import subprocess\n",
    "from io import StringIO\n",
    "\n",
    "gpu_index, free_mem = get_free_gpu()\n",
    "\n",
    "if int(free_mem) > 4000:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_index)\n",
    "    print(f\"Setting gpu with {free_mem} free\")\n",
    "else:\n",
    "    print(f\"Setting NO gpu, most free memory: {free_mem} !\")\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medcat.cat import CAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suffix='_time'\n",
    "suffix = \"\"\n",
    "\n",
    "\n",
    "treatment_doc_filename = \"treatment_docs_v3.csv\"\n",
    "treatment_control_ratio_n = 1  # 1:n\n",
    "pre_annotation_path = f\"current_pat_annots_parts{suffix}/\"\n",
    "pre_annotation_path_mrc = f\"current_pat_annots_mrc_parts{suffix}/\"\n",
    "\n",
    "\n",
    "print(pre_annotation_path)\n",
    "print(pre_annotation_path_mrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not remote_dump:\n",
    "\n",
    "    pre_path = \"/mnt/hdd1/samora/HFE_time_v1/\"\n",
    "\n",
    "    pre_annotation_path = pre_path + pre_annotation_path\n",
    "\n",
    "    pre_annotation_path_mrc = pre_path + pre_annotation_path_mrc\n",
    "\n",
    "    Path(pre_annotation_path).mkdir(parents=True, exist_ok=True)\n",
    "    Path(pre_annotation_path_mrc).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(pre_annotation_path)\n",
    "    print(pre_annotation_path_mrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_pat_line_path = f\"current_pat_lines_parts{suffix}/\"\n",
    "from pathlib import Path\n",
    "\n",
    "if not remote_dump:\n",
    "\n",
    "    current_pat_line_path = pre_path + current_pat_line_path\n",
    "\n",
    "    current_pat_lines_path = current_pat_line_path\n",
    "\n",
    "    Path(current_pat_line_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "print(current_pat_line_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get treatment docs if not already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_exists = exists(treatment_doc_filename)\n",
    "if not file_exists:\n",
    "    docs = cohort_searcher_no_terms(\n",
    "        index_name=\"epr_documents\",\n",
    "        fields_list=\"\"\"client_idcode document_guid\tdocument_description\tbody_analysed updatetime clientvisit_visitidcode\"\"\".split(),\n",
    "        search_string=\"\"\" \"Haemochromatosis\" \n",
    "                        OR \"Hemochromatosis\" \n",
    "                        OR \"HFE\" \n",
    "                        OR \"HHC\" \n",
    "                        OR \"c282y\" \n",
    "                        OR \"h63d\" \n",
    "                        OR \"S65C\" \n",
    "                        OR \"Cys282Tyr\" \n",
    "                        OR \"p.C282Y\" \n",
    "                        OR \"HHemochromatosis\" \n",
    "                        OR \"HLAH\"\n",
    "                        OR \"Bronze diabetes\"\n",
    "                        OR \"Bronzed cirrhosis\"\n",
    "                        OR \"282y\"\n",
    "                        OR \"282C/Y\"\n",
    "                        OR \"rs1799945\"\n",
    "                        OR \"rs1800562\"\n",
    "                        OR \"rs1800730\"\n",
    "                        OR \"c.187C>G\"\n",
    "                        OR \"c.845G>A\"\n",
    "                        OR \"c.193A>T\"\n",
    "                        OR \"p.His63Asp\"\n",
    "                        OR \"p. Cys282Tyr\"\n",
    "                        OR \"p.Ser65Cys\"\n",
    "                        OR \"Transfusional haemosiderosis\"\n",
    "                        OR \"Gly320Val\"\n",
    "                        OR \"Troisier\"\n",
    "                        OR \"Iron Storage Disorder\"\n",
    "                        OR \"C282Y/H63D\"\n",
    "                        \n",
    "                         \n",
    "                         \n",
    "                        \"\"\",\n",
    "    )\n",
    "    docs.to_csv(treatment_doc_filename)\n",
    "else:\n",
    "    docs = pd.read_csv(treatment_doc_filename)\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs[\"client_idcode\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_filter = False\n",
    "if use_filter:\n",
    "    json_filter_path = (\n",
    "        \"/data/AS/Samora/HFE/HFE/v18/MedCAT_Export_With_Text_2022-12-24_21_30_48.json\"\n",
    "    )\n",
    "    import json\n",
    "\n",
    "    with open(json_filter_path, \"r\") as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    len(json_data[\"projects\"][0])\n",
    "    json_cuis = json_data[\"projects\"][0][\"cuis\"].split(\",\")\n",
    "    cat.cdb.filter_by_cui(json_cuis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_client_id_list = list(docs[\"client_idcode\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(treatment_client_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random.seed(42)\n",
    "use_controls = False\n",
    "if use_controls:\n",
    "    # Get control docs default 1:1\n",
    "\n",
    "    all_idcodes = pd.read_csv(\"all_client_idcodes_epr_unique.csv\")[\"client_idcode\"]\n",
    "\n",
    "    print(len(all_idcodes), len(treatment_client_id_list))\n",
    "\n",
    "    full_control_client_id_list = list(set(all_idcodes) - set(treatment_client_id_list))\n",
    "\n",
    "    full_control_client_id_list.sort()  # ensure sort for repeatability\n",
    "\n",
    "    len(full_control_client_id_list) - len(all_idcodes)\n",
    "\n",
    "    n_treatments = len(treatment_client_id_list) * treatment_control_ratio_n\n",
    "    print(\n",
    "        f\"{n_treatments} selected as controls\"\n",
    "    )  # Soft control selection, many treatments will be false positives\n",
    "    treatment_control_sample = pd.Series(full_control_client_id_list).sample(\n",
    "        n_treatments, random_state=42\n",
    "    )\n",
    "\n",
    "    treatment_control_sample\n",
    "\n",
    "    all_patient_list_control = list(treatment_control_sample.values)\n",
    "\n",
    "    with open(\"control_list.pkl\", \"wb\") as f:\n",
    "        pickle.dump(all_patient_list_control, f)\n",
    "\n",
    "    print(all_patient_list_control[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patient_list = list(treatment_client_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_controls:\n",
    "    all_patient_list = all_patient_list + all_patient_list_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_concatted_master_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_builder = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_threshold = 200\n",
    "\n",
    "failed_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(all_patient_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_patient_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exist_check(path, sftp_obj=None):\n",
    "    if remote_dump:\n",
    "        return sftp_exists(path, sftp_obj)\n",
    "    else:\n",
    "        return exists(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"remote_dump {remote_dump}\")\n",
    "print(pre_annotation_path)\n",
    "print(pre_annotation_path_mrc)\n",
    "print(current_pat_line_path)\n",
    "\n",
    "if remote_dump:\n",
    "\n",
    "    pre_path = \"/mnt/hdd1/samora/HFE_time_v1/\"\n",
    "\n",
    "    def sftp_exists(path, sftp_obj=None):\n",
    "        try:\n",
    "            if not share_sftp:\n",
    "                ssh_client = paramiko.SSHClient()\n",
    "                ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "                ssh_client.connect(\n",
    "                    hostname=hostname, username=username, password=password\n",
    "                )\n",
    "\n",
    "                sftp_obj = ssh_client.open_sftp()\n",
    "\n",
    "            sftp_obj.stat(path)\n",
    "\n",
    "            if not share_sftp:\n",
    "                sftp_obj.close()\n",
    "                sftp_obj.close()\n",
    "            return True\n",
    "        except FileNotFoundError:\n",
    "            return False\n",
    "\n",
    "    # Set the hostname, username, and password for the remote machine\n",
    "\n",
    "    if not aliencat or dgx:\n",
    "        hostname = \"%HOSTIPADDRESS%\"\n",
    "\n",
    "    if aliencat and not dgx:\n",
    "        hostname = \"localhost\"\n",
    "\n",
    "    username = \"%USERNAME%\"\n",
    "    password = \"%PASSWORD%\"\n",
    "\n",
    "    # Create an SSH client and connect to the remote machine\n",
    "    ssh_client = paramiko.SSHClient()\n",
    "    ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh_client.connect(hostname=hostname, username=username, password=password)\n",
    "\n",
    "    sftp_client = ssh_client.open_sftp()\n",
    "\n",
    "    if remote_dump:\n",
    "        try:\n",
    "            sftp_client.chdir(pre_path)  # Test if remote_path exists\n",
    "        except IOError:\n",
    "            sftp_client.mkdir(pre_path)  # Create remote_path\n",
    "\n",
    "    pre_annotation_path = f\"{pre_path}{pre_annotation_path}\"\n",
    "    pre_annotation_path_mrc = f\"{pre_path}{pre_annotation_path_mrc}\"\n",
    "    current_pat_line_path = f\"{pre_path}{current_pat_line_path}\"\n",
    "    current_pat_lines_path = current_pat_line_path\n",
    "\n",
    "    if not remote_dump:\n",
    "        Path(current_pat_annot_path).mkdir(parents=True, exist_ok=True)\n",
    "        Path(pre_annotation_path_mrc).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            sftp_client.chdir(pre_annotation_path)  # Test if remote_path exists\n",
    "        except IOError:\n",
    "            sftp_client.mkdir(pre_annotation_path)  # Create remote_path\n",
    "\n",
    "        try:\n",
    "            sftp_client.chdir(pre_annotation_path_mrc)  # Test if remote_path exists\n",
    "        except IOError:\n",
    "            sftp_client.mkdir(pre_annotation_path_mrc)  # Create remote_path\n",
    "\n",
    "        try:\n",
    "            sftp_client.chdir(current_pat_line_path)  # Test if remote_path exists\n",
    "        except IOError:\n",
    "            sftp_client.mkdir(current_pat_line_path)  # Create remote_path\n",
    "else:\n",
    "    sftp_client = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_dir_wrapper(path, sftp_obj=None):\n",
    "    # global sftp_client\n",
    "    if remote_dump:\n",
    "        if not share_sftp:\n",
    "            ssh_client = paramiko.SSHClient()\n",
    "            ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "            ssh_client.connect(hostname=hostname, username=username, password=password)\n",
    "\n",
    "            sftp_client = ssh_client.open_sftp()\n",
    "            sftp_obj = sftp_client\n",
    "        elif sftp_obj is None:\n",
    "            sftp_obj = sftp_client\n",
    "\n",
    "        res = sftp_obj.listdir(path)\n",
    "\n",
    "        return res\n",
    "\n",
    "    else:\n",
    "\n",
    "        return os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_results(file_data, path, sftp_obj=None):\n",
    "    if remote_dump:\n",
    "        if not share_sftp:\n",
    "            ssh_client = paramiko.SSHClient()\n",
    "            ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "            ssh_client.connect(hostname=hostname, username=username, password=password)\n",
    "\n",
    "            sftp_client = ssh_client.open_sftp()\n",
    "            sftp_obj = sftp_client\n",
    "\n",
    "        with sftp_obj.open(path, \"w\") as file:\n",
    "\n",
    "            pickle.dump(file_data, file)\n",
    "        if not share_sftp:\n",
    "            sftp_obj.close()\n",
    "            sftp_obj.close()\n",
    "\n",
    "    else:\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(file_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = [x for x in range(1, 13)]\n",
    "years = [x for x in range(1995, 2023)]\n",
    "import itertools\n",
    "\n",
    "combinations = list(itertools.product(years, months))\n",
    "len(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_options = {\n",
    "    \"demo\": True,\n",
    "    \"bmi\": True,\n",
    "    \"bloods\": True,\n",
    "    \"drugs\": True,\n",
    "    \"diagnostics\": True,\n",
    "    \"core_02\": True,\n",
    "    \"bed\": True,\n",
    "    \"vte_status\": True,\n",
    "    \"hosp_site\": True,\n",
    "    \"core_resus\": True,\n",
    "    \"news\": True,\n",
    "    \"annotations\": True,\n",
    "    \"annotations_mrc\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_demographics3(patlist, target_date_range):\n",
    "    # print(\"get demo3 non batch call --debug\")\n",
    "\n",
    "    start_year, start_month, end_year, end_month = get_start_end_year_month(\n",
    "        target_date_range\n",
    "    )\n",
    "\n",
    "    demo = cohort_searcher_with_terms_and_search(\n",
    "        index_name=\"epr_documents\",\n",
    "        fields_list=[\n",
    "            \"client_idcode\",\n",
    "            \"client_firstname\",\n",
    "            \"client_lastname\",\n",
    "            \"client_dob\",\n",
    "            \"client_gendercode\",\n",
    "            \"client_racecode\",\n",
    "            \"client_deceaseddtm\",\n",
    "            \"updatetime\",\n",
    "        ],\n",
    "        term_name=\"client_idcode.keyword\",\n",
    "        entered_list=patlist,\n",
    "        search_string=f\"updatetime:[{start_year}-{start_month} TO {end_year}-{end_month}] \",\n",
    "    )\n",
    "    demo[\"updatetime\"] = pd.to_datetime(demo[\"updatetime\"], utc=True)\n",
    "    demo = demo.sort_values(\n",
    "        [\"client_idcode\", \"updatetime\"]\n",
    "    )  # .drop_duplicates(subset = [\"client_idcode\"], keep = \"last\", inplace = True)\n",
    "    if len(demo) > 1:\n",
    "        try:\n",
    "\n",
    "            return demo.iloc[-1].to_frame()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    elif len(demo) == 1:\n",
    "        return demo\n",
    "\n",
    "    else:\n",
    "        demo = pd.DataFrame(data=None, columns=None)\n",
    "        demo[\"client_idcode\"] = patlist\n",
    "        return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_end_year_month(target_date_range):\n",
    "\n",
    "    start_year = target_date_range[0]\n",
    "    start_month = target_date_range[1]\n",
    "    end_year = start_year\n",
    "\n",
    "    if target_date_range[1] == 12:\n",
    "        end_year = start_year + 1\n",
    "        end_month = 1\n",
    "    else:\n",
    "        end_year = target_date_range[0]\n",
    "        end_month = start_month + 1\n",
    "\n",
    "    if start_month < 10:\n",
    "        start_month = \"0\" + str(start_month)\n",
    "\n",
    "    if end_month < 10:\n",
    "        end_month = \"0\" + str(end_month)\n",
    "\n",
    "    return start_year, start_month, end_year, end_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should one impute from the global mean\n",
    "# impute from local mean?\n",
    "# correct stratification. by year and month...?\n",
    "# by 6 months?\n",
    "# dynamic and user spec time strat?\n",
    "# encode month and year separately or every date/month\n",
    "# c. 240\n",
    "# with hfe annotation in vector, for positive can easily split by < >  date\n",
    "# instead filter datetime at search level, since we want to produce individual vectors\n",
    "# loop over vector production procedure for entry in date list for total 336 * npat = 336 * 20000 = 6,720,000 rows\n",
    "# for c. 7000 included variables, n cells = 7000 * 6,720,000 = 47,040,000,000\n",
    "# do we have updatetime for all data classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_empty_date_vector():\n",
    "    months = [x for x in range(1, 13)]\n",
    "    years = [x for x in range(1995, 2023)]\n",
    "    combinations = list(itertools.product(years, months))\n",
    "    combinations = [str(item) + \"_\" + \"date_time_stamp\" for item in combinations]\n",
    "\n",
    "    return pd.DataFrame(data=0.0, index=np.arange(1), columns=combinations).astype(\n",
    "        float\n",
    "    )  # untested float cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_timestamp_to_tuple(timestamp):\n",
    "    # parse the timestamp string into a datetime object\n",
    "    dt = datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "\n",
    "    # extract the year and month from the datetime object\n",
    "    year = dt.year\n",
    "    month = dt.month\n",
    "\n",
    "    # return the tuple of year and month\n",
    "    return (year, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enum_target_date_vector(target_date_range, current_pat_client_id_code):\n",
    "\n",
    "    empty_date_vector = get_empty_date_vector()\n",
    "\n",
    "    empty_date_vector.at[0, str(target_date_range) + \"_date_time_stamp\"] = 1\n",
    "\n",
    "    empty_date_vector[\"client_idcode\"] = current_pat_client_id_code\n",
    "\n",
    "    return empty_date_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problems...:\n",
    "\n",
    "# post code field map code to region for feature\n",
    "# Religion code\n",
    "# Language code\n",
    "# occuptation code not specified\n",
    "# country of birth empty\n",
    "# marital status, populated but relevance?\n",
    "# title code, populated\n",
    "# address city populated\n",
    "#\n",
    "# OR Weight (kg)\n",
    "# OR \\\"Height in cm\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good candidates:\n",
    "# City of birth many results\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"remote_dump {remote_dump}\")\n",
    "print(pre_annotation_path)\n",
    "print(pre_annotation_path_mrc)\n",
    "print(current_pat_line_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_pat_annotations_mrc_cs(\n",
    "    current_pat_client_id_code, target_date_range, pat_batch, sftp_obj=None\n",
    "):\n",
    "    global start_time\n",
    "\n",
    "    #     current_annot_file_path = pre_annotation_path_mrc + current_pat_client_id_code+\"_\"+str(target_date_range)\n",
    "\n",
    "    current_annot_file_path = (\n",
    "        pre_annotation_path_mrc\n",
    "        + current_pat_client_id_code\n",
    "        + \"/\"\n",
    "        + current_pat_client_id_code\n",
    "        + \"_\"\n",
    "        + str(target_date_range)\n",
    "    )\n",
    "\n",
    "    file_exists = exist_check(current_annot_file_path, sftp_obj)\n",
    "\n",
    "    if not file_exists:\n",
    "\n",
    "        start_year, start_month, end_year, end_month = get_start_end_year_month(\n",
    "            target_date_range\n",
    "        )\n",
    "\n",
    "        if batch_mode:\n",
    "            current_pat_docs = filter_dataframe_by_timestamp(\n",
    "                pat_batch,\n",
    "                start_year,\n",
    "                start_month,\n",
    "                end_year,\n",
    "                end_month,\n",
    "                \"observationdocument_recordeddtm\",\n",
    "            )\n",
    "\n",
    "        else:\n",
    "\n",
    "            current_pat_docs = cohort_searcher_with_terms_and_search(\n",
    "                index_name=\"observations\",\n",
    "                fields_list=\"\"\"observation_guid client_idcode\tobscatalogmasteritem_displayname\tobservation_valuetext_analysed\tobservationdocument_recordeddtm clientvisit_visitidcode\"\"\".split(),\n",
    "                term_name=\"client_idcode.keyword\",\n",
    "                entered_list=[current_pat_client_id_code],\n",
    "                search_string='obscatalogmasteritem_displayname:(\"AoMRC_ClinicalSummary_FT\") AND '\n",
    "                + f\"observationdocument_recordeddtm:[{start_year}-{start_month} TO {end_year}-{end_month}]\",\n",
    "            )\n",
    "\n",
    "        n_docs_to_annotate = len(current_pat_docs)\n",
    "        update_pbar(\n",
    "            current_pat_client_id_code + \"_\" + str(target_date_range),\n",
    "            start_time,\n",
    "            5,\n",
    "            \"annotations_mrc\",\n",
    "            n_docs_to_annotate=n_docs_to_annotate,\n",
    "        )\n",
    "    else:\n",
    "        n_docs_to_annotate = \"Reading preannotated mrc...\"\n",
    "    annotation_map = {\n",
    "        \"True\": 1,\n",
    "        \"Presence\": 1,\n",
    "        \"Recent\": 1,\n",
    "        \"Past\": 0,\n",
    "        \"Subject/Experiencer\": 1,\n",
    "        \"Other\": 0,\n",
    "        \"Hypothetical\": 0,\n",
    "        \"Patient\": 1,\n",
    "    }\n",
    "\n",
    "    # remove filter from cdb?\n",
    "    # print(\"getting annotations\")\n",
    "\n",
    "    #     file_exists = exists(pre_annotation_path_mrc + current_pat_client_id_code)\n",
    "\n",
    "    if not file_exists:\n",
    "        with io.capture_output():\n",
    "            pats_anno_annotations = cat.get_entities_multi_texts(\n",
    "                current_pat_docs[\"observation_valuetext_analysed\"].dropna()\n",
    "            )\n",
    "            # , n_process=1\n",
    "\n",
    "            dump_results(pats_anno_annotations, current_annot_file_path, sftp_obj)\n",
    "\n",
    "    #                 with open(pre_annotation_path_mrc + current_pat_client_id_code, 'wb') as f:\n",
    "    #                     pickle.dump(pats_anno_annotations, f)\n",
    "\n",
    "    else:\n",
    "        if remote_dump:\n",
    "            if not share_sftp:\n",
    "                ssh_client = paramiko.SSHClient()\n",
    "                ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "                ssh_client.connect(\n",
    "                    hostname=hostname, username=username, password=password\n",
    "                )\n",
    "\n",
    "                sftp_client = ssh_client.open_sftp()\n",
    "                sftp_obj = sftp_client\n",
    "\n",
    "            with sftp_obj.open(current_annot_file_path, \"r\") as file:\n",
    "\n",
    "                pats_anno_annotations = pickle.load(file)\n",
    "\n",
    "            if not share_sftp:\n",
    "                sftp_obj.close()\n",
    "                sftp_obj.close()\n",
    "\n",
    "        else:\n",
    "            with open(current_annot_file_path, \"rb\") as f:\n",
    "                pats_anno_annotations = pickle.load(f)\n",
    "\n",
    "    n_docs_to_annotate = len(pats_anno_annotations)\n",
    "\n",
    "    # print(f\"Annotated {current_pat_client_id_code}\")\n",
    "    # length of chars in documents summed\n",
    "    # average number of documents as a divisor for mention counts\n",
    "    # we want to keep the fact that lots of documents is a bad sign...\n",
    "    # Lots of mentions of something could indicate severity etc\n",
    "\n",
    "    # pats_anno_annotations = cat.get_entities(current_pat_docs['body_analysed'])\n",
    "    update_pbar(\n",
    "        current_pat_client_id_code + \"_\" + str(target_date_range),\n",
    "        start_time,\n",
    "        5,\n",
    "        \"annotations_mrc\",\n",
    "        n_docs_to_annotate=n_docs_to_annotate,\n",
    "    )\n",
    "\n",
    "    sum_count = 0\n",
    "    for i in range(0, len(pats_anno_annotations)):\n",
    "        sum_count = sum_count + len(list(pats_anno_annotations[i][\"entities\"].keys()))\n",
    "\n",
    "    sum_count_index_list = [x for x in range(0, sum_count)]\n",
    "    all_doc_entities = {\"entities\": dict.fromkeys(sum_count_index_list, {})}\n",
    "    sum_count_index = 0\n",
    "    for i in range(0, len(pats_anno_annotations)):\n",
    "\n",
    "        key_list = list(pats_anno_annotations[i][\"entities\"].keys())\n",
    "        for j in range(0, len(key_list)):\n",
    "\n",
    "            all_doc_entities[\"entities\"][sum_count_index] = pats_anno_annotations[i][\n",
    "                \"entities\"\n",
    "            ].get(key_list[j])\n",
    "            sum_count_index = sum_count_index + 1\n",
    "\n",
    "    pats_anno_annotations = all_doc_entities\n",
    "\n",
    "    all_cui_list = []\n",
    "\n",
    "    all_meta_anno = False\n",
    "    confidence_threshold_presence = 0.8\n",
    "    confidence_threshold_concept_accuracy = 0.8\n",
    "\n",
    "    cui_list_pretty_names = []\n",
    "    doc_keys = list(pats_anno_annotations.keys())\n",
    "    for i in range(0, len(doc_keys)):\n",
    "        current_pats_entry = pats_anno_annotations.get(\"entities\")\n",
    "        current_pats_entry_keys = list(pats_anno_annotations.get(\"entities\").keys())\n",
    "        for j in range(0, len(current_pats_entry_keys)):\n",
    "            all_cui_list.append(\n",
    "                current_pats_entry.get(current_pats_entry_keys[j])[\"cui\"]\n",
    "            )\n",
    "            cui_list_pretty_names.append(\n",
    "                current_pats_entry.get(current_pats_entry_keys[j])[\"pretty_name\"]\n",
    "            )\n",
    "    # print(\"len(all_cui_list)\", len(all_cui_list))\n",
    "    # print(\"len(set(all_cui_list))\", len(set(all_cui_list)))\n",
    "\n",
    "    cui_list = all_cui_list\n",
    "\n",
    "    #     cui_list_pretty_names = []\n",
    "    #     for i in range(0, len(cui_list)):\n",
    "    #         cui_list_pretty_names.append(cat.cdb.cui2preferred_name.get(cui_list[i]))\n",
    "\n",
    "    # print(\"len(set(cui_list_pretty_names))\", len(set(cui_list_pretty_names)))\n",
    "\n",
    "    cui_list_pretty_names = list(set(cui_list_pretty_names))\n",
    "\n",
    "    # cui_list_pretty_names.remove(None)\n",
    "\n",
    "    cui_list_pretty_names_meta_list = []\n",
    "    if all_meta_anno:\n",
    "        [x + \"_meta\" for x in cui_list_pretty_names]\n",
    "\n",
    "        cui_list_pretty_names_meta_list = []\n",
    "\n",
    "        meta_key_list = [\"Time\", \"Presence\", \"Subject/Experiencer\"]\n",
    "\n",
    "        meta_sub_key_list = [\"value\"]  # ,'confidence', 'name']\n",
    "\n",
    "        for i in range(0, len(cui_list_pretty_names)):\n",
    "            for j in range(0, len(meta_key_list)):\n",
    "                for k in range(0, len(meta_sub_key_list)):\n",
    "                    cui_list_pretty_names_meta_list.append(\n",
    "                        cui_list_pretty_names[i]\n",
    "                        + \"_\"\n",
    "                        + meta_key_list[j]\n",
    "                        + \"_\"\n",
    "                        + meta_sub_key_list[k]\n",
    "                    )\n",
    "\n",
    "        print(\n",
    "            \"len(set(cui_list_pretty_names_meta_list))\",\n",
    "            len(set(cui_list_pretty_names_meta_list)),\n",
    "        )\n",
    "\n",
    "    cui_list_pretty_names_count_list = []\n",
    "    for i in range(0, len(cui_list_pretty_names)):\n",
    "        cui_list_pretty_names_count_list.append(\n",
    "            cui_list_pretty_names[i] + \"_count_mrc_cs\"\n",
    "        )\n",
    "        cui_list_pretty_names_count_list.append(\n",
    "            cui_list_pretty_names[i] + \"_count_subject_present_mrc_cs\"\n",
    "        )\n",
    "\n",
    "        if negated_presence_annotations:\n",
    "            cui_list_pretty_names_count_list.append(\n",
    "                cui_list_pretty_names[i] + \"_count_subject_not_present_mrc_cs\"\n",
    "            )\n",
    "            cui_list_pretty_names_count_list.append(\n",
    "                cui_list_pretty_names[i] + \"_count_relative_not_present_mrc_cs\"\n",
    "            )\n",
    "\n",
    "    all_columns_to_append = cui_list_pretty_names_count_list\n",
    "    if all_meta_anno:\n",
    "        all_columns_to_append.append(cui_list_pretty_names_meta_list)\n",
    "\n",
    "    dummy_data = np.empty((1, len(all_columns_to_append)))\n",
    "    dummy_data[:] = np.nan\n",
    "    df_pat_entry = pd.DataFrame(data=dummy_data, columns=all_columns_to_append)\n",
    "\n",
    "    # df = pd.read_csv(file_name) #call outside\n",
    "    df_pat = df_pat_entry.copy()\n",
    "    df_pat[\"client_idcode\"] = current_pat_client_id_code\n",
    "    df_pat[\"n_docs\"] = n_docs_to_annotate\n",
    "\n",
    "    df_pat.reset_index(inplace=True)\n",
    "    # df_pat['n'] = [i for i in range(0, len(df_pat))]\n",
    "    df_pat = df_pat[[\"client_idcode\"]].copy()\n",
    "    df_pat_target = df_pat.copy(deep=True)\n",
    "    # print(\"Reindexing df_pat_target\")\n",
    "    df_pat_target = df_pat_target.reindex(\n",
    "        list(df_pat_target.columns) + all_columns_to_append, axis=1\n",
    "    )\n",
    "\n",
    "    a = list(df_pat_target.columns)\n",
    "    b = cui_list_pretty_names_meta_list\n",
    "\n",
    "    # print(\"filling df pat target with nans\")\n",
    "    df_pat_target[[x for x in a if (x not in b)]] = df_pat_target[\n",
    "        [x for x in a if (x not in b)]\n",
    "    ].fillna(0)\n",
    "\n",
    "    # break\n",
    "    df_pat_target[\"client_idcode\"].iloc[0] = current_pat_client_id_code\n",
    "\n",
    "    df_pat_target = df_pat_target.copy()\n",
    "\n",
    "    [x for x in range(0, len(df_pat_target))]\n",
    "\n",
    "    list(cui_list_pretty_names_meta_list)\n",
    "\n",
    "    df_pat_target = df_pat_target.copy()  # [0:1]\n",
    "\n",
    "    entry_counter = 0\n",
    "    meta_counter = 0\n",
    "    i = 0\n",
    "    # print(\"Starting annotation frame builder...\")\n",
    "    # for i in tqdm(range(0, len(df_pat_target))):\n",
    "    # ci = df_pat_target['client_idcode'].iloc[i]\n",
    "\n",
    "    # annotations = cat.get_entities(current_pat_docs['body_analysed']) #docs.get(ci)\n",
    "    annotations = pats_anno_annotations\n",
    "\n",
    "    if annotations is not None:\n",
    "        annotation_keys = list(annotations[\"entities\"].keys())\n",
    "\n",
    "        for j in range(0, len(annotation_keys)):\n",
    "\n",
    "            cui = annotations[\"entities\"][annotation_keys[j]].get(\"cui\")\n",
    "            if cui in cui_list:\n",
    "                current_col_name = annotations[\"entities\"][annotation_keys[j]].get(\n",
    "                    \"pretty_name\"\n",
    "                )\n",
    "                current_col_meta = annotations[\"entities\"][annotation_keys[j]].get(\n",
    "                    \"meta_anns\"\n",
    "                )\n",
    "\n",
    "                df_pat_target.at[i, current_col_name + \"_count_mrc_cs\"] = (\n",
    "                    df_pat_target.loc[i][current_col_name + \"_count_mrc_cs\"] + 1\n",
    "                )\n",
    "\n",
    "                if current_col_meta is not None:\n",
    "\n",
    "                    if (\n",
    "                        current_col_meta[\"Presence\"][\"value\"] == \"True\"\n",
    "                        and current_col_meta[\"Subject/Experiencer\"][\"value\"]\n",
    "                        == \"Patient\"\n",
    "                        and current_col_meta[\"Presence\"][\"confidence\"]\n",
    "                        > confidence_threshold_presence\n",
    "                        and current_col_meta[\"Subject/Experiencer\"][\"confidence\"]\n",
    "                        > confidence_threshold_presence\n",
    "                        and annotations[\"entities\"][annotation_keys[j]][\"acc\"]\n",
    "                        > confidence_threshold_concept_accuracy\n",
    "                    ):\n",
    "\n",
    "                        df_pat_target.at[\n",
    "                            i, current_col_name + \"_count_subject_present_mrc_cs\"\n",
    "                        ] = (\n",
    "                            df_pat_target.loc[i][\n",
    "                                current_col_name + \"_count_subject_present_mrc_cs\"\n",
    "                            ]\n",
    "                            + 1\n",
    "                        )\n",
    "\n",
    "                    elif (\n",
    "                        current_col_meta[\"Presence\"][\"value\"] == \"True\"\n",
    "                        and current_col_meta[\"Subject/Experiencer\"][\"value\"]\n",
    "                        == \"Relative\"\n",
    "                        and current_col_meta[\"Presence\"][\"confidence\"]\n",
    "                        > confidence_threshold_presence\n",
    "                        and current_col_meta[\"Subject/Experiencer\"][\"confidence\"]\n",
    "                        > confidence_threshold_presence\n",
    "                        and annotations[\"entities\"][annotation_keys[j]][\"acc\"]\n",
    "                        > confidence_threshold_concept_accuracy\n",
    "                    ):\n",
    "                        if (\n",
    "                            current_col_name + \"_count_relative_present_mrc_cs\"\n",
    "                            in df_pat_target.columns\n",
    "                        ):\n",
    "\n",
    "                            df_pat_target.at[\n",
    "                                i, current_col_name + \"_count_relative_present_mrc_cs\"\n",
    "                            ] = (\n",
    "                                df_pat_target.loc[i][\n",
    "                                    current_col_name + \"_count_relative_present_mrc_cs\"\n",
    "                                ]\n",
    "                                + 1\n",
    "                            )\n",
    "\n",
    "                        else:\n",
    "                            df_pat_target[\n",
    "                                current_col_name + \"_count_relative_present_mrc_cs\"\n",
    "                            ] = 1\n",
    "\n",
    "                    if negated_presence_annotations:\n",
    "\n",
    "                        if current_col_meta is not None:\n",
    "\n",
    "                            if (\n",
    "                                current_col_meta[\"Presence\"][\"value\"] == \"False\"\n",
    "                                and current_col_meta[\"Subject/Experiencer\"][\"value\"]\n",
    "                                == \"Patient\"\n",
    "                                and current_col_meta[\"Presence\"][\"confidence\"]\n",
    "                                > confidence_threshold_presence\n",
    "                                and current_col_meta[\"Subject/Experiencer\"][\n",
    "                                    \"confidence\"\n",
    "                                ]\n",
    "                                > confidence_threshold_presence\n",
    "                                and annotations[\"entities\"][annotation_keys[j]][\"acc\"]\n",
    "                                > confidence_threshold_concept_accuracy\n",
    "                            ):\n",
    "\n",
    "                                df_pat_target.at[\n",
    "                                    i,\n",
    "                                    current_col_name\n",
    "                                    + \"_count_subject_not_present_mrc_cs\",\n",
    "                                ] = (\n",
    "                                    df_pat_target.loc[i][\n",
    "                                        current_col_name\n",
    "                                        + \"_count_subject_not_present_mrc_cs\"\n",
    "                                    ]\n",
    "                                    + 1\n",
    "                                )\n",
    "\n",
    "                            elif (\n",
    "                                current_col_meta[\"Presence\"][\"value\"] == \"True\"\n",
    "                                and current_col_meta[\"Subject/Experiencer\"][\"value\"]\n",
    "                                == \"Relative\"\n",
    "                                and current_col_meta[\"Presence\"][\"confidence\"]\n",
    "                                > confidence_threshold_presence\n",
    "                                and current_col_meta[\"Subject/Experiencer\"][\n",
    "                                    \"confidence\"\n",
    "                                ]\n",
    "                                > confidence_threshold_presence\n",
    "                                and annotations[\"entities\"][annotation_keys[j]][\"acc\"]\n",
    "                                > confidence_threshold_concept_accuracy\n",
    "                            ):\n",
    "                                if (\n",
    "                                    current_col_name\n",
    "                                    + \"_count_relative_not_present_mrc_cs\"\n",
    "                                    in df_pat_target.columns\n",
    "                                ):\n",
    "\n",
    "                                    df_pat_target.at[\n",
    "                                        i,\n",
    "                                        current_col_name\n",
    "                                        + \"_count_relative_not_present_mrc_cs\",\n",
    "                                    ] = (\n",
    "                                        df_pat_target.loc[i][\n",
    "                                            current_col_name\n",
    "                                            + \"_count_relative_not_present_mrc_cs\"\n",
    "                                        ]\n",
    "                                        + 1\n",
    "                                    )\n",
    "\n",
    "                                else:\n",
    "                                    df_pat_target[\n",
    "                                        current_col_name\n",
    "                                        + \"_count_relative_not_present_mrc_cs\"\n",
    "                                    ] = 1\n",
    "\n",
    "                    else:\n",
    "                        # OLD: set to nan instead of zero for impute and medcat precision persistence\n",
    "                        # Dont set anything here as this will overwrite existing 1 entries?\n",
    "                        pass\n",
    "                        # df_pat_target.at[i, current_col_name+\"_count_subject_present\"] = np.nan\n",
    "\n",
    "                if all_meta_anno:\n",
    "                    if len(list(current_col_meta.keys())) > 0:\n",
    "\n",
    "                        for key in current_col_meta.keys():\n",
    "\n",
    "                            sub_anot = current_col_meta.get(key)\n",
    "                            if len(list(sub_anot.keys())) > 0:\n",
    "                                for sub_key in sub_anot.keys():\n",
    "                                    if sub_key in meta_sub_key_list:\n",
    "                                        try:\n",
    "\n",
    "                                            key_result = sub_anot.get(sub_key)\n",
    "                                            if type(key_result) is str:\n",
    "                                                key_result = annotation_map.get(\n",
    "                                                    key_result\n",
    "                                                )\n",
    "\n",
    "                                            # print(current_col_name+'_'+str(key)+'_'+str(sub_key), key_result, sub_anot.get(sub_key))\n",
    "\n",
    "                                            df_pat_target.at[\n",
    "                                                i,\n",
    "                                                current_col_name\n",
    "                                                + \"_\"\n",
    "                                                + str(key)\n",
    "                                                + \"_\"\n",
    "                                                + str(sub_key),\n",
    "                                            ] = key_result\n",
    "\n",
    "                                            meta_counter = meta_counter + 1\n",
    "                                        except Exception as e:\n",
    "                                            print(e)\n",
    "                                            pass\n",
    "\n",
    "                meta_counter = meta_counter + 1\n",
    "                entry_counter = entry_counter + 1\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "    update_pbar(\n",
    "        current_pat_client_id_code + \"_\" + str(target_date_range),\n",
    "        start_time,\n",
    "        5,\n",
    "        \"annotations_mrc\",\n",
    "        n_docs_to_annotate=n_docs_to_annotate,\n",
    "    )\n",
    "    # df_pat_target.drop(\"n\", axis=1, inplace=True)\n",
    "\n",
    "    # df_pat_target.to_csv(entry_file_name)\n",
    "    # print(f\"Made {entry_counter} entry_counter  entries\")\n",
    "    # print(f\"Made {meta_counter} meta_counter entries\")\n",
    "    # print(\"done\")\n",
    "    return df_pat_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_pat_annotations(\n",
    "    current_pat_client_id_code, target_date_range, pat_batch, sftp_obj=None\n",
    "):\n",
    "    global start_time\n",
    "\n",
    "    #     current_annotation_file_path = pre_annotation_path + current_pat_client_id_code+\"_\"+str(target_date_range)\n",
    "\n",
    "    current_annotation_file_path = (\n",
    "        pre_annotation_path\n",
    "        + current_pat_client_id_code\n",
    "        + \"/\"\n",
    "        + current_pat_client_id_code\n",
    "        + \"_\"\n",
    "        + str(target_date_range)\n",
    "    )\n",
    "\n",
    "    file_exists = exist_check(current_annotation_file_path, sftp_obj)\n",
    "\n",
    "    if not file_exists:\n",
    "\n",
    "        start_year, start_month, end_year, end_month = get_start_end_year_month(\n",
    "            target_date_range\n",
    "        )\n",
    "\n",
    "        if batch_mode:\n",
    "            current_pat_docs = filter_dataframe_by_timestamp(\n",
    "                pat_batch, start_year, start_month, end_year, end_month, \"updatetime\"\n",
    "            )\n",
    "\n",
    "        else:\n",
    "\n",
    "            current_pat_docs = cohort_searcher_with_terms_and_search(\n",
    "                index_name=\"epr_documents\",\n",
    "                fields_list=\"\"\"client_idcode document_guid\tdocument_description\tbody_analysed updatetime clientvisit_visitidcode\"\"\".split(),\n",
    "                term_name=\"client_idcode.keyword\",\n",
    "                entered_list=[current_pat_client_id_code],\n",
    "                search_string=f\"updatetime:[{start_year}-{start_month} TO {end_year}-{end_month}] \",\n",
    "            )\n",
    "\n",
    "        n_docs_to_annotate = len(current_pat_docs)\n",
    "        update_pbar(\n",
    "            current_pat_client_id_code + \"_\" + str(target_date_range),\n",
    "            start_time,\n",
    "            5,\n",
    "            \"annotations\",\n",
    "            n_docs_to_annotate=n_docs_to_annotate,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        n_docs_to_annotate = \"Reading preannotated...\"\n",
    "\n",
    "    annotation_map = {\n",
    "        \"True\": 1,\n",
    "        \"Presence\": 1,\n",
    "        \"Recent\": 1,\n",
    "        \"Past\": 0,\n",
    "        \"Subject/Experiencer\": 1,\n",
    "        \"Other\": 0,\n",
    "        \"Hypothetical\": 0,\n",
    "        \"Patient\": 1,\n",
    "    }\n",
    "\n",
    "    # remove filter from cdb?\n",
    "    # print(\"getting annotations\")\n",
    "\n",
    "    #     file_exists = exists(pre_annotation_path + current_pat_client_id_code)\n",
    "\n",
    "    if not file_exists:\n",
    "        with io.capture_output():\n",
    "            pats_anno_annotations = cat.get_entities_multi_texts(\n",
    "                current_pat_docs[\"body_analysed\"].dropna()\n",
    "            )\n",
    "            # , n_process=1\n",
    "        if store_annot:\n",
    "            dump_results(pats_anno_annotations, current_annotation_file_path, sftp_obj)\n",
    "\n",
    "    else:\n",
    "        if remote_dump:\n",
    "            if not share_sftp:\n",
    "                ssh_client = paramiko.SSHClient()\n",
    "                ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "                ssh_client.connect(\n",
    "                    hostname=hostname, username=username, password=password\n",
    "                )\n",
    "\n",
    "                sftp_client = ssh_client.open_sftp()\n",
    "                sftp_obj = sftp_client\n",
    "\n",
    "            with sftp_obj.open(current_annotation_file_path, \"r\") as file:\n",
    "\n",
    "                pats_anno_annotations = pickle.load(file)\n",
    "\n",
    "            if not share_sftp:\n",
    "                sftp_obj.close()\n",
    "                sftp_obj.close()\n",
    "\n",
    "        else:\n",
    "\n",
    "            with open(current_annotation_file_path, \"rb\") as f:\n",
    "                pats_anno_annotations = pickle.load(f)\n",
    "\n",
    "    n_docs_to_annotate = len(pats_anno_annotations)\n",
    "\n",
    "    # print(f\"Annotated {current_pat_client_id_code}\")\n",
    "    # length of chars in documents summed\n",
    "    # average number of documents as a divisor for mention counts\n",
    "    # we want to keep the fact that lots of documents is a bad sign...\n",
    "    # Lots of mentions of something could indicate severity etc\n",
    "\n",
    "    # pats_anno_annotations = cat.get_entities(current_pat_docs['body_analysed'])\n",
    "    update_pbar(\n",
    "        current_pat_client_id_code + \"_\" + str(target_date_range),\n",
    "        start_time,\n",
    "        5,\n",
    "        \"annotations\",\n",
    "        n_docs_to_annotate=n_docs_to_annotate,\n",
    "    )\n",
    "\n",
    "    sum_count = 0\n",
    "    for i in range(0, len(pats_anno_annotations)):\n",
    "        sum_count = sum_count + len(list(pats_anno_annotations[i][\"entities\"].keys()))\n",
    "\n",
    "    sum_count_index_list = [x for x in range(0, sum_count)]\n",
    "    all_doc_entities = {\"entities\": dict.fromkeys(sum_count_index_list, {})}\n",
    "    sum_count_index = 0\n",
    "    for i in range(0, len(pats_anno_annotations)):\n",
    "\n",
    "        key_list = list(pats_anno_annotations[i][\"entities\"].keys())\n",
    "        for j in range(0, len(key_list)):\n",
    "\n",
    "            all_doc_entities[\"entities\"][sum_count_index] = pats_anno_annotations[i][\n",
    "                \"entities\"\n",
    "            ].get(key_list[j])\n",
    "            sum_count_index = sum_count_index + 1\n",
    "\n",
    "    pats_anno_annotations = all_doc_entities\n",
    "\n",
    "    all_cui_list = []\n",
    "\n",
    "    all_meta_anno = False\n",
    "    confidence_threshold_presence = 0.8\n",
    "    confidence_threshold_concept_accuracy = 0.8\n",
    "\n",
    "    cui_list_pretty_names = []\n",
    "    doc_keys = list(pats_anno_annotations.keys())\n",
    "    for i in range(0, len(doc_keys)):\n",
    "        current_pats_entry = pats_anno_annotations.get(\"entities\")\n",
    "        current_pats_entry_keys = list(pats_anno_annotations.get(\"entities\").keys())\n",
    "        for j in range(0, len(current_pats_entry_keys)):\n",
    "            all_cui_list.append(\n",
    "                current_pats_entry.get(current_pats_entry_keys[j])[\"cui\"]\n",
    "            )\n",
    "            cui_list_pretty_names.append(\n",
    "                current_pats_entry.get(current_pats_entry_keys[j])[\"pretty_name\"]\n",
    "            )\n",
    "    # print(\"len(all_cui_list)\", len(all_cui_list))\n",
    "    # print(\"len(set(all_cui_list))\", len(set(all_cui_list)))\n",
    "\n",
    "    cui_list = all_cui_list\n",
    "\n",
    "    #     cui_list_pretty_names = []\n",
    "    #     for i in range(0, len(cui_list)):\n",
    "    #         cui_list_pretty_names.append(cat.cdb.cui2preferred_name.get(cui_list[i]))\n",
    "\n",
    "    # print(\"len(set(cui_list_pretty_names))\", len(set(cui_list_pretty_names)))\n",
    "\n",
    "    cui_list_pretty_names = list(set(cui_list_pretty_names))\n",
    "\n",
    "    # cui_list_pretty_names.remove(None)\n",
    "\n",
    "    cui_list_pretty_names_meta_list = []\n",
    "    if all_meta_anno:\n",
    "        [x + \"_meta\" for x in cui_list_pretty_names]\n",
    "\n",
    "        cui_list_pretty_names_meta_list = []\n",
    "\n",
    "        meta_key_list = [\"Time\", \"Presence\", \"Subject/Experiencer\"]\n",
    "\n",
    "        meta_sub_key_list = [\"value\"]  # ,'confidence', 'name']\n",
    "\n",
    "        for i in range(0, len(cui_list_pretty_names)):\n",
    "            for j in range(0, len(meta_key_list)):\n",
    "                for k in range(0, len(meta_sub_key_list)):\n",
    "                    cui_list_pretty_names_meta_list.append(\n",
    "                        cui_list_pretty_names[i]\n",
    "                        + \"_\"\n",
    "                        + meta_key_list[j]\n",
    "                        + \"_\"\n",
    "                        + meta_sub_key_list[k]\n",
    "                    )\n",
    "\n",
    "        print(\n",
    "            \"len(set(cui_list_pretty_names_meta_list))\",\n",
    "            len(set(cui_list_pretty_names_meta_list)),\n",
    "        )\n",
    "\n",
    "    cui_list_pretty_names_count_list = []\n",
    "    for i in range(0, len(cui_list_pretty_names)):\n",
    "        cui_list_pretty_names_count_list.append(cui_list_pretty_names[i] + \"_count\")\n",
    "        cui_list_pretty_names_count_list.append(\n",
    "            cui_list_pretty_names[i] + \"_count_subject_present\"\n",
    "        )\n",
    "\n",
    "        if negated_presence_annotations:\n",
    "            cui_list_pretty_names_count_list.append(\n",
    "                cui_list_pretty_names[i] + \"_count_subject_not_present\"\n",
    "            )\n",
    "            cui_list_pretty_names_count_list.append(\n",
    "                cui_list_pretty_names[i] + \"_count_relative_not_present\"\n",
    "            )\n",
    "\n",
    "    all_columns_to_append = cui_list_pretty_names_count_list\n",
    "    if all_meta_anno:\n",
    "        all_columns_to_append.append(cui_list_pretty_names_meta_list)\n",
    "\n",
    "    dummy_data = np.empty((1, len(all_columns_to_append)))\n",
    "    dummy_data[:] = np.nan\n",
    "    df_pat_entry = pd.DataFrame(data=dummy_data, columns=all_columns_to_append)\n",
    "\n",
    "    # df = pd.read_csv(file_name) #call outside\n",
    "    df_pat = df_pat_entry.copy()\n",
    "    df_pat[\"client_idcode\"] = current_pat_client_id_code\n",
    "    df_pat[\"n_docs\"] = n_docs_to_annotate\n",
    "\n",
    "    df_pat.reset_index(inplace=True)\n",
    "    # df_pat['n'] = [i for i in range(0, len(df_pat))]\n",
    "    df_pat = df_pat[[\"client_idcode\"]].copy()\n",
    "    df_pat_target = df_pat.copy(deep=True)\n",
    "    # print(\"Reindexing df_pat_target\")\n",
    "    df_pat_target = df_pat_target.reindex(\n",
    "        list(df_pat_target.columns) + all_columns_to_append, axis=1\n",
    "    )\n",
    "\n",
    "    a = list(df_pat_target.columns)\n",
    "    b = cui_list_pretty_names_meta_list\n",
    "\n",
    "    # print(\"filling df pat target with nans\")\n",
    "    df_pat_target[[x for x in a if (x not in b)]] = df_pat_target[\n",
    "        [x for x in a if (x not in b)]\n",
    "    ].fillna(0)\n",
    "\n",
    "    # break\n",
    "    df_pat_target[\"client_idcode\"].iloc[0] = current_pat_client_id_code\n",
    "\n",
    "    df_pat_target = df_pat_target.copy()\n",
    "\n",
    "    [x for x in range(0, len(df_pat_target))]\n",
    "\n",
    "    list(cui_list_pretty_names_meta_list)\n",
    "\n",
    "    df_pat_target = df_pat_target.copy()  # [0:1]\n",
    "\n",
    "    entry_counter = 0\n",
    "    meta_counter = 0\n",
    "    i = 0\n",
    "    # print(\"Starting annotation frame builder...\")\n",
    "    # for i in tqdm(range(0, len(df_pat_target))):\n",
    "    # ci = df_pat_target['client_idcode'].iloc[i]\n",
    "\n",
    "    # annotations = cat.get_entities(current_pat_docs['body_analysed']) #docs.get(ci)\n",
    "    annotations = pats_anno_annotations\n",
    "\n",
    "    if annotations is not None:\n",
    "        annotation_keys = list(annotations[\"entities\"].keys())\n",
    "\n",
    "        for j in range(0, len(annotation_keys)):\n",
    "\n",
    "            cui = annotations[\"entities\"][annotation_keys[j]].get(\"cui\")\n",
    "            if cui in cui_list:\n",
    "                current_col_name = annotations[\"entities\"][annotation_keys[j]].get(\n",
    "                    \"pretty_name\"\n",
    "                )\n",
    "                current_col_meta = annotations[\"entities\"][annotation_keys[j]].get(\n",
    "                    \"meta_anns\"\n",
    "                )\n",
    "\n",
    "                df_pat_target.at[i, current_col_name + \"_count\"] = (\n",
    "                    df_pat_target.loc[i][current_col_name + \"_count\"] + 1\n",
    "                )\n",
    "\n",
    "                if current_col_meta is not None:\n",
    "\n",
    "                    if (\n",
    "                        current_col_meta[\"Presence\"][\"value\"] == \"True\"\n",
    "                        and current_col_meta[\"Subject/Experiencer\"][\"value\"]\n",
    "                        == \"Patient\"\n",
    "                        and current_col_meta[\"Presence\"][\"confidence\"]\n",
    "                        > confidence_threshold_presence\n",
    "                        and current_col_meta[\"Subject/Experiencer\"][\"confidence\"]\n",
    "                        > confidence_threshold_presence\n",
    "                        and annotations[\"entities\"][annotation_keys[j]][\"acc\"]\n",
    "                        > confidence_threshold_concept_accuracy\n",
    "                    ):\n",
    "\n",
    "                        df_pat_target.at[\n",
    "                            i, current_col_name + \"_count_subject_present\"\n",
    "                        ] = (\n",
    "                            df_pat_target.loc[i][\n",
    "                                current_col_name + \"_count_subject_present\"\n",
    "                            ]\n",
    "                            + 1\n",
    "                        )\n",
    "\n",
    "                    elif (\n",
    "                        current_col_meta[\"Presence\"][\"value\"] == \"True\"\n",
    "                        and current_col_meta[\"Subject/Experiencer\"][\"value\"]\n",
    "                        == \"Relative\"\n",
    "                        and current_col_meta[\"Presence\"][\"confidence\"]\n",
    "                        > confidence_threshold_presence\n",
    "                        and current_col_meta[\"Subject/Experiencer\"][\"confidence\"]\n",
    "                        > confidence_threshold_presence\n",
    "                        and annotations[\"entities\"][annotation_keys[j]][\"acc\"]\n",
    "                        > confidence_threshold_concept_accuracy\n",
    "                    ):\n",
    "                        if (\n",
    "                            current_col_name + \"_count_relative_present\"\n",
    "                            in df_pat_target.columns\n",
    "                        ):\n",
    "\n",
    "                            df_pat_target.at[\n",
    "                                i, current_col_name + \"_count_relative_present\"\n",
    "                            ] = (\n",
    "                                df_pat_target.loc[i][\n",
    "                                    current_col_name + \"_count_relative_present\"\n",
    "                                ]\n",
    "                                + 1\n",
    "                            )\n",
    "\n",
    "                        else:\n",
    "                            df_pat_target[\n",
    "                                current_col_name + \"_count_relative_present\"\n",
    "                            ] = 1\n",
    "\n",
    "                    if negated_presence_annotations:\n",
    "\n",
    "                        if current_col_meta is not None:\n",
    "\n",
    "                            if (\n",
    "                                current_col_meta[\"Presence\"][\"value\"] == \"False\"\n",
    "                                and current_col_meta[\"Subject/Experiencer\"][\"value\"]\n",
    "                                == \"Patient\"\n",
    "                                and current_col_meta[\"Presence\"][\"confidence\"]\n",
    "                                > confidence_threshold_presence\n",
    "                                and current_col_meta[\"Subject/Experiencer\"][\n",
    "                                    \"confidence\"\n",
    "                                ]\n",
    "                                > confidence_threshold_presence\n",
    "                                and annotations[\"entities\"][annotation_keys[j]][\"acc\"]\n",
    "                                > confidence_threshold_concept_accuracy\n",
    "                            ):\n",
    "\n",
    "                                df_pat_target.at[\n",
    "                                    i, current_col_name + \"_count_subject_not_present\"\n",
    "                                ] = (\n",
    "                                    df_pat_target.loc[i][\n",
    "                                        current_col_name + \"_count_subject_not_present\"\n",
    "                                    ]\n",
    "                                    + 1\n",
    "                                )\n",
    "\n",
    "                            elif (\n",
    "                                current_col_meta[\"Presence\"][\"value\"] == \"True\"\n",
    "                                and current_col_meta[\"Subject/Experiencer\"][\"value\"]\n",
    "                                == \"Relative\"\n",
    "                                and current_col_meta[\"Presence\"][\"confidence\"]\n",
    "                                > confidence_threshold_presence\n",
    "                                and current_col_meta[\"Subject/Experiencer\"][\n",
    "                                    \"confidence\"\n",
    "                                ]\n",
    "                                > confidence_threshold_presence\n",
    "                                and annotations[\"entities\"][annotation_keys[j]][\"acc\"]\n",
    "                                > confidence_threshold_concept_accuracy\n",
    "                            ):\n",
    "                                if (\n",
    "                                    current_col_name + \"_count_relative_not_present\"\n",
    "                                    in df_pat_target.columns\n",
    "                                ):\n",
    "\n",
    "                                    df_pat_target.at[\n",
    "                                        i,\n",
    "                                        current_col_name\n",
    "                                        + \"_count_relative_not_present\",\n",
    "                                    ] = (\n",
    "                                        df_pat_target.loc[i][\n",
    "                                            current_col_name\n",
    "                                            + \"_count_relative_not_present\"\n",
    "                                        ]\n",
    "                                        + 1\n",
    "                                    )\n",
    "\n",
    "                                else:\n",
    "                                    df_pat_target[\n",
    "                                        current_col_name + \"_count_relative_not_present\"\n",
    "                                    ] = 1\n",
    "\n",
    "                    else:\n",
    "                        # OLD: set to nan instead of zero for impute and medcat precision persistence\n",
    "                        # Dont set anything here as this will overwrite existing 1 entries?\n",
    "                        pass\n",
    "                        # df_pat_target.at[i, current_col_name+\"_count_subject_present\"] = np.nan\n",
    "\n",
    "                if all_meta_anno:\n",
    "                    if len(list(current_col_meta.keys())) > 0:\n",
    "\n",
    "                        for key in current_col_meta.keys():\n",
    "\n",
    "                            sub_anot = current_col_meta.get(key)\n",
    "                            if len(list(sub_anot.keys())) > 0:\n",
    "                                for sub_key in sub_anot.keys():\n",
    "                                    if sub_key in meta_sub_key_list:\n",
    "                                        try:\n",
    "\n",
    "                                            key_result = sub_anot.get(sub_key)\n",
    "                                            if type(key_result) is str:\n",
    "                                                key_result = annotation_map.get(\n",
    "                                                    key_result\n",
    "                                                )\n",
    "\n",
    "                                            # print(current_col_name+'_'+str(key)+'_'+str(sub_key), key_result, sub_anot.get(sub_key))\n",
    "\n",
    "                                            df_pat_target.at[\n",
    "                                                i,\n",
    "                                                current_col_name\n",
    "                                                + \"_\"\n",
    "                                                + str(key)\n",
    "                                                + \"_\"\n",
    "                                                + str(sub_key),\n",
    "                                            ] = key_result\n",
    "\n",
    "                                            meta_counter = meta_counter + 1\n",
    "                                        except Exception as e:\n",
    "                                            print(e)\n",
    "                                            pass\n",
    "\n",
    "                meta_counter = meta_counter + 1\n",
    "                entry_counter = entry_counter + 1\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "    update_pbar(\n",
    "        current_pat_client_id_code + \"_\" + str(target_date_range),\n",
    "        start_time,\n",
    "        5,\n",
    "        \"annotations\",\n",
    "        n_docs_to_annotate=n_docs_to_annotate,\n",
    "    )\n",
    "    # df_pat_target.drop(\"n\", axis=1, inplace=True)\n",
    "\n",
    "    # df_pat_target.to_csv(entry_file_name)\n",
    "    # print(f\"Made {entry_counter} entry_counter  entries\")\n",
    "    # print(f\"Made {meta_counter} meta_counter entries\")\n",
    "    # print(\"done\")\n",
    "    return df_pat_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pbar(\n",
    "    current_pat_client_id_code, start_time, stage_int, stage_str, **n_docs_to_annotate\n",
    "):\n",
    "    global colour_val\n",
    "    global t\n",
    "    global skipped_counter\n",
    "\n",
    "    # global skipped_counter\n",
    "    # colour_val = color_bars[stage_int] + stage_str\n",
    "    colour_val = Fore.GREEN + Style.BRIGHT + stage_str\n",
    "\n",
    "    if multi_process:\n",
    "\n",
    "        counter_disp = skipped_counter.value\n",
    "\n",
    "    else:\n",
    "        counter_disp = skipped_counter\n",
    "\n",
    "    t.set_description(\n",
    "        f\"s: {counter_disp} | {current_pat_client_id_code} | task: {colour_val} | {n_docs_to_annotate}\"\n",
    "    )\n",
    "    if (time.time() - start_time) > slow_execution_threshold_low:\n",
    "        t.colour = Fore.YELLOW\n",
    "        colour_val = Fore.YELLOW + stage_str\n",
    "        t.set_description(\n",
    "            f\"s: {counter_disp} | {current_pat_client_id_code} | task: {colour_val} | {n_docs_to_annotate}\"\n",
    "        )\n",
    "\n",
    "    elif (time.time() - start_time) > slow_execution_threshold_high:\n",
    "        t.colour = Fore.RED + Style.BRIGHT\n",
    "        colour_val = Fore.RED + Style.BRIGHT + stage_str\n",
    "        t.set_description(\n",
    "            f\"s: {counter_disp} | {current_pat_client_id_code} | task: {colour_val} | {n_docs_to_annotate}\"\n",
    "        )\n",
    "\n",
    "    elif (time.time() - start_time) > slow_execution_threshold_extreme:\n",
    "        t.colour = Fore.RED + Style.DIM\n",
    "        colour_val = Fore.RED + Style.DIM + stage_str\n",
    "        t.set_description(\n",
    "            f\"s: {counter_disp} | {current_pat_client_id_code} | task: {colour_val} | {n_docs_to_annotate}\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        t.colour = Fore.GREEN + Style.DIM\n",
    "        colour_val = Fore.GREEN + Style.DIM + stage_str\n",
    "        t.set_description(\n",
    "            f\"s: {counter_disp} | {current_pat_client_id_code} | task: {colour_val} | {n_docs_to_annotate}\"\n",
    "        )\n",
    "\n",
    "    t.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_pat_lines_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_list_start = [\n",
    "    x.replace(\".csv\", \"\") for x in list_dir_wrapper(current_pat_lines_path, sftp_client)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stripped_list_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_list = [\n",
    "    x.replace(\".csv\", \"\") for x in list_dir_wrapper(current_pat_lines_path, sftp_client)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_pat_annotations_to_file(current_pat_client_id_code, target_date_range):\n",
    "    start_time = time.time()\n",
    "\n",
    "    file_exists = exist_check(\n",
    "        pre_annotation_path + current_pat_client_id_code + \"_\" + str(target_date_range)\n",
    "    )\n",
    "\n",
    "    if not file_exists:\n",
    "\n",
    "        start_year, start_month, end_year, end_month = get_start_end_year_month(\n",
    "            target_date_range\n",
    "        )\n",
    "\n",
    "        current_pat_docs = cohort_searcher_with_terms_and_search(\n",
    "            index_name=\"epr_documents\",\n",
    "            fields_list=\"\"\"client_idcode document_guid\tdocument_description\tbody_analysed updatetime clientvisit_visitidcode\"\"\".split(),\n",
    "            term_name=\"client_idcode.keyword\",\n",
    "            entered_list=[current_pat_client_id_code],\n",
    "            search_string=f\"updatetime:[{start_year}-{start_month} TO {end_year}-{end_month}] \",\n",
    "        )\n",
    "\n",
    "        n_docs_to_annotate = len(current_pat_docs)\n",
    "        update_pbar(\n",
    "            current_pat_client_id_code + \"_\" + str(target_date_range),\n",
    "            start_time,\n",
    "            5,\n",
    "            \"annotations\",\n",
    "            n_docs_to_annotate=n_docs_to_annotate,\n",
    "        )\n",
    "\n",
    "\n",
    "    # remove filter from cdb?\n",
    "    # print(\"getting annotations\")\n",
    "\n",
    "    #     file_exists = exists(pre_annotation_path + current_pat_client_id_code)\n",
    "\n",
    "    if not file_exists:\n",
    "        with io.capture_output():\n",
    "            pats_anno_annotations = cat.get_entities_multi_texts(\n",
    "                current_pat_docs[\"body_analysed\"].dropna()\n",
    "            )\n",
    "            # , n_process=1\n",
    "        if store_annot:\n",
    "            dump_results(\n",
    "                pats_anno_annotations,\n",
    "                pre_annotation_path\n",
    "                + current_pat_client_id_code\n",
    "                + \"_\"\n",
    "                + str(target_date_range),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataframe_by_timestamp(\n",
    "    df, start_year, start_month, end_year, end_month, timestamp_string\n",
    "):\n",
    "    # Convert timestamp column to datetime format\n",
    "    df[timestamp_string] = pd.to_datetime(df[timestamp_string], utc=True)\n",
    "\n",
    "    # imputed from elastic. Mirror:\n",
    "    start_day = 1\n",
    "    end_day = 1\n",
    "    hour = 23\n",
    "    minute = 59\n",
    "    second = 59\n",
    "\n",
    "    # Filter based on year and month ranges\n",
    "    filtered_df = df[\n",
    "        (\n",
    "            df[timestamp_string]\n",
    "            >= str(\n",
    "                datetime(start_year, int(start_month), start_day, hour, minute, second)\n",
    "            )\n",
    "        )\n",
    "        & (\n",
    "            df[timestamp_string]\n",
    "            <= str(datetime(end_year, int(end_month), end_day, hour, minute, second))\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_pat_annotations_mct_batch_to_file(\n",
    "    current_pat_client_id_code,\n",
    "    target_date_range,\n",
    "    pat_doc_batch,\n",
    "    sftp_obj=None,\n",
    "    skip_check=False,\n",
    "):\n",
    "    start_time = time.time()\n",
    "\n",
    "    current_annot_file_path = (\n",
    "        pre_annotation_path_mrc\n",
    "        + current_pat_client_id_code\n",
    "        + \"/\"\n",
    "        + current_pat_client_id_code\n",
    "        + \"_\"\n",
    "        + str(target_date_range)\n",
    "    )\n",
    "\n",
    "    if skip_check:\n",
    "        file_exists = False\n",
    "    else:\n",
    "        file_exists = exist_check(current_annot_file_path, sftp_obj)\n",
    "\n",
    "    if not file_exists:\n",
    "\n",
    "        start_year, start_month, end_year, end_month = get_start_end_year_month(\n",
    "            target_date_range\n",
    "        )\n",
    "\n",
    "        current_pat_docs = filter_dataframe_by_timestamp(\n",
    "            pat_doc_batch,\n",
    "            start_year,\n",
    "            start_month,\n",
    "            end_year,\n",
    "            end_month,\n",
    "            \"observationdocument_recordeddtm\",\n",
    "        )\n",
    "\n",
    "        n_docs_to_annotate = len(current_pat_docs)\n",
    "        update_pbar(\n",
    "            current_pat_client_id_code + \"_\" + str(target_date_range),\n",
    "            start_time,\n",
    "            5,\n",
    "            \"annotations_mct\",\n",
    "            n_docs_to_annotate=n_docs_to_annotate,\n",
    "        )\n",
    "\n",
    "    if not file_exists:\n",
    "        with io.capture_output():\n",
    "            pats_anno_annotations = cat.get_entities_multi_texts(\n",
    "                current_pat_docs[\"observation_valuetext_analysed\"].dropna()\n",
    "            )\n",
    "            # , n_process=1\n",
    "        if store_annot:\n",
    "            dump_results(pats_anno_annotations, current_annot_file_path, sftp_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_pat_annotations_batch_to_file(\n",
    "    current_pat_client_id_code,\n",
    "    target_date_range,\n",
    "    pat_doc_batch,\n",
    "    sftp_obj=None,\n",
    "    skip_check=False,\n",
    "):\n",
    "    start_time = time.time()\n",
    "\n",
    "    current_annotation_file_path = (\n",
    "        pre_annotation_path\n",
    "        + current_pat_client_id_code\n",
    "        + \"/\"\n",
    "        + current_pat_client_id_code\n",
    "        + \"_\"\n",
    "        + str(target_date_range)\n",
    "    )\n",
    "\n",
    "    if skip_check:\n",
    "        file_exists = False\n",
    "    else:\n",
    "        file_exists = exist_check(current_annotation_file_path, sftp_obj)\n",
    "\n",
    "    if not file_exists:\n",
    "\n",
    "        start_year, start_month, end_year, end_month = get_start_end_year_month(\n",
    "            target_date_range\n",
    "        )\n",
    "\n",
    "        current_pat_docs = filter_dataframe_by_timestamp(\n",
    "            pat_doc_batch, start_year, start_month, end_year, end_month, \"updatetime\"\n",
    "        )\n",
    "\n",
    "        n_docs_to_annotate = len(current_pat_docs)\n",
    "        update_pbar(\n",
    "            current_pat_client_id_code + \"_\" + str(target_date_range),\n",
    "            start_time,\n",
    "            5,\n",
    "            \"annotations\",\n",
    "            n_docs_to_annotate=n_docs_to_annotate,\n",
    "        )\n",
    "\n",
    "\n",
    "    # remove filter from cdb?\n",
    "    # print(\"getting annotations\")\n",
    "\n",
    "    #     file_exists = exists(pre_annotation_path + current_pat_client_id_code)\n",
    "\n",
    "    if not file_exists:\n",
    "        with io.capture_output():\n",
    "            pats_anno_annotations = cat.get_entities_multi_texts(\n",
    "                current_pat_docs[\"body_analysed\"].dropna()\n",
    "            )\n",
    "            # , n_process=1\n",
    "        if store_annot:\n",
    "            dump_results(pats_anno_annotations, current_annotation_file_path, sftp_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_demographics3_batch(patlist, target_date_range, pat_batch):\n",
    "\n",
    "    start_year, start_month, end_year, end_month = get_start_end_year_month(\n",
    "        target_date_range\n",
    "    )\n",
    "\n",
    "    if batch_mode:\n",
    "\n",
    "        demo = filter_dataframe_by_timestamp(\n",
    "            pat_batch, start_year, start_month, end_year, end_month, \"updatetime\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        demo = cohort_searcher_with_terms_and_search(\n",
    "            index_name=\"epr_documents\",\n",
    "            fields_list=[\n",
    "                \"client_idcode\",\n",
    "                \"client_firstname\",\n",
    "                \"client_lastname\",\n",
    "                \"client_dob\",\n",
    "                \"client_gendercode\",\n",
    "                \"client_racecode\",\n",
    "                \"client_deceaseddtm\",\n",
    "                \"updatetime\",\n",
    "            ],\n",
    "            term_name=\"client_idcode.keyword\",\n",
    "            entered_list=patlist,\n",
    "            search_string=f\"updatetime:[{start_year}-{start_month} TO {end_year}-{end_month}] \",\n",
    "        )\n",
    "\n",
    "    demo[\"updatetime\"] = pd.to_datetime(demo[\"updatetime\"], utc=True)\n",
    "    demo = demo.sort_values(\n",
    "        [\"client_idcode\", \"updatetime\"]\n",
    "    )  # .drop_duplicates(subset = [\"client_idcode\"], keep = \"last\", inplace = True)\n",
    "\n",
    "    # if more than one in the range return the nearest the end of the period\n",
    "    if len(demo) > 1:\n",
    "        try:\n",
    "            # print(\"case1\")\n",
    "            return demo.tail(1)\n",
    "            # return demo.iloc[-1].to_frame()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    # if only one return it\n",
    "    elif len(demo) == 1:\n",
    "        return demo\n",
    "\n",
    "    # otherwise return only the client id\n",
    "    else:\n",
    "        demo = pd.DataFrame(data=None, columns=None)\n",
    "        demo[\"client_idcode\"] = patlist\n",
    "        return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_demo(current_pat_client_id_code, target_date_range, pat_batch):\n",
    "\n",
    "    current_pat_demo = get_demographics3_batch(\n",
    "        [current_pat_client_id_code], target_date_range, pat_batch\n",
    "    )\n",
    "\n",
    "    # display(current_pat_demo)\n",
    "\n",
    "    # print(len(current_pat_demo.columns))\n",
    "\n",
    "    if len(current_pat_demo.columns) > 1:\n",
    "\n",
    "        current_pat_demo = append_age_at_record_series(current_pat_demo)\n",
    "\n",
    "        # demo_dataframe = pd.DataFrame(current_pat_demo).T\n",
    "\n",
    "        # demo_dataframe.reset_index(inplace=True)\n",
    "\n",
    "        demo_dataframe = current_pat_demo.copy()\n",
    "\n",
    "        current_pat_demo = EthnicityAbstractor.abstractEthnicity(\n",
    "            demo_dataframe,\n",
    "            outputNameString=\"_census\",\n",
    "            ethnicityColumnString=\"client_racecode\",\n",
    "        )\n",
    "\n",
    "        dummied_dummy_ethnicity_dataframe = pd.DataFrame(\n",
    "            data=[np.zeros(5)],\n",
    "            columns=[\n",
    "                \"census_white\",\n",
    "                \"census_asian_or_asian_british\",\n",
    "                \"census_black_african_caribbean_or_black_british\",\n",
    "                \"census_mixed_or_multiple_ethnic_groups\",\n",
    "                \"census_other_ethnic_group\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        cen_res = pd.get_dummies(current_pat_demo[\"census\"], prefix=\"census\")\n",
    "\n",
    "        dummied_dummy_ethnicity_dataframe[cen_res.columns[0]] = cen_res[\n",
    "            cen_res.columns[0]\n",
    "        ]\n",
    "\n",
    "        abstrated_ethnicity_dummied = dummied_dummy_ethnicity_dataframe\n",
    "\n",
    "        # abstrated_ethnicity_dummied = pd.concat([dummied_dummy_ethnicity_dataframe, pd.get_dummies(current_pat_demo['census'], prefix = 'census')], axis=1)\n",
    "\n",
    "        current_pat_demo = pd.concat(\n",
    "            [current_pat_demo.reset_index(), abstrated_ethnicity_dummied], axis=1\n",
    "        )\n",
    "\n",
    "        current_pat_demo.reset_index(inplace=True)\n",
    "\n",
    "        # current_pat_demo\n",
    "\n",
    "        sex_map = {\"Male\": 1, \"Female\": 0, \"male\": 1, \"female\": 0}\n",
    "\n",
    "        #         def is_dead(line):\n",
    "        #             try:\n",
    "        #                 if(type(line)==str or type(line)==float):\n",
    "        #                     return np.isnan(line)\n",
    "        #                 else:\n",
    "        #                     return line.isnull\n",
    "        #             except Exception as e:\n",
    "        #                 print(e)\n",
    "        #                 print(type(line))\n",
    "        #                 print(line)\n",
    "        #         def is_dead(line):\n",
    "        #             return line.isna()\n",
    "\n",
    "        current_pat_demo[\"male\"] = current_pat_demo[\"client_gendercode\"].map(sex_map)\n",
    "\n",
    "        # current_pat_demo['dead'] = current_pat_demo['client_deceaseddtm'].apply(is_dead)\n",
    "\n",
    "        # current_pat_demo['dead'] = int(int((np.isnan(current_pat_demo['client_deceaseddtm'].iloc[0])))!=1)\n",
    "\n",
    "        current_pat_demo[\"dead\"] = int(\n",
    "            type(current_pat_demo[\"client_deceaseddtm\"]) == str\n",
    "        )\n",
    "\n",
    "        current_pat_demo = current_pat_demo[\n",
    "            [\n",
    "                \"client_idcode\",\n",
    "                \"male\",\n",
    "                \"age\",\n",
    "                \"dead\",\n",
    "                \"census_white\",\n",
    "                \"census_asian_or_asian_british\",\n",
    "                \"census_black_african_caribbean_or_black_british\",\n",
    "                \"census_mixed_or_multiple_ethnic_groups\",\n",
    "                \"census_other_ethnic_group\",\n",
    "            ]\n",
    "        ].copy()\n",
    "\n",
    "        current_pat_demo[\"dead\"] = current_pat_demo[\"dead\"].astype(int)\n",
    "\n",
    "        current_pat_demo[\"age\"] = current_pat_demo[\"age\"].astype(int)\n",
    "\n",
    "        current_pat_demo[\"dead\"] = current_pat_demo[\"dead\"].astype(float)\n",
    "\n",
    "        current_pat_demo[\"age\"] = current_pat_demo[\"age\"].astype(float)\n",
    "\n",
    "        current_pat_demo[\"male\"] = current_pat_demo[\"male\"].astype(float)\n",
    "\n",
    "        return current_pat_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smoking(current_pat_client_id_code, target_date_range, pat_batch):\n",
    "    start_year, start_month, end_year, end_month = get_start_end_year_month(\n",
    "        target_date_range\n",
    "    )\n",
    "\n",
    "    search_term = \"CORE_SmokingStatus\"\n",
    "\n",
    "    if batch_mode:\n",
    "        current_pat_raw = filter_dataframe_by_timestamp(\n",
    "            pat_batch,\n",
    "            start_year,\n",
    "            start_month,\n",
    "            end_year,\n",
    "            end_month,\n",
    "            \"observationdocument_recordeddtm\",\n",
    "        )\n",
    "\n",
    "    else:\n",
    "\n",
    "        current_pat_raw = cohort_searcher_with_terms_and_search(\n",
    "            index_name=\"observations\",\n",
    "            fields_list=\"\"\"observation_guid client_idcode\tobscatalogmasteritem_displayname\tobservation_valuetext_analysed\tobservationdocument_recordeddtm clientvisit_visitidcode\"\"\".split(),\n",
    "            term_name=\"client_idcode.keyword\",\n",
    "            entered_list=[current_pat_client_id_code],\n",
    "            #                                                                 search_string=f\"obscatalogmasteritem_displayname:(\\\"{search_term}\\\")\")\n",
    "            search_string=f'obscatalogmasteritem_displayname:(\"{search_term}\") AND '\n",
    "            + f\"observationdocument_recordeddtm:[{start_year}-{start_month} TO {end_year}-{end_month}]\",\n",
    "        )\n",
    "    if len(current_pat_raw) == 0:\n",
    "\n",
    "        features = pd.DataFrame(\n",
    "            data=[current_pat_client_id_code], columns=[\"client_idcode\"]\n",
    "        )\n",
    "\n",
    "    features_data = current_pat_raw[\n",
    "        current_pat_raw[\"obscatalogmasteritem_displayname\"] == search_term\n",
    "    ].copy()\n",
    "\n",
    "    # screen and purge dud values\n",
    "    # features_data =  features_data[(features_data['observation_valuetext_analysed'].astype(float)<20)& (features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    features_data.dropna(inplace=True)\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    features_data = current_pat_raw[\n",
    "        current_pat_raw[\"obscatalogmasteritem_displayname\"] == search_term\n",
    "    ].copy()\n",
    "    # features_data =  features_data[(features_data['observation_valuetext_analysed'].astype(float)<20)& (features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    # features_data.dropna(inplace=True)\n",
    "\n",
    "    term = \"smoking_status\".lower()\n",
    "\n",
    "    if len(features_data) > 0:\n",
    "        features = pd.DataFrame(\n",
    "            data=[current_pat_client_id_code], columns=[\"client_idcode\"]\n",
    "        ).copy()\n",
    "\n",
    "        value_array = features_data[\"observation_valuetext_analysed\"].dropna()\n",
    "\n",
    "        features[f\"{term}_current\"] = value_array.str.contains(\"Current Smoker\").astype(\n",
    "            int\n",
    "        )\n",
    "        features[f\"{term}_non\"] = value_array.str.contains(\"Non-Smoker\").astype(int)\n",
    "\n",
    "    else:\n",
    "        features = pd.DataFrame(\n",
    "            data=[current_pat_client_id_code], columns=[\"client_idcode\"]\n",
    "        ).copy()\n",
    "        features[f\"{term}_current\"] = np.nan\n",
    "        features[f\"{term}_non\"] = np.nan\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_core_02(current_pat_client_id_code, target_date_range, pat_batch):\n",
    "\n",
    "    start_year, start_month, end_year, end_month = get_start_end_year_month(\n",
    "        target_date_range\n",
    "    )\n",
    "\n",
    "    search_term = \"CORE_SpO2\"\n",
    "\n",
    "    if batch_mode:\n",
    "        current_pat_raw = filter_dataframe_by_timestamp(\n",
    "            pat_batch,\n",
    "            start_year,\n",
    "            start_month,\n",
    "            end_year,\n",
    "            end_month,\n",
    "            \"observationdocument_recordeddtm\",\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        current_pat_raw = cohort_searcher_with_terms_and_search(\n",
    "            index_name=\"observations\",\n",
    "            fields_list=\"\"\"observation_guid client_idcode\tobscatalogmasteritem_displayname\tobservation_valuetext_analysed\tobservationdocument_recordeddtm clientvisit_visitidcode\"\"\".split(),\n",
    "            term_name=\"client_idcode.keyword\",\n",
    "            entered_list=[current_pat_client_id_code],\n",
    "            search_string=f'obscatalogmasteritem_displayname:(\"{search_term}\") AND '\n",
    "            + f\"observationdocument_recordeddtm:[{start_year}-{start_month} TO {end_year}-{end_month}]\",\n",
    "        )\n",
    "\n",
    "    if len(current_pat_raw) == 0:\n",
    "\n",
    "        features = pd.DataFrame(\n",
    "            data=[current_pat_client_id_code], columns=[\"client_idcode\"]\n",
    "        )\n",
    "\n",
    "    features_data = current_pat_raw[\n",
    "        current_pat_raw[\"obscatalogmasteritem_displayname\"] == search_term\n",
    "    ].copy()\n",
    "\n",
    "    # screen and purge dud values\n",
    "    # features_data =  features_data[(features_data['observation_valuetext_analysed'].astype(float)<20)& (features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    features_data.dropna(inplace=True)\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    features_data = current_pat_raw[\n",
    "        current_pat_raw[\"obscatalogmasteritem_displayname\"] == search_term\n",
    "    ].copy()\n",
    "    # features_data =  features_data[(features_data['observation_valuetext_analysed'].astype(float)<20)& (features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    # features_data.dropna(inplace=True)\n",
    "\n",
    "    term = \"core_sp_02\".lower()\n",
    "\n",
    "    if len(features_data) > 0:\n",
    "        features = pd.DataFrame(\n",
    "            data=[current_pat_client_id_code], columns=[\"client_idcode\"]\n",
    "        ).copy()\n",
    "\n",
    "        all_terms = list(\n",
    "            features_data[\"observation_valuetext_analysed\"].dropna().unique()\n",
    "        )\n",
    "\n",
    "        for term in all_terms:\n",
    "\n",
    "            features[f'{term.replace(\"-\", \"_\").replace(\"%\", \"pct\")}'] = 1\n",
    "            # features[f'{bed_term}'] = 1\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bed(current_pat_client_id_code, target_date_range, pat_batch):\n",
    "\n",
    "    start_year, start_month, end_year, end_month = get_start_end_year_month(\n",
    "        target_date_range\n",
    "    )\n",
    "\n",
    "    search_term = \"CORE_BedNumber3\"\n",
    "\n",
    "    if batch_mode:\n",
    "        current_pat_raw = filter_dataframe_by_timestamp(\n",
    "            pat_batch,\n",
    "            start_year,\n",
    "            start_month,\n",
    "            end_year,\n",
    "            end_month,\n",
    "            \"observationdocument_recordeddtm\",\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        current_pat_raw = cohort_searcher_with_terms_and_search(\n",
    "            index_name=\"observations\",\n",
    "            fields_list=\"\"\"observation_guid client_idcode\tobscatalogmasteritem_displayname\tobservation_valuetext_analysed\tobservationdocument_recordeddtm clientvisit_visitidcode\"\"\".split(),\n",
    "            term_name=\"client_idcode.keyword\",\n",
    "            entered_list=[current_pat_client_id_code],\n",
    "            search_string=f'obscatalogmasteritem_displayname:(\"{search_term}\") AND '\n",
    "            + f\"observationdocument_recordeddtm:[{start_year}-{start_month} TO {end_year}-{end_month}]\",\n",
    "        )\n",
    "\n",
    "    if len(current_pat_raw) == 0:\n",
    "\n",
    "        features = pd.DataFrame(\n",
    "            data=[current_pat_client_id_code], columns=[\"client_idcode\"]\n",
    "        )\n",
    "\n",
    "    features_data = current_pat_raw[\n",
    "        current_pat_raw[\"obscatalogmasteritem_displayname\"] == search_term\n",
    "    ].copy()\n",
    "\n",
    "    # screen and purge dud values\n",
    "    # features_data =  features_data[(features_data['observation_valuetext_analysed'].astype(float)<20)& (features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    features_data.dropna(inplace=True)\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    features_data = current_pat_raw[\n",
    "        current_pat_raw[\"obscatalogmasteritem_displayname\"] == search_term\n",
    "    ].copy()\n",
    "    # features_data =  features_data[(features_data['observation_valuetext_analysed'].astype(float)<20)& (features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    # features_data.dropna(inplace=True)\n",
    "\n",
    "    \"bed\".lower()\n",
    "\n",
    "    if len(features_data) > 0:\n",
    "        features = pd.DataFrame(\n",
    "            data=[current_pat_client_id_code], columns=[\"client_idcode\"]\n",
    "        ).copy()\n",
    "\n",
    "        all_bed_terms = list(\n",
    "            features_data[\"observation_valuetext_analysed\"].dropna().unique()\n",
    "        )\n",
    "\n",
    "        for bed_term in all_bed_terms:\n",
    "\n",
    "            features[f\"bed_{bed_term}\"] = 1\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vte_status(current_pat_client_id_code, target_date_range, pat_batch):\n",
    "\n",
    "    start_year, start_month, end_year, end_month = get_start_end_year_month(\n",
    "        target_date_range\n",
    "    )\n",
    "\n",
    "    search_term = \"CORE_VTE_STATUS\"\n",
    "\n",
    "    if batch_mode:\n",
    "        current_pat_raw = filter_dataframe_by_timestamp(\n",
    "            pat_batch,\n",
    "            start_year,\n",
    "            start_month,\n",
    "            end_year,\n",
    "            end_month,\n",
    "            \"observationdocument_recordeddtm\",\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        current_pat_raw = cohort_searcher_with_terms_and_search(\n",
    "            index_name=\"observations\",\n",
    "            fields_list=\"\"\"observation_guid client_idcode\tobscatalogmasteritem_displayname\tobservation_valuetext_analysed\tobservationdocument_recordeddtm clientvisit_visitidcode\"\"\".split(),\n",
    "            term_name=\"client_idcode.keyword\",\n",
    "            entered_list=[current_pat_client_id_code],\n",
    "            search_string=f'obscatalogmasteritem_displayname:(\"{search_term}\") AND '\n",
    "            + f\"observationdocument_recordeddtm:[{start_year}-{start_month} TO {end_year}-{end_month}]\",\n",
    "        )\n",
    "\n",
    "    if len(current_pat_raw) == 0:\n",
    "\n",
    "        features = pd.DataFrame(\n",
    "            data=[current_pat_client_id_code], columns=[\"client_idcode\"]\n",
    "        )\n",
    "\n",
    "    features_data = current_pat_raw[\n",
    "        current_pat_raw[\"obscatalogmasteritem_displayname\"] == search_term\n",
    "    ].copy()\n",
    "\n",
    "    # screen and purge dud values\n",
    "    # features_data =  features_data[(features_data['observation_valuetext_analysed'].astype(float)<20)& (features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    features_data.dropna(inplace=True)\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    features_data = current_pat_raw[\n",
    "        current_pat_raw[\"obscatalogmasteritem_displayname\"] == search_term\n",
    "    ].copy()\n",
    "    # features_data =  features_data[(features_data['observation_valuetext_analysed'].astype(float)<20)& (features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    # features_data.dropna(inplace=True)\n",
    "\n",
    "    term = \"VTE_Status\".lower()\n",
    "\n",
    "    if len(features_data) > 0:\n",
    "        features = pd.DataFrame(\n",
    "            data=[current_pat_client_id_code], columns=[\"client_idcode\"]\n",
    "        ).copy()\n",
    "\n",
    "        di = {\n",
    "            \"High risk of VTE High risk of bleeding\": 1,\n",
    "            \"High risk of VTE Low risk of bleeding\": 0,\n",
    "        }\n",
    "\n",
    "        value_array = features_data[\"observation_valuetext_analysed\"].map(di)\n",
    "\n",
    "        value_array = value_array.astype(float)\n",
    "\n",
    "        features[f\"{term}_mean\"] = value_array.mean()\n",
    "        features[f\"{term}_median\"] = value_array.median()\n",
    "        features[f\"{term}_std\"] = value_array.std()\n",
    "        features[f\"{term}_max\"] = max(value_array)\n",
    "        features[f\"{term}_min\"] = min(value_array)\n",
    "        features[f\"{term}_n\"] = value_array.shape[0]\n",
    "    else:\n",
    "        features = pd.DataFrame(\n",
    "            data=[current_pat_client_id_code], columns=[\"client_idcode\"]\n",
    "        ).copy()\n",
    "        features[f\"{term}_mean\"] = np.nan\n",
    "        features[f\"{term}_median\"] = np.nan\n",
    "        features[f\"{term}_std\"] = np.nan\n",
    "        features[f\"{term}_max\"] = np.nan\n",
    "        features[f\"{term}_min\"] = np.nan\n",
    "        features[f\"{term}_n\"] = np.nan\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hosp_site(current_pat_client_id_code, target_date_range, pat_batch):\n",
    "\n",
    "    start_year, start_month, end_year, end_month = get_start_end_year_month(\n",
    "        target_date_range\n",
    "    )\n",
    "\n",
    "    search_term = \"CORE_HospitalSite\"\n",
    "    if batch_mode:\n",
    "        current_pat_raw = filter_dataframe_by_timestamp(\n",
    "            pat_batch,\n",
    "            start_year,\n",
    "            start_month,\n",
    "            end_year,\n",
    "            end_month,\n",
    "            \"observationdocument_recordeddtm\",\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        current_pat_raw = cohort_searcher_with_terms_and_search(\n",
    "            index_name=\"observations\",\n",
    "            fields_list=\"\"\"observation_guid client_idcode\tobscatalogmasteritem_displayname\tobservation_valuetext_analysed\tobservationdocument_recordeddtm clientvisit_visitidcode\"\"\".split(),\n",
    "            term_name=\"client_idcode.keyword\",\n",
    "            entered_list=[current_pat_client_id_code],\n",
    "            search_string=f'obscatalogmasteritem_displayname:(\"{search_term}\") AND '\n",
    "            + f\"observationdocument_recordeddtm:[{start_year}-{start_month} TO {end_year}-{end_month}]\",\n",
    "        )\n",
    "\n",
    "    if len(current_pat_raw) == 0:\n",
    "\n",
    "        features = pd.DataFrame(\n",
    "            data=[current_pat_client_id_code], columns=[\"client_idcode\"]\n",
    "        )\n",
    "\n",
    "    features_data = current_pat_raw[\n",
    "        current_pat_raw[\"obscatalogmasteritem_displayname\"] == search_term\n",
    "    ].copy()\n",
    "\n",
    "    # screen and purge dud values\n",
    "    # features_data =  features_data[(features_data['observation_valuetext_analysed'].astype(float)<20)& (features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    features_data.dropna(inplace=True)\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    features_data = current_pat_raw[\n",
    "        current_pat_raw[\"obscatalogmasteritem_displayname\"] == search_term\n",
    "    ].copy()\n",
    "    # features_data =  features_data[(features_data['observation_valuetext_analysed'].astype(float)<20)& (features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    # features_data.dropna(inplace=True)\n",
    "\n",
    "    term = \"hosp_site\".lower()\n",
    "\n",
    "    if len(features_data) > 0:\n",
    "        features = pd.DataFrame(\n",
    "            data=[current_pat_client_id_code], columns=[\"client_idcode\"]\n",
    "        ).copy()\n",
    "        # value_array = features_data['observation_valuetext_analysed'].astype(float)\n",
    "        value_array = features_data[\"observation_valuetext_analysed\"].dropna()\n",
    "\n",
    "        features[f\"{term}_dh\"] = value_array.str.contains(\"DH\").astype(int)\n",
    "        features[f\"{term}_ph\"] = value_array.str.contains(\"PRUH\").astype(int)\n",
    "\n",
    "    else:\n",
    "        features = pd.DataFrame(\n",
    "            data=[current_pat_client_id_code], columns=[\"client_idcode\"]\n",
    "        ).copy()\n",
    "        features[f\"{term}_dh\"] = np.nan\n",
    "        features[f\"{term}_ph\"] = np.nan\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_core_resus(current_pat_client_id_code, target_date_range, pat_batch):\n",
    "\n",
    "    start_year, start_month, end_year, end_month = get_start_end_year_month(\n",
    "        target_date_range\n",
    "    )\n",
    "\n",
    "    if batch_mode:\n",
    "        current_pat_raw = filter_dataframe_by_timestamp(\n",
    "            pat_batch,\n",
    "            start_year,\n",
    "            start_month,\n",
    "            end_year,\n",
    "            end_month,\n",
    "            \"observationdocument_recordeddtm\",\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        current_pat_raw = cohort_searcher_with_terms_and_search(\n",
    "            index_name=\"observations\",\n",
    "            fields_list=\"\"\"observation_guid client_idcode\tobscatalogmasteritem_displayname\tobservation_valuetext_analysed\tobservationdocument_recordeddtm clientvisit_visitidcode\"\"\".split(),\n",
    "            term_name=\"client_idcode.keyword\",\n",
    "            entered_list=[current_pat_client_id_code],\n",
    "            search_string='obscatalogmasteritem_displayname:(\"CORE_RESUS_STATUS\") AND '\n",
    "            + f\"observationdocument_recordeddtm:[{start_year}-{start_month} TO {end_year}-{end_month}]\",\n",
    "        )\n",
    "\n",
    "    if len(current_pat_raw) == 0:\n",
    "\n",
    "        features = pd.DataFrame(\n",
    "            data=[current_pat_client_id_code], columns=[\"client_idcode\"]\n",
    "        )\n",
    "\n",
    "    features_data = current_pat_raw[\n",
    "        current_pat_raw[\"obscatalogmasteritem_displayname\"] == \"CORE_RESUS_STATUS\"\n",
    "    ].copy()\n",
    "\n",
    "    # screen and purge dud values\n",
    "    # features_data =  features_data[(features_data['observation_valuetext_analysed'].astype(float)<20)& (features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    # features_data.dropna(inplace=True)\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    # features_data = current_pat_raw[current_pat_raw['obscatalogmasteritem_displayname']=='CORE_RESUS_STATUS'].copy()\n",
    "    # features_data =  features_data[(features_data['observation_valuetext_analysed'].astype(float)<20)& (features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    # features_data.dropna(inplace=True)\n",
    "\n",
    "    term = \"CORE_RESUS_STATUS\".lower()\n",
    "\n",
    "    if len(features_data) > 0:\n",
    "        features = pd.DataFrame(\n",
    "            data=[current_pat_client_id_code], columns=[\"client_idcode\"]\n",
    "        ).copy()\n",
    "        features[f\"{term}_For cardiopulmonary resuscitation\"] = len(\n",
    "            features_data[\n",
    "                features_data[\"observation_valuetext_analysed\"]\n",
    "                == \"For cardiopulmonary resuscitation\"\n",
    "            ]\n",
    "        )\n",
    "        features[f\"{term}_Not for cardiopulmonary resuscitation\"] = len(\n",
    "            features_data[\n",
    "                features_data[\"observation_valuetext_analysed\"]\n",
    "                == \"Not for cardiopulmonary resuscitation\"\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        features = pd.DataFrame(\n",
    "            data=[current_pat_client_id_code], columns=[\"client_idcode\"]\n",
    "        ).copy()\n",
    "        features[f\"{term}_For cardiopulmonary resuscitation\"] = len(\n",
    "            features_data[\n",
    "                features_data[\"observation_valuetext_analysed\"]\n",
    "                == \"For cardiopulmonary resuscitation\"\n",
    "            ]\n",
    "        )\n",
    "        features[f\"{term}_Not for cardiopulmonary resuscitation\"] = len(\n",
    "            features_data[\n",
    "                features_data[\"observation_valuetext_analysed\"]\n",
    "                == \"Not for cardiopulmonary resuscitation\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(current_pat_client_id_code, target_date_range, pat_batch):\n",
    "\n",
    "    start_year, start_month, end_year, end_month = get_start_end_year_month(\n",
    "        target_date_range\n",
    "    )\n",
    "\n",
    "    if batch_mode:\n",
    "        current_pat_raw_news = filter_dataframe_by_timestamp(\n",
    "            pat_batch,\n",
    "            start_year,\n",
    "            start_month,\n",
    "            end_year,\n",
    "            end_month,\n",
    "            \"observationdocument_recordeddtm\",\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        current_pat_raw_news = cohort_searcher_with_terms_and_search(\n",
    "            index_name=\"observations\",\n",
    "            fields_list=\"\"\"observation_guid client_idcode\tobscatalogmasteritem_displayname\tobservation_valuetext_analysed\tobservationdocument_recordeddtm clientvisit_visitidcode\"\"\".split(),\n",
    "            term_name=\"client_idcode.keyword\",\n",
    "            entered_list=[current_pat_client_id_code],\n",
    "            search_string='obscatalogmasteritem_displayname:(\"NEWS\" OR \"NEWS2\") AND '\n",
    "            + f\"observationdocument_recordeddtm:[{start_year}-{start_month} TO {end_year}-{end_month}]\",\n",
    "        )\n",
    "\n",
    "    # if(len(current_pat_raw_news)==0):\n",
    "\n",
    "    news_features = pd.DataFrame(\n",
    "        data=[current_pat_client_id_code], columns=[\"client_idcode\"]\n",
    "    )\n",
    "\n",
    "    news_features_data = current_pat_raw_news[\n",
    "        current_pat_raw_news[\"obscatalogmasteritem_displayname\"] == \"NEWS2_Score\"\n",
    "    ].copy()\n",
    "\n",
    "    # screen and purge dud values\n",
    "    news_features_data = news_features_data[\n",
    "        (news_features_data[\"observation_valuetext_analysed\"].astype(float) < 20)\n",
    "        & (news_features_data[\"observation_valuetext_analysed\"].astype(float) > -20)\n",
    "    ].copy()\n",
    "\n",
    "    if len(news_features_data) > 0:\n",
    "        news_features = pd.DataFrame(\n",
    "            data=[current_pat_client_id_code], columns=[\"client_idcode\"]\n",
    "        ).copy()\n",
    "        value_array = (\n",
    "            news_features_data[\"observation_valuetext_analysed\"].dropna().astype(float)\n",
    "        )\n",
    "        news_features[\"news_score_mean\"] = value_array.mean()\n",
    "        news_features[\"news_score_median\"] = value_array.median()\n",
    "        news_features[\"news_score_std\"] = value_array.std()\n",
    "        news_features[\"news_score_max\"] = max(value_array)\n",
    "        news_features[\"news_score_min\"] = min(value_array)\n",
    "        news_features[\"news_score_n\"] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features[\"news_score_mean\"] = np.nan\n",
    "        news_features[\"news_score_median\"] = np.nan\n",
    "        news_features[\"news_score_std\"] = np.nan\n",
    "        news_features[\"news_score_max\"] = np.nan\n",
    "        news_features[\"news_score_min\"] = np.nan\n",
    "        news_features[\"news_score_n\"] = np.nan\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    news_features_data = current_pat_raw_news[\n",
    "        current_pat_raw_news[\"obscatalogmasteritem_displayname\"] == \"NEWS_Systolic_BP\"\n",
    "    ].copy()\n",
    "    # news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    news_features_data.dropna(subset=[\"observation_valuetext_analysed\"], inplace=True)\n",
    "\n",
    "    if len(news_features_data) > 0:\n",
    "        # news_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        value_array = (\n",
    "            news_features_data[\"observation_valuetext_analysed\"]\n",
    "            .dropna()\n",
    "            .dropna()\n",
    "            .astype(float)\n",
    "        )\n",
    "        news_features[\"news_systolic_bp_mean\"] = value_array.mean()\n",
    "        news_features[\"news_systolic_bp_median\"] = value_array.median()\n",
    "        news_features[\"news_systolic_bp_std\"] = value_array.std()\n",
    "        news_features[\"news_systolic_bp_max\"] = max(value_array)\n",
    "        news_features[\"news_systolic_bp_min\"] = min(value_array)\n",
    "        news_features[\"news_systolic_bp_n\"] = value_array.shape[0]\n",
    "    else:\n",
    "\n",
    "        news_features[\"news_systolic_bp_mean\"] = np.nan\n",
    "        news_features[\"news_systolic_bp_median\"] = np.nan\n",
    "        news_features[\"news_systolic_bp_std\"] = np.nan\n",
    "        news_features[\"news_systolic_bp_max\"] = np.nan\n",
    "        news_features[\"news_systolic_bp_min\"] = np.nan\n",
    "        news_features[\"news_systolic_bp_n\"] = np.nan\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    news_features_data = current_pat_raw_news[\n",
    "        current_pat_raw_news[\"obscatalogmasteritem_displayname\"] == \"NEWS_Diastolic_BP\"\n",
    "    ].copy()\n",
    "    # news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    news_features_data.dropna(subset=[\"observation_valuetext_analysed\"], inplace=True)\n",
    "\n",
    "    if len(news_features_data) > 0:\n",
    "        # news_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        value_array = (\n",
    "            news_features_data[\"observation_valuetext_analysed\"].dropna().astype(float)\n",
    "        )\n",
    "        news_features[\"news_diastolic_bp_mean\"] = value_array.mean()\n",
    "        news_features[\"news_diastolic_bp_median\"] = value_array.median()\n",
    "        news_features[\"news_diastolic_bp_std\"] = value_array.std()\n",
    "        news_features[\"news_diastolic_bp_max\"] = max(value_array)\n",
    "        news_features[\"news_diastolic_bp_min\"] = min(value_array)\n",
    "        news_features[\"news_diastolic_bp_n\"] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features[\"news_diastolic_bp_mean\"] = np.nan\n",
    "        news_features[\"news_diastolic_bp_median\"] = np.nan\n",
    "        news_features[\"news_diastolic_bp_std\"] = np.nan\n",
    "        news_features[\"news_diastolic_bp_max\"] = np.nan\n",
    "        news_features[\"news_diastolic_bp_min\"] = np.nan\n",
    "        news_features[\"news_diastolic_bp_n\"] = np.nan\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    news_features_data = current_pat_raw_news[\n",
    "        current_pat_raw_news[\"obscatalogmasteritem_displayname\"]\n",
    "        == \"NEWS_Respiration_Rate\"\n",
    "    ].copy()\n",
    "    # news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    news_features_data.dropna(subset=[\"observation_valuetext_analysed\"], inplace=True)\n",
    "\n",
    "    if len(news_features_data) > 0:\n",
    "        # news_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        value_array = (\n",
    "            news_features_data[\"observation_valuetext_analysed\"].dropna().astype(float)\n",
    "        )\n",
    "        news_features[\"news_respiration_rate_mean\"] = value_array.mean()\n",
    "        news_features[\"news_respiration_rate_median\"] = value_array.median()\n",
    "        news_features[\"news_respiration_rate_std\"] = value_array.std()\n",
    "        news_features[\"news_respiration_rate_max\"] = max(value_array)\n",
    "        news_features[\"news_respiration_rate_min\"] = min(value_array)\n",
    "        news_features[\"news_respiration_rate_n\"] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features[\"news_respiration_rate_mean\"] = np.nan\n",
    "        news_features[\"news_respiration_rate_median\"] = np.nan\n",
    "        news_features[\"news_respiration_rate_std\"] = np.nan\n",
    "        news_features[\"news_respiration_rate_max\"] = np.nan\n",
    "        news_features[\"news_respiration_rate_min\"] = np.nan\n",
    "        news_features[\"news_respiration_rate_n\"] = np.nan\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    news_features_data = current_pat_raw_news[\n",
    "        current_pat_raw_news[\"obscatalogmasteritem_displayname\"] == \"NEWS_Heart_Rate\"\n",
    "    ].copy()\n",
    "\n",
    "    news_features_data.dropna(subset=[\"observation_valuetext_analysed\"], inplace=True)\n",
    "\n",
    "    if len(news_features_data) > 0:\n",
    "\n",
    "        value_array = (\n",
    "            news_features_data[\"observation_valuetext_analysed\"].dropna().astype(float)\n",
    "        )\n",
    "        news_features[\"news_heart_rate_mean\"] = value_array.mean()\n",
    "        news_features[\"news_heart_rate_median\"] = value_array.median()\n",
    "        news_features[\"news_heart_rate_std\"] = value_array.std()\n",
    "        news_features[\"news_heart_rate_max\"] = max(value_array)\n",
    "        news_features[\"news_heart_rate_min\"] = min(value_array)\n",
    "        news_features[\"news_heart_rate_n\"] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features[\"news_heart_rate_mean\"] = np.nan\n",
    "        news_features[\"news_heart_rate_median\"] = np.nan\n",
    "        news_features[\"news_heart_rate_std\"] = np.nan\n",
    "        news_features[\"news_heart_rate_max\"] = np.nan\n",
    "        news_features[\"news_heart_rate_min\"] = np.nan\n",
    "        news_features[\"news_heart_rate_n\"] = np.nan\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    news_features_data = current_pat_raw_news[\n",
    "        current_pat_raw_news[\"obscatalogmasteritem_displayname\"]\n",
    "        == \"NEWS_Oxygen_Saturation\"\n",
    "    ].copy()\n",
    "    # news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    news_features_data.dropna(subset=[\"observation_valuetext_analysed\"], inplace=True)\n",
    "\n",
    "    if len(news_features_data) > 0:\n",
    "\n",
    "        value_array = (\n",
    "            news_features_data[\"observation_valuetext_analysed\"].dropna().astype(float)\n",
    "        )\n",
    "        news_features[\"news_oxygen_saturation_mean\"] = value_array.mean()\n",
    "        news_features[\"news_oxygen_saturation_median\"] = value_array.median()\n",
    "        news_features[\"news_oxygen_saturation_std\"] = value_array.std()\n",
    "        news_features[\"news_oxygen_saturation_max\"] = max(value_array)\n",
    "        news_features[\"news_oxygen_saturation_min\"] = min(value_array)\n",
    "        news_features[\"news_oxygen_saturation_n\"] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features[\"news_oxygen_saturation_mean\"] = np.nan\n",
    "        news_features[\"news_oxygen_saturation_median\"] = np.nan\n",
    "        news_features[\"news_oxygen_saturation_std\"] = np.nan\n",
    "        news_features[\"news_oxygen_saturation_max\"] = np.nan\n",
    "        news_features[\"news_oxygen_saturation_min\"] = np.nan\n",
    "        news_features[\"news_oxygen_saturation_n\"] = np.nan\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    news_features_data = current_pat_raw_news[\n",
    "        current_pat_raw_news[\"obscatalogmasteritem_displayname\"] == \"NEWS Temperature\"\n",
    "    ].copy()\n",
    "    # news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    news_features_data.dropna(subset=[\"observation_valuetext_analysed\"], inplace=True)\n",
    "\n",
    "    if len(news_features_data) > 0:\n",
    "        # news_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        value_array = (\n",
    "            news_features_data[\"observation_valuetext_analysed\"].dropna().astype(float)\n",
    "        )\n",
    "        news_features[\"news_temperature_mean\"] = value_array.mean()\n",
    "        news_features[\"news_temperature_median\"] = value_array.median()\n",
    "        news_features[\"news_temperature_std\"] = value_array.std()\n",
    "        news_features[\"news_temperature_max\"] = max(value_array)\n",
    "        news_features[\"news_temperature_min\"] = min(value_array)\n",
    "        news_features[\"news_temperature_n\"] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features[\"news_temperature_mean\"] = np.nan\n",
    "        news_features[\"news_temperature_median\"] = np.nan\n",
    "        news_features[\"news_temperature_std\"] = np.nan\n",
    "        news_features[\"news_temperature_max\"] = np.nan\n",
    "        news_features[\"news_temperature_min\"] = np.nan\n",
    "        news_features[\"news_temperature_n\"] = np.nan\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    news_features_data = current_pat_raw_news[\n",
    "        current_pat_raw_news[\"obscatalogmasteritem_displayname\"] == \"NEWS_AVPU\"\n",
    "    ].copy()\n",
    "    # news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    news_features_data.dropna(subset=[\"observation_valuetext_analysed\"], inplace=True)\n",
    "\n",
    "    if len(news_features_data) > 0:\n",
    "        # news_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        value_array = (\n",
    "            news_features_data[\"observation_valuetext_analysed\"].dropna().astype(float)\n",
    "        )\n",
    "        news_features[\"news_avpu_mean\"] = value_array.mean()\n",
    "        news_features[\"news_avpu_median\"] = value_array.median()\n",
    "        news_features[\"news_avpu_std\"] = value_array.std()\n",
    "        news_features[\"news_avpu_max\"] = max(value_array)\n",
    "        news_features[\"news_avpu_min\"] = min(value_array)\n",
    "        news_features[\"news_avpu_n\"] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features[\"news_avpu_mean\"] = np.nan\n",
    "        news_features[\"news_avpu_median\"] = np.nan\n",
    "        news_features[\"news_avpu_std\"] = np.nan\n",
    "        news_features[\"news_avpu_max\"] = np.nan\n",
    "        news_features[\"news_avpu_min\"] = np.nan\n",
    "        news_features[\"news_avpu_n\"] = np.nan\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    news_features_data = current_pat_raw_news[\n",
    "        current_pat_raw_news[\"obscatalogmasteritem_displayname\"]\n",
    "        == \"NEWS_Supplemental_Oxygen\"\n",
    "    ].copy()\n",
    "    # news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    news_features_data.dropna(subset=[\"observation_valuetext_analysed\"], inplace=True)\n",
    "\n",
    "    term = \"supplemental_oxygen\"\n",
    "\n",
    "    if len(news_features_data) > 0:\n",
    "        # news_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        value_array = (\n",
    "            news_features_data[\"observation_valuetext_analysed\"].dropna().astype(float)\n",
    "        )\n",
    "        news_features[f\"news_{term}_mean\"] = value_array.mean()\n",
    "        news_features[f\"news_{term}_median\"] = value_array.median()\n",
    "        news_features[f\"news_{term}_std\"] = value_array.std()\n",
    "        news_features[f\"news_{term}_max\"] = max(value_array)\n",
    "        news_features[f\"news_{term}_min\"] = min(value_array)\n",
    "        news_features[f\"news_{term}_n\"] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features[f\"news_{term}_mean\"] = np.nan\n",
    "        news_features[f\"news_{term}_median\"] = np.nan\n",
    "        news_features[f\"news_{term}_std\"] = np.nan\n",
    "        news_features[f\"news_{term}_max\"] = np.nan\n",
    "        news_features[f\"news_{term}_min\"] = np.nan\n",
    "        news_features[f\"news_{term}_n\"] = np.nan\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    news_features_data = current_pat_raw_news[\n",
    "        current_pat_raw_news[\"obscatalogmasteritem_displayname\"] == \"NEWS2_Sp02_Target\"\n",
    "    ].copy()\n",
    "    # news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    news_features_data.dropna(subset=[\"observation_valuetext_analysed\"], inplace=True)\n",
    "\n",
    "    term = \"Sp02_Target\".lower()\n",
    "\n",
    "    if len(news_features_data) > 0:\n",
    "        # news_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        value_array = (\n",
    "            news_features_data[\"observation_valuetext_analysed\"].dropna().astype(float)\n",
    "        )\n",
    "        news_features[f\"news_{term}_mean\"] = value_array.mean()\n",
    "        news_features[f\"news_{term}_median\"] = value_array.median()\n",
    "        news_features[f\"news_{term}_std\"] = value_array.std()\n",
    "        news_features[f\"news_{term}_max\"] = max(value_array)\n",
    "        news_features[f\"news_{term}_min\"] = min(value_array)\n",
    "        news_features[f\"news_{term}_n\"] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features[f\"news_{term}_mean\"] = np.nan\n",
    "        news_features[f\"news_{term}_median\"] = np.nan\n",
    "        news_features[f\"news_{term}_std\"] = np.nan\n",
    "        news_features[f\"news_{term}_max\"] = np.nan\n",
    "        news_features[f\"news_{term}_min\"] = np.nan\n",
    "        news_features[f\"news_{term}_n\"] = np.nan\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    news_features_data = current_pat_raw_news[\n",
    "        current_pat_raw_news[\"obscatalogmasteritem_displayname\"] == \"NEWS2_Sp02_Scale\"\n",
    "    ].copy()\n",
    "    # news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    news_features_data.dropna(subset=[\"observation_valuetext_analysed\"], inplace=True)\n",
    "\n",
    "    term = \"Sp02_Scale\".lower()\n",
    "\n",
    "    if len(news_features_data) > 0:\n",
    "        # news_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        value_array = (\n",
    "            news_features_data[\"observation_valuetext_analysed\"].dropna().astype(float)\n",
    "        )\n",
    "        news_features[f\"news_{term}_mean\"] = value_array.mean()\n",
    "        news_features[f\"news_{term}_median\"] = value_array.median()\n",
    "        news_features[f\"news_{term}_std\"] = value_array.std()\n",
    "        news_features[f\"news_{term}_max\"] = max(value_array)\n",
    "        news_features[f\"news_{term}_min\"] = min(value_array)\n",
    "        news_features[f\"news_{term}_n\"] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features[f\"news_{term}_mean\"] = np.nan\n",
    "        news_features[f\"news_{term}_median\"] = np.nan\n",
    "        news_features[f\"news_{term}_std\"] = np.nan\n",
    "        news_features[f\"news_{term}_max\"] = np.nan\n",
    "        news_features[f\"news_{term}_min\"] = np.nan\n",
    "        news_features[f\"news_{term}_n\"] = np.nan\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    news_features_data = current_pat_raw_news[\n",
    "        current_pat_raw_news[\"obscatalogmasteritem_displayname\"] == \"NEWS_Pulse_Type\"\n",
    "    ].copy()\n",
    "    # news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    news_features_data.dropna(subset=[\"observation_valuetext_analysed\"], inplace=True)\n",
    "\n",
    "    term = \"pulse_type\".lower()\n",
    "\n",
    "    if len(news_features_data) > 0:\n",
    "        # news_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        value_array = (\n",
    "            news_features_data[\"observation_valuetext_analysed\"].dropna().astype(float)\n",
    "        )\n",
    "        news_features[f\"news_{term}_mean\"] = value_array.mean()\n",
    "        news_features[f\"news_{term}_median\"] = value_array.median()\n",
    "        news_features[f\"news_{term}_std\"] = value_array.std()\n",
    "        news_features[f\"news_{term}_max\"] = max(value_array)\n",
    "        news_features[f\"news_{term}_min\"] = min(value_array)\n",
    "        news_features[f\"news_{term}_n\"] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features[f\"news_{term}_mean\"] = np.nan\n",
    "        news_features[f\"news_{term}_median\"] = np.nan\n",
    "        news_features[f\"news_{term}_std\"] = np.nan\n",
    "        news_features[f\"news_{term}_max\"] = np.nan\n",
    "        news_features[f\"news_{term}_min\"] = np.nan\n",
    "        news_features[f\"news_{term}_n\"] = np.nan\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    news_features_data = current_pat_raw_news[\n",
    "        current_pat_raw_news[\"obscatalogmasteritem_displayname\"] == \"NEWS_Pain_Score\"\n",
    "    ].copy()\n",
    "    # news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    news_features_data.dropna(subset=[\"observation_valuetext_analysed\"], inplace=True)\n",
    "\n",
    "    term = \"Pain_Score\".lower()\n",
    "\n",
    "    if len(news_features_data) > 0:\n",
    "        # news_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        value_array = news_features_data[\"observation_valuetext_analysed\"].astype(float)\n",
    "        news_features[f\"news_{term}_mean\"] = value_array.mean()\n",
    "        news_features[f\"news_{term}_median\"] = value_array.median()\n",
    "        news_features[f\"news_{term}_std\"] = value_array.std()\n",
    "        news_features[f\"news_{term}_max\"] = max(value_array)\n",
    "        news_features[f\"news_{term}_min\"] = min(value_array)\n",
    "        news_features[f\"news_{term}_n\"] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features[f\"news_{term}_mean\"] = np.nan\n",
    "        news_features[f\"news_{term}_median\"] = np.nan\n",
    "        news_features[f\"news_{term}_std\"] = np.nan\n",
    "        news_features[f\"news_{term}_max\"] = np.nan\n",
    "        news_features[f\"news_{term}_min\"] = np.nan\n",
    "        news_features[f\"news_{term}_n\"] = np.nan\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    news_features_data = current_pat_raw_news[\n",
    "        current_pat_raw_news[\"obscatalogmasteritem_displayname\"] == \"NEWS Oxygen Litres\"\n",
    "    ].copy()\n",
    "    # news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    news_features_data.dropna(subset=[\"observation_valuetext_analysed\"], inplace=True)\n",
    "\n",
    "    term = \"oxygen_litres\".lower()\n",
    "\n",
    "    if len(news_features_data) > 0:\n",
    "        # news_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        value_array = (\n",
    "            news_features_data[\"observation_valuetext_analysed\"].dropna().astype(float)\n",
    "        )\n",
    "        news_features[f\"news_{term}_mean\"] = value_array.mean()\n",
    "        news_features[f\"news_{term}_median\"] = value_array.median()\n",
    "        news_features[f\"news_{term}_std\"] = value_array.std()\n",
    "        news_features[f\"news_{term}_max\"] = max(value_array)\n",
    "        news_features[f\"news_{term}_min\"] = min(value_array)\n",
    "        news_features[f\"news_{term}_n\"] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features[f\"news_{term}_mean\"] = np.nan\n",
    "        news_features[f\"news_{term}_median\"] = np.nan\n",
    "        news_features[f\"news_{term}_std\"] = np.nan\n",
    "        news_features[f\"news_{term}_max\"] = np.nan\n",
    "        news_features[f\"news_{term}_min\"] = np.nan\n",
    "        news_features[f\"news_{term}_n\"] = np.nan\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    news_features_data = current_pat_raw_news[\n",
    "        current_pat_raw_news[\"obscatalogmasteritem_displayname\"]\n",
    "        == \"NEWS Oxygen Delivery\"\n",
    "    ].copy()\n",
    "    # news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    news_features_data.dropna(subset=[\"observation_valuetext_analysed\"], inplace=True)\n",
    "\n",
    "    term = \"oxygen_delivery\".lower()\n",
    "\n",
    "    if len(news_features_data) > 0:\n",
    "        # news_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        value_array = (\n",
    "            news_features_data[\"observation_valuetext_analysed\"].dropna().astype(float)\n",
    "        )\n",
    "        news_features[f\"news_{term}_mean\"] = value_array.mean()\n",
    "        news_features[f\"news_{term}_median\"] = value_array.median()\n",
    "        news_features[f\"news_{term}_std\"] = value_array.std()\n",
    "        news_features[f\"news_{term}_max\"] = max(value_array)\n",
    "        news_features[f\"news_{term}_min\"] = min(value_array)\n",
    "        news_features[f\"news_{term}_n\"] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features[f\"news_{term}_mean\"] = np.nan\n",
    "        news_features[f\"news_{term}_median\"] = np.nan\n",
    "        news_features[f\"news_{term}_std\"] = np.nan\n",
    "        news_features[f\"news_{term}_max\"] = np.nan\n",
    "        news_features[f\"news_{term}_min\"] = np.nan\n",
    "        news_features[f\"news_{term}_n\"] = np.nan\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    news_features_data = current_pat_raw_news[\n",
    "        current_pat_raw_news[\"obscatalogmasteritem_displayname\"]\n",
    "        == \"NEWS Oxygen Delivery\"\n",
    "    ].copy()\n",
    "    # news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    news_features_data.dropna(subset=[\"observation_valuetext_analysed\"], inplace=True)\n",
    "\n",
    "    term = \"oxygen_delivery\".lower()\n",
    "\n",
    "    if len(news_features_data) > 0:\n",
    "        # news_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        value_array = news_features_data[\"observation_valuetext_analysed\"].astype(float)\n",
    "        news_features[f\"news_{term}_mean\"] = value_array.mean()\n",
    "        news_features[f\"news_{term}_median\"] = value_array.median()\n",
    "        news_features[f\"news_{term}_std\"] = value_array.std()\n",
    "        news_features[f\"news_{term}_max\"] = max(value_array)\n",
    "        news_features[f\"news_{term}_min\"] = min(value_array)\n",
    "        news_features[f\"news_{term}_n\"] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features[f\"news_{term}_mean\"] = np.nan\n",
    "        news_features[f\"news_{term}_median\"] = np.nan\n",
    "        news_features[f\"news_{term}_std\"] = np.nan\n",
    "        news_features[f\"news_{term}_max\"] = np.nan\n",
    "        news_features[f\"news_{term}_min\"] = np.nan\n",
    "        news_features[f\"news_{term}_n\"] = np.nan\n",
    "\n",
    "    return news_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bmi_features(current_pat_client_id_code, target_date_range, pat_batch):\n",
    "    start_year, start_month, end_year, end_month = get_start_end_year_month(\n",
    "        target_date_range\n",
    "    )\n",
    "\n",
    "    if batch_mode:\n",
    "        current_pat_raw_bmi = filter_dataframe_by_timestamp(\n",
    "            pat_batch,\n",
    "            start_year,\n",
    "            start_month,\n",
    "            end_year,\n",
    "            end_month,\n",
    "            \"observationdocument_recordeddtm\",\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        current_pat_raw_bmi = cohort_searcher_with_terms_and_search(\n",
    "            index_name=\"observations\",\n",
    "            fields_list=\"\"\"observation_guid client_idcode\tobscatalogmasteritem_displayname\tobservation_valuetext_analysed\tobservationdocument_recordeddtm clientvisit_visitidcode\"\"\".split(),\n",
    "            term_name=\"client_idcode.keyword\",\n",
    "            entered_list=[current_pat_client_id_code],\n",
    "            search_string='obscatalogmasteritem_displayname:(\"OBS BMI\" OR \"OBS Weight\" OR \"OBS height\") AND '\n",
    "            + f\"observationdocument_recordeddtm:[{start_year}-{start_month} TO {end_year}-{end_month}]\",\n",
    "        )\n",
    "\n",
    "    if (\n",
    "        len(\n",
    "            current_pat_raw_bmi[\n",
    "                current_pat_raw_bmi[\"obscatalogmasteritem_displayname\"]\n",
    "                == \"OBS BMI Calculation\"\n",
    "            ]\n",
    "        )\n",
    "    ) == 0:\n",
    "        bmi_features = pd.DataFrame(\n",
    "            data=[current_pat_client_id_code], columns=[\"client_idcode\"]\n",
    "        )\n",
    "\n",
    "    # get bmi features\n",
    "    # set client id to current pat\n",
    "    bmi_sample = current_pat_raw_bmi[\n",
    "        current_pat_raw_bmi[\"obscatalogmasteritem_displayname\"] == \"OBS BMI Calculation\"\n",
    "    ]\n",
    "    # screen and purge dud values\n",
    "    bmi_sample = bmi_sample[\n",
    "        (bmi_sample[\"observation_valuetext_analysed\"].astype(float) < 200)\n",
    "        & (bmi_sample[\"observation_valuetext_analysed\"].astype(float) > 6)\n",
    "    ]\n",
    "\n",
    "    if len(bmi_sample) > 0:\n",
    "        bmi_features = pd.DataFrame(\n",
    "            data=[current_pat_client_id_code], columns=[\"client_idcode\"]\n",
    "        ).copy()\n",
    "\n",
    "        # if(len(current_pat_raw_bmi)>0):\n",
    "\n",
    "        value_array = bmi_sample[\"observation_valuetext_analysed\"].astype(float)\n",
    "\n",
    "        bmi_features[\"bmi_mean\"] = value_array.mean()\n",
    "        bmi_features[\"bmi_median\"] = value_array.median()\n",
    "        bmi_features[\"bmi_std\"] = value_array.std()\n",
    "        bmi_features[\"bmi_high\"] = int(bool(value_array.median() > 24.9))\n",
    "        bmi_features[\"bmi_low\"] = int(bool(value_array.median() < 18.5))\n",
    "        bmi_features[\"bmi_extreme\"] = int(bool(value_array.median() > 30))\n",
    "        bmi_features[\"bmi_max\"] = max(value_array)\n",
    "        bmi_features[\"bmi_min\"] = min(value_array)\n",
    "\n",
    "    else:\n",
    "        bmi_features = pd.DataFrame(\n",
    "            data=[current_pat_client_id_code], columns=[\"client_idcode\"]\n",
    "        ).copy()\n",
    "        bmi_features[\"bmi_mean\"] = np.nan\n",
    "        bmi_features[\"bmi_median\"] = np.nan\n",
    "        bmi_features[\"bmi_std\"] = np.nan\n",
    "        bmi_features[\"bmi_high\"] = np.nan\n",
    "        bmi_features[\"bmi_low\"] = np.nan\n",
    "        bmi_features[\"bmi_extreme\"] = np.nan\n",
    "        bmi_features[\"bmi_max\"] = np.nan\n",
    "        bmi_features[\"bmi_min\"] = np.nan\n",
    "\n",
    "    height_sample = current_pat_raw_bmi[\n",
    "        current_pat_raw_bmi[\"obscatalogmasteritem_displayname\"] == \"OBS Height\"\n",
    "    ]\n",
    "    # screen and purge dud values\n",
    "    height_sample = height_sample[\n",
    "        (height_sample[\"observation_valuetext_analysed\"].astype(float) < 300)\n",
    "        & (height_sample[\"observation_valuetext_analysed\"].astype(float) > 30)\n",
    "    ]\n",
    "\n",
    "    # get height features\n",
    "    if len(height_sample) > 0:\n",
    "\n",
    "        if len(current_pat_raw_bmi) > 0:\n",
    "\n",
    "            value_array = height_sample[\"observation_valuetext_analysed\"].astype(float)\n",
    "\n",
    "            bmi_features[\"height_mean\"] = value_array.mean()\n",
    "            bmi_features[\"height_median\"] = value_array.median()\n",
    "            bmi_features[\"height_std\"] = value_array.std()\n",
    "\n",
    "    else:\n",
    "        bmi_features[\"height_mean\"] = np.nan\n",
    "        bmi_features[\"height_median\"] = np.nan\n",
    "        bmi_features[\"height_std\"] = np.nan\n",
    "\n",
    "    weight_sample = current_pat_raw_bmi[\n",
    "        current_pat_raw_bmi[\"obscatalogmasteritem_displayname\"] == \"OBS Weight\"\n",
    "    ]\n",
    "    # screen and purge dud values\n",
    "    weight_sample = weight_sample[\n",
    "        (weight_sample[\"observation_valuetext_analysed\"].astype(float) < 800)\n",
    "        & (weight_sample[\"observation_valuetext_analysed\"].astype(float) > 1)\n",
    "    ]\n",
    "\n",
    "    # get weight features\n",
    "    if len(weight_sample) > 0:\n",
    "\n",
    "        if len(current_pat_raw_bmi) > 0:\n",
    "\n",
    "            value_array = weight_sample[\"observation_valuetext_analysed\"].astype(float)\n",
    "\n",
    "            bmi_features[\"weight_mean\"] = value_array.mean()\n",
    "            bmi_features[\"weight_median\"] = value_array.median()\n",
    "            bmi_features[\"weight_std\"] = value_array.std()\n",
    "            bmi_features[\"weight_max\"] = max(value_array)\n",
    "            bmi_features[\"weight_min\"] = min(value_array)\n",
    "\n",
    "    else:\n",
    "        bmi_features[\"weight_mean\"] = np.nan\n",
    "        bmi_features[\"weight_median\"] = np.nan\n",
    "        bmi_features[\"weight_std\"] = np.nan\n",
    "        bmi_features[\"weight_max\"] = np.nan\n",
    "        bmi_features[\"weight_min\"] = np.nan\n",
    "\n",
    "    return bmi_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_pat_client_id_code = \"%testclientidcode%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 1995\n",
    "start_month = \"1\"\n",
    "end_year = 2023\n",
    "end_month = \"11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_searcher_with_terms_and_search(\n",
    "    index_name=\"basic_observations\",\n",
    "    fields_list=[\n",
    "        \"client_idcode\",\n",
    "        \"basicobs_itemname_analysed\",\n",
    "        \"basicobs_value_numeric\",\n",
    "        \"basicobs_entered\",\n",
    "        \"clientvisit_serviceguid\",\n",
    "    ],\n",
    "    term_name=\"client_idcode.keyword\",\n",
    "    entered_list=[current_pat_client_id_code],\n",
    "    search_string=\"basicobs_value_numeric:* AND \"\n",
    "    + f\"updatetime:[{start_year}-{start_month} TO {end_year}-{end_month}] \",\n",
    ")  # In kibana can we pull the mean and std of each blood test for a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_pat_bloods(current_pat_client_id_code, target_date_range, pat_batch):\n",
    "\n",
    "    start_year, start_month, end_year, end_month = get_start_end_year_month(\n",
    "        target_date_range\n",
    "    )\n",
    "\n",
    "    if batch_mode:\n",
    "        current_pat_bloods = filter_dataframe_by_timestamp(\n",
    "            pat_batch, start_year, start_month, end_year, end_month, \"basicobs_entered\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        current_pat_bloods = cohort_searcher_with_terms_and_search(\n",
    "            index_name=\"basic_observations\",\n",
    "            fields_list=[\n",
    "                \"client_idcode\",\n",
    "                \"basicobs_itemname_analysed\",\n",
    "                \"basicobs_value_numeric\",\n",
    "                \"basicobs_entered\",\n",
    "                \"clientvisit_serviceguid\",\n",
    "            ],\n",
    "            term_name=\"client_idcode.keyword\",\n",
    "            entered_list=[current_pat_client_id_code],\n",
    "            search_string=\"basicobs_value_numeric:* AND \"\n",
    "            + f\"updatetime:[{start_year}-{start_month} TO {end_year}-{end_month}] \",\n",
    "        )  # In kibana can we pull the mean and std of each blood test for a reference.\n",
    "\n",
    "    if batch_mode:\n",
    "        current_pat_bloods[\"datetime\"] = current_pat_bloods[\"basicobs_entered\"].copy()\n",
    "\n",
    "    else:\n",
    "        current_pat_bloods[\"datetime\"] = pd.Series(\n",
    "            current_pat_bloods[\"basicobs_entered\"]\n",
    "        ).apply(convert_date)\n",
    "\n",
    "    basicobs_itemname_analysed_list = list(\n",
    "        current_pat_bloods[\"basicobs_itemname_analysed\"].unique()\n",
    "    )\n",
    "\n",
    "    basicobs_itemname_analysed_df_dict = {\n",
    "        elem: current_pat_bloods[current_pat_bloods.basicobs_itemname_analysed == elem]\n",
    "        for elem in basicobs_itemname_analysed_list\n",
    "    }\n",
    "\n",
    "    df_unique = current_pat_bloods.copy()\n",
    "\n",
    "    df_unique.drop_duplicates(subset=\"client_idcode\", inplace=True)\n",
    "\n",
    "    df_unique.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "    obs_columns_list = basicobs_itemname_analysed_list\n",
    "\n",
    "    obs_columns_set = list(set(obs_columns_list))\n",
    "\n",
    "\n",
    "    # print(\"Building obs_columns_set_columns_for_df\")\n",
    "\n",
    "    obs_columns_set_columns_for_df = []\n",
    "    for i in range(0, len(obs_columns_set)):\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i] + \"_mean\")\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i] + \"_median\")\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i] + \"_mode\")\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i] + \"_std\")\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i] + \"_num-tests\")\n",
    "        obs_columns_set_columns_for_df.append(\n",
    "            obs_columns_set[i] + \"_days-since-last-test\"\n",
    "        )\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i] + \"_max\")\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i] + \"_min\")\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i] + \"_most-recent\")\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i] + \"_earliest-test\")\n",
    "        obs_columns_set_columns_for_df.append(\n",
    "            obs_columns_set[i] + \"_days-between-first-last\"\n",
    "        )\n",
    "        obs_columns_set_columns_for_df.append(\n",
    "            obs_columns_set[i] + \"_contains-extreme-low\"\n",
    "        )\n",
    "        obs_columns_set_columns_for_df.append(\n",
    "            obs_columns_set[i] + \"_contains-extreme-high\"\n",
    "        )\n",
    "\n",
    "    orig_columns = list(df_unique.columns)\n",
    "\n",
    "    comb_cols = orig_columns + obs_columns_set_columns_for_df\n",
    "\n",
    "    df_unique = df_unique.reindex(comb_cols, axis=1)\n",
    "\n",
    "    df_unique = df_unique.copy()\n",
    "    df_unique.drop(\n",
    "        [\n",
    "            \"index\",\n",
    "            \"_index\",\n",
    "            \"_id\",\n",
    "            \"_score\",\n",
    "            \"basicobs_itemname_analysed\",\n",
    "            \"basicobs_value_numeric\",\n",
    "            \"basicobs_entered\",\n",
    "            \"clientvisit_serviceguid\",\n",
    "            \"datetime\",\n",
    "        ],\n",
    "        inplace=True,\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    if batch_mode:\n",
    "\n",
    "        today = datetime.now(timezone.utc)\n",
    "\n",
    "    else:\n",
    "        today = datetime.today()\n",
    "\n",
    "\n",
    "    df_unique_filtered = df_unique.copy()\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for j in range(0, len(basicobs_itemname_analysed_list)):\n",
    "        col_name = basicobs_itemname_analysed_list[j]\n",
    "\n",
    "        filtered_df = basicobs_itemname_analysed_df_dict.get(col_name)\n",
    "\n",
    "        filtered_column_values = filtered_df.basicobs_value_numeric.astype(\n",
    "            float\n",
    "        )._get_numeric_data()\n",
    "\n",
    "        df_len = len(filtered_df)\n",
    "        # print(f\"df_len {df_len}\")\n",
    "        if df_len >= 1:\n",
    "            # Mean assurance*\n",
    "            agg_val = float(filtered_column_values.values[0])\n",
    "\n",
    "            df_unique_filtered.at[i, col_name + \"_mean\"] = agg_val\n",
    "\n",
    "        if df_len >= 2:\n",
    "            # try:\n",
    "            # Mean\n",
    "            agg_val = filtered_column_values.mean()\n",
    "\n",
    "            df_unique_filtered.at[i, col_name + \"_mean\"] = agg_val\n",
    "\n",
    "            # recent\n",
    "            agg_val = filtered_df.sort_values(by=\"datetime\").iloc[-1][\n",
    "                \"basicobs_value_numeric\"\n",
    "            ]\n",
    "\n",
    "            df_unique_filtered.at[i, col_name + \"_most-recent\"] = agg_val\n",
    "\n",
    "            # earliest-test\n",
    "            agg_val = filtered_df.sort_values(by=\"datetime\").iloc[0][\n",
    "                \"basicobs_value_numeric\"\n",
    "            ]\n",
    "            df_unique_filtered.at[i, col_name + \"_earliest-test\"] = agg_val\n",
    "\n",
    "            # days-since-last-test\n",
    "            date_object = filtered_df.sort_values(by=\"datetime\").iloc[-1][\"datetime\"]\n",
    "\n",
    "            delta = today - date_object\n",
    "\n",
    "            agg_val = delta.days\n",
    "\n",
    "            df_unique_filtered.at[i, col_name + \"_days-since-last-test\"] = agg_val\n",
    "\n",
    "            # n tests\n",
    "\n",
    "            agg_val = len(filtered_column_values)\n",
    "\n",
    "            df_unique_filtered.at[i, col_name + \"_num-tests\"] = agg_val\n",
    "\n",
    "        if df_len >= 3:\n",
    "            # try:\n",
    "            # median\n",
    "            # agg_val = np.median(filtered_df['basicobs_value_numeric'].to_numpy())\n",
    "            agg_val = filtered_column_values.median()\n",
    "            df_unique_filtered.at[i, col_name + \"_median\"] = agg_val\n",
    "\n",
    "            # mode\n",
    "            # agg_val = stats.mode(filtered_df['basicobs_value_numeric'].to_numpy(), keepdims=True)[0][0]\n",
    "\n",
    "            agg_val = stats.mode(filtered_column_values)[0][0]\n",
    "            df_unique_filtered.at[i, col_name + \"_mode\"] = agg_val\n",
    "\n",
    "            # std\n",
    "            agg_val = filtered_column_values.std()\n",
    "            df_unique_filtered.at[i, col_name + \"_std\"] = agg_val\n",
    "\n",
    "            # min\n",
    "            agg_val = min(filtered_column_values)\n",
    "            df_unique_filtered.at[i, col_name + \"_min\"] = agg_val\n",
    "\n",
    "            # max\n",
    "            agg_val = max(filtered_column_values)\n",
    "            df_unique_filtered.at[i, col_name + \"_max\"] = agg_val\n",
    "\n",
    "            # contains extreme low\n",
    "            col_name_mean = (\n",
    "                basicobs_itemname_analysed_df_dict.get(col_name)\n",
    "                .basicobs_value_numeric._get_numeric_data()\n",
    "                .mean()\n",
    "            )\n",
    "            col_name_std = (\n",
    "                basicobs_itemname_analysed_df_dict.get(col_name)\n",
    "                .basicobs_value_numeric._get_numeric_data()\n",
    "                .std()\n",
    "            )\n",
    "\n",
    "            col_name_low = col_name_mean - (col_name_std * 3)\n",
    "\n",
    "            agg_val = int(float(min(filtered_column_values)) < col_name_low)\n",
    "            df_unique_filtered.at[i, col_name + \"_contains-extreme-low\"] = agg_val\n",
    "\n",
    "            # contains extreme high\n",
    "            col_name_high = col_name_mean + (col_name_std * 3)\n",
    "\n",
    "            agg_val = int(float(max(filtered_column_values)) > col_name_high)\n",
    "\n",
    "            df_unique_filtered.at[i, col_name + \"_contains-extreme-high\"] = agg_val\n",
    "\n",
    "            # days_between earliest and last\n",
    "\n",
    "            earliest = filtered_df.sort_values(by=\"datetime\").iloc[-1][\"datetime\"]\n",
    "\n",
    "            oldest = filtered_df.sort_values(by=\"datetime\").iloc[-1][\"datetime\"]\n",
    "\n",
    "            delta = earliest - oldest\n",
    "\n",
    "            agg_val = delta.days\n",
    "\n",
    "            df_unique_filtered.at[i, col_name + \"_days-between-first-last\"] = agg_val\n",
    "\n",
    "            # current_pat_bloods = df_unique_filtered\n",
    "    return df_unique_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_pat_drugs(current_pat_client_id_code, target_date_range, pat_batch):\n",
    "    start_year, start_month, end_year, end_month = get_start_end_year_month(\n",
    "        target_date_range\n",
    "    )\n",
    "\n",
    "    # Drugs\n",
    "    if batch_mode:\n",
    "        drugs = filter_dataframe_by_timestamp(\n",
    "            pat_batch, start_year, start_month, end_year, end_month, \"order_entered\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "\n",
    "        drugs = cohort_searcher_with_terms_and_search(\n",
    "            index_name=\"order\",\n",
    "            fields_list=\"\"\"client_idcode order_guid\torder_name\torder_summaryline order_holdreasontext\torder_entered clientvisit_visitidcode\"\"\".split(),\n",
    "            term_name=\"client_idcode.keyword\",\n",
    "            entered_list=[current_pat_client_id_code],\n",
    "            search_string='order_typecode:\"medication\" AND '\n",
    "            + f\"updatetime:[{start_year}-{start_month} TO {end_year}-{end_month}] \",\n",
    "        )\n",
    "\n",
    "    current_pat_diagnostics = drugs.copy()\n",
    "\n",
    "    if batch_mode:\n",
    "        current_pat_diagnostics[\"datetime\"] = current_pat_diagnostics[\n",
    "            \"order_entered\"\n",
    "        ].copy()\n",
    "\n",
    "    else:\n",
    "        current_pat_diagnostics[\"datetime\"] = (\n",
    "            pd.Series(current_pat_diagnostics[\"order_entered\"])\n",
    "            .dropna()\n",
    "            .apply(convert_date)\n",
    "        )\n",
    "\n",
    "    order_name_list = list(current_pat_diagnostics[\"order_name\"].unique())\n",
    "\n",
    "    order_name_df_dict = {\n",
    "        elem: current_pat_diagnostics[current_pat_diagnostics.order_name == elem]\n",
    "        for elem in order_name_list\n",
    "    }\n",
    "\n",
    "    df_unique = current_pat_diagnostics.copy()\n",
    "\n",
    "    df_unique.drop_duplicates(subset=\"client_idcode\", inplace=True)\n",
    "\n",
    "    df_unique.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "    obs_columns_list = order_name_list\n",
    "\n",
    "    obs_columns_set = list(set(obs_columns_list))\n",
    "\n",
    "\n",
    "    # print(\"Building obs_columns_set_columns_for_df\")\n",
    "\n",
    "    obs_columns_set_columns_for_df = []\n",
    "    for i in range(0, len(obs_columns_set)):\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i] + \"_num-drug-order\")\n",
    "        obs_columns_set_columns_for_df.append(\n",
    "            obs_columns_set[i] + \"_days-since-last-drug-order\"\n",
    "        )\n",
    "        obs_columns_set_columns_for_df.append(\n",
    "            obs_columns_set[i] + \"_days-between-first-last-drug\"\n",
    "        )\n",
    "\n",
    "    orig_columns = list(df_unique.columns)\n",
    "\n",
    "    comb_cols = orig_columns + obs_columns_set_columns_for_df\n",
    "\n",
    "    df_unique = df_unique.reindex(comb_cols, axis=1)\n",
    "\n",
    "    df_unique = df_unique.copy()\n",
    "    df_unique.drop(\n",
    "        [\n",
    "            \"_index\",\n",
    "            \"_id\",\n",
    "            \"_score\",\n",
    "            \"order_guid\",\n",
    "            \"order_name\",\n",
    "            \"order_summaryline\",\n",
    "            \"order_holdreasontext\",\n",
    "            \"order_entered\",\n",
    "            \"clientvisit_visitidcode\",\n",
    "        ],\n",
    "        inplace=True,\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    if batch_mode:\n",
    "\n",
    "        today = datetime.now(timezone.utc)\n",
    "\n",
    "    else:\n",
    "        today = datetime.today()\n",
    "\n",
    "\n",
    "    df_unique_filtered = df_unique.copy()\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for j in range(0, len(obs_columns_list)):\n",
    "        col_name = obs_columns_list[j]\n",
    "\n",
    "        filtered_df = order_name_df_dict.get(col_name)\n",
    "\n",
    "        df_len = len(filtered_df)\n",
    "        if df_len >= 1:\n",
    "            # n tests\n",
    "\n",
    "            agg_val = len(filtered_df)\n",
    "\n",
    "            df_unique_filtered.at[i, col_name + \"_num-drug-order\"] = agg_val\n",
    "\n",
    "            # days-since-last-test\n",
    "            date_object = filtered_df.sort_values(by=\"datetime\").iloc[-1][\"datetime\"]\n",
    "\n",
    "            delta = today - date_object\n",
    "\n",
    "            agg_val = delta.days\n",
    "\n",
    "            df_unique_filtered.at[i, col_name + \"_days-since-last-drug-order\"] = agg_val\n",
    "\n",
    "        if df_len >= 2:\n",
    "\n",
    "            # days_between earliest and last\n",
    "\n",
    "            earliest = filtered_df.sort_values(by=\"datetime\").iloc[-1][\"datetime\"]\n",
    "\n",
    "            oldest = filtered_df.sort_values(by=\"datetime\").iloc[-1][\"datetime\"]\n",
    "\n",
    "            delta = earliest - oldest\n",
    "\n",
    "            agg_val = delta.days\n",
    "\n",
    "            df_unique_filtered.at[i, col_name + \"_days-between-first-last-drug\"] = (\n",
    "                agg_val\n",
    "            )\n",
    "\n",
    "    return df_unique_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_pat_diagnostics(\n",
    "    current_pat_client_id_code, target_date_range, pat_batch\n",
    "):\n",
    "\n",
    "    start_year, start_month, end_year, end_month = get_start_end_year_month(\n",
    "        target_date_range\n",
    "    )\n",
    "\n",
    "    # Diagnostic tests\n",
    "    if batch_mode:\n",
    "        diagnostics = filter_dataframe_by_timestamp(\n",
    "            pat_batch, start_year, start_month, end_year, end_month, \"order_entered\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        diagnostics = cohort_searcher_with_terms_and_search(\n",
    "            index_name=\"order\",\n",
    "            fields_list=\"\"\"client_idcode order_guid\torder_name\torder_summaryline order_holdreasontext\torder_entered clientvisit_visitidcode\"\"\".split(),\n",
    "            term_name=\"client_idcode.keyword\",\n",
    "            entered_list=[current_pat_client_id_code],\n",
    "            search_string='order_typecode:\"diagnostic\"AND '\n",
    "            + f\"updatetime:[{start_year}-{start_month} TO {end_year}-{end_month}] \",\n",
    "        )\n",
    "\n",
    "    current_pat_diagnostics = diagnostics.copy()\n",
    "\n",
    "    if batch_mode:\n",
    "        current_pat_diagnostics[\"datetime\"] = current_pat_diagnostics[\n",
    "            \"order_entered\"\n",
    "        ].copy()\n",
    "\n",
    "    else:\n",
    "        current_pat_diagnostics[\"datetime\"] = (\n",
    "            pd.Series(current_pat_diagnostics[\"order_entered\"])\n",
    "            .dropna()\n",
    "            .apply(convert_date)\n",
    "        )\n",
    "\n",
    "    order_name_list = list(current_pat_diagnostics[\"order_name\"].unique())\n",
    "\n",
    "    order_name_df_dict = {\n",
    "        elem: current_pat_diagnostics[current_pat_diagnostics.order_name == elem]\n",
    "        for elem in order_name_list\n",
    "    }\n",
    "\n",
    "    df_unique = current_pat_diagnostics.copy()\n",
    "\n",
    "    df_unique.drop_duplicates(subset=\"client_idcode\", inplace=True)\n",
    "\n",
    "    df_unique.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "    obs_columns_list = order_name_list\n",
    "\n",
    "    obs_columns_set = list(set(obs_columns_list))\n",
    "\n",
    "\n",
    "    # print(\"Building obs_columns_set_columns_for_df\")\n",
    "\n",
    "    obs_columns_set_columns_for_df = []\n",
    "    for i in range(0, len(obs_columns_set)):\n",
    "        obs_columns_set_columns_for_df.append(\n",
    "            obs_columns_set[i] + \"_num-diagnostic-order\"\n",
    "        )\n",
    "        obs_columns_set_columns_for_df.append(\n",
    "            obs_columns_set[i] + \"_days-since-last-diagnostic-order\"\n",
    "        )\n",
    "        obs_columns_set_columns_for_df.append(\n",
    "            obs_columns_set[i] + \"_days-between-first-last-diagnostic\"\n",
    "        )\n",
    "\n",
    "    orig_columns = list(df_unique.columns)\n",
    "\n",
    "    comb_cols = orig_columns + obs_columns_set_columns_for_df\n",
    "\n",
    "    df_unique = df_unique.reindex(comb_cols, axis=1)\n",
    "\n",
    "    df_unique = df_unique.copy()\n",
    "    df_unique.drop(\n",
    "        [\n",
    "            \"_index\",\n",
    "            \"_id\",\n",
    "            \"_score\",\n",
    "            \"order_guid\",\n",
    "            \"order_name\",\n",
    "            \"order_summaryline\",\n",
    "            \"order_holdreasontext\",\n",
    "            \"order_entered\",\n",
    "            \"clientvisit_visitidcode\",\n",
    "        ],\n",
    "        inplace=True,\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    if batch_mode:\n",
    "\n",
    "        today = datetime.now(timezone.utc)\n",
    "\n",
    "    else:\n",
    "        today = datetime.today()\n",
    "\n",
    "\n",
    "    df_unique_filtered = df_unique.copy()\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for j in range(0, len(obs_columns_list)):\n",
    "        col_name = obs_columns_list[j]\n",
    "\n",
    "        filtered_df = order_name_df_dict.get(col_name)\n",
    "\n",
    "        df_len = len(filtered_df)\n",
    "        if df_len >= 1:\n",
    "            # n tests\n",
    "\n",
    "            agg_val = len(filtered_df)\n",
    "\n",
    "            df_unique_filtered.at[i, col_name + \"_num-diagnostic-order\"] = agg_val\n",
    "\n",
    "            # days-since-last-test\n",
    "            date_object = filtered_df.sort_values(by=\"datetime\").iloc[-1][\"datetime\"]\n",
    "\n",
    "            delta = today - date_object\n",
    "\n",
    "            agg_val = delta.days\n",
    "\n",
    "            df_unique_filtered.at[i, col_name + \"_days-since-last-diagnostic-order\"] = (\n",
    "                agg_val\n",
    "            )\n",
    "\n",
    "        if df_len >= 2:\n",
    "\n",
    "            # days_between earliest and last\n",
    "\n",
    "            earliest = filtered_df.sort_values(by=\"datetime\").iloc[-1][\"datetime\"]\n",
    "\n",
    "            oldest = filtered_df.sort_values(by=\"datetime\").iloc[-1][\"datetime\"]\n",
    "\n",
    "            delta = earliest - oldest\n",
    "\n",
    "            agg_val = delta.days\n",
    "\n",
    "            df_unique_filtered.at[\n",
    "                i, col_name + \"_days-between-first-last-diagnostic\"\n",
    "            ] = agg_val\n",
    "\n",
    "    return df_unique_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pat_batch_obs(current_pat_client_id_code, search_term):\n",
    "\n",
    "    batch_target = cohort_searcher_with_terms_and_search(\n",
    "        index_name=\"observations\",\n",
    "        fields_list=\"\"\"observation_guid client_idcode\tobscatalogmasteritem_displayname\tobservation_valuetext_analysed\tobservationdocument_recordeddtm clientvisit_visitidcode\"\"\".split(),\n",
    "        term_name=\"client_idcode.keyword\",\n",
    "        entered_list=[current_pat_client_id_code],\n",
    "        search_string=f'obscatalogmasteritem_displayname:(\"{search_term}\") AND '\n",
    "        + f\"observationdocument_recordeddtm:[{global_start_year}-{global_start_month} TO {global_end_year}-{global_end_month}]\",\n",
    "    )\n",
    "\n",
    "    return batch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pat_batch_news(current_pat_client_id_code, search_term):\n",
    "    batch_target = cohort_searcher_with_terms_and_search(\n",
    "        index_name=\"observations\",\n",
    "        fields_list=\"\"\"observation_guid client_idcode\tobscatalogmasteritem_displayname\tobservation_valuetext_analysed\tobservationdocument_recordeddtm clientvisit_visitidcode\"\"\".split(),\n",
    "        term_name=\"client_idcode.keyword\",\n",
    "        entered_list=[current_pat_client_id_code],\n",
    "        search_string='obscatalogmasteritem_displayname:(\"NEWS\" OR \"NEWS2\") AND '\n",
    "        + f\"observationdocument_recordeddtm:[{global_start_year}-{global_start_month} TO {global_end_year}-{global_end_month}]\",\n",
    "    )\n",
    "\n",
    "    return batch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pat_batch_bmi(current_pat_client_id_code, search_term):\n",
    "    batch_target = cohort_searcher_with_terms_and_search(\n",
    "        index_name=\"observations\",\n",
    "        fields_list=\"\"\"observation_guid client_idcode\tobscatalogmasteritem_displayname\tobservation_valuetext_analysed\tobservationdocument_recordeddtm clientvisit_visitidcode\"\"\".split(),\n",
    "        term_name=\"client_idcode.keyword\",\n",
    "        entered_list=[current_pat_client_id_code],\n",
    "        search_string='obscatalogmasteritem_displayname:(\"OBS BMI\" OR \"OBS Weight\" OR \"OBS height\") AND '\n",
    "        + f\"observationdocument_recordeddtm:[{global_start_year}-{global_start_month} TO {global_end_year}-{global_end_month}]\",\n",
    "    )\n",
    "\n",
    "    return batch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pat_batch_bloods(current_pat_client_id_code, search_term):\n",
    "    batch_target = cohort_searcher_with_terms_and_search(\n",
    "        index_name=\"basic_observations\",\n",
    "        fields_list=[\n",
    "            \"client_idcode\",\n",
    "            \"basicobs_itemname_analysed\",\n",
    "            \"basicobs_value_numeric\",\n",
    "            \"basicobs_entered\",\n",
    "            \"clientvisit_serviceguid\",\n",
    "        ],\n",
    "        term_name=\"client_idcode.keyword\",\n",
    "        entered_list=[current_pat_client_id_code],\n",
    "        search_string=\"basicobs_value_numeric:* AND \"\n",
    "        + f\"updatetime:[{global_start_year}-{global_start_month} TO {global_end_year}-{global_end_month}] \",\n",
    "    )  # In kibana can we pull the mean and std of each blood test for a reference.\n",
    "\n",
    "    return batch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pat_batch_drugs(current_pat_client_id_code, search_term):\n",
    "    batch_target = cohort_searcher_with_terms_and_search(\n",
    "        index_name=\"order\",\n",
    "        fields_list=\"\"\"client_idcode order_guid\torder_name\torder_summaryline order_holdreasontext\torder_entered clientvisit_visitidcode\"\"\".split(),\n",
    "        term_name=\"client_idcode.keyword\",\n",
    "        entered_list=[current_pat_client_id_code],\n",
    "        search_string='order_typecode:\"medication\" AND '\n",
    "        + f\"updatetime:[{global_start_year}-{global_start_month} TO {global_end_year}-{global_end_month}] \",\n",
    "    )\n",
    "\n",
    "    return batch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pat_batch_diagnostics(current_pat_client_id_code, search_term):\n",
    "    batch_target = cohort_searcher_with_terms_and_search(\n",
    "        index_name=\"order\",\n",
    "        fields_list=\"\"\"client_idcode order_guid\torder_name\torder_summaryline order_holdreasontext\torder_entered clientvisit_visitidcode\"\"\".split(),\n",
    "        term_name=\"client_idcode.keyword\",\n",
    "        entered_list=[current_pat_client_id_code],\n",
    "        search_string='order_typecode:\"diagnostic\"AND '\n",
    "        + f\"updatetime:[{global_start_year}-{global_start_month} TO {global_end_year}-{global_end_month}] \",\n",
    "    )\n",
    "\n",
    "    return batch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pat_batch_epr_docs(current_pat_client_id_code, search_term):\n",
    "    batch_target = cohort_searcher_with_terms_and_search(\n",
    "        index_name=\"epr_documents\",\n",
    "        fields_list=\"\"\"client_idcode document_guid\tdocument_description\tbody_analysed updatetime clientvisit_visitidcode\"\"\".split(),\n",
    "        term_name=\"client_idcode.keyword\",\n",
    "        entered_list=[current_pat_client_id_code],\n",
    "        search_string=f\"updatetime:[{global_start_year}-{global_start_month} TO {global_end_year}-{global_end_month}] \",\n",
    "    )\n",
    "    return batch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pat_batch_mct_docs(current_pat_client_id_code, search_term):\n",
    "    batch_target = cohort_searcher_with_terms_and_search(\n",
    "        index_name=\"observations\",\n",
    "        fields_list=\"\"\"observation_guid client_idcode\tobscatalogmasteritem_displayname\tobservation_valuetext_analysed\tobservationdocument_recordeddtm clientvisit_visitidcode\"\"\".split(),\n",
    "        term_name=\"client_idcode.keyword\",\n",
    "        entered_list=[current_pat_client_id_code],\n",
    "        search_string='obscatalogmasteritem_displayname:(\"AoMRC_ClinicalSummary_FT\") AND '\n",
    "        + f\"observationdocument_recordeddtm:[{global_start_year}-{global_start_month} TO {global_end_year}-{global_end_month}]\",\n",
    "    )\n",
    "    return batch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pat_batch_demo(current_pat_client_id_code, search_term):\n",
    "    batch_target = cohort_searcher_with_terms_and_search(\n",
    "        index_name=\"epr_documents\",\n",
    "        fields_list=[\n",
    "            \"client_idcode\",\n",
    "            \"client_firstname\",\n",
    "            \"client_lastname\",\n",
    "            \"client_dob\",\n",
    "            \"client_gendercode\",\n",
    "            \"client_racecode\",\n",
    "            \"client_deceaseddtm\",\n",
    "            \"updatetime\",\n",
    "        ],\n",
    "        term_name=\"client_idcode.keyword\",\n",
    "        entered_list=[current_pat_client_id_code],\n",
    "        search_string=f\"updatetime:[{global_start_year}-{global_start_month} TO {global_end_year}-{global_end_month}] \",\n",
    "    )\n",
    "    return batch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in tqdm(range(0,len(all_patient_list))):\n",
    "# for k in tqdm(range(0,4)):\n",
    "def main(current_pat_client_id_code, target_date_range):\n",
    "    global skipped_counter\n",
    "    global start_time\n",
    "    start_time = time.time()\n",
    "\n",
    "    if (\n",
    "        current_pat_client_id_code + \"_\" + str(target_date_range)\n",
    "        not in stripped_list_start\n",
    "    ):\n",
    "\n",
    "        stripped_list = [\n",
    "            x.replace(\".csv\", \"\") for x in list_dir_wrapper(current_pat_lines_path)\n",
    "        ]\n",
    "\n",
    "        if (\n",
    "            current_pat_client_id_code + \"_\" + str(target_date_range)\n",
    "            not in stripped_list\n",
    "        ):\n",
    "            # print(start_time, current_pat_client_id_code)\n",
    "            try:\n",
    "                # current_pat_client_id_code = all_patient_list[k]\n",
    "\n",
    "                patient_vector = []\n",
    "\n",
    "                p_bar_entry = current_pat_client_id_code + \"_\" + str(target_date_range)\n",
    "                #\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 0, \"demo\")\n",
    "\n",
    "                if main_options.get(\"demo\"):\n",
    "                    current_pat_demo = get_demo(\n",
    "                        current_pat_client_id_code, target_date_range\n",
    "                    )\n",
    "                    patient_vector.append(current_pat_demo)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 1, \"bmi\")\n",
    "\n",
    "                if main_options.get(\"bmi\"):\n",
    "                    bmi_features = get_bmi_features(\n",
    "                        current_pat_client_id_code, target_date_range\n",
    "                    )\n",
    "                    patient_vector.append(bmi_features)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 2, \"bloods\")\n",
    "\n",
    "                if main_options.get(\"bloods\"):\n",
    "                    current_pat_bloods = get_current_pat_bloods(\n",
    "                        current_pat_client_id_code, target_date_range\n",
    "                    )\n",
    "                    patient_vector.append(current_pat_bloods)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 3, \"drugs\")\n",
    "\n",
    "                if main_options.get(\"drugs\"):\n",
    "                    current_pat_drugs = get_current_pat_drugs(\n",
    "                        current_pat_client_id_code, target_date_range\n",
    "                    )\n",
    "                    patient_vector.append(current_pat_drugs)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 4, \"diagnostics\")\n",
    "\n",
    "                if main_options.get(\"diagnostics\"):\n",
    "                    current_pat_diagnostics = get_current_pat_diagnostics(\n",
    "                        current_pat_client_id_code, target_date_range\n",
    "                    )\n",
    "                    patient_vector.append(current_pat_diagnostics)\n",
    "\n",
    "                # update_pbar(current_pat_client_id_code, start_time, 5, 'annotations', n_docs_to_annotate)\n",
    "\n",
    "                if main_options.get(\"annotations\"):\n",
    "                    df_pat_target = get_current_pat_annotations(\n",
    "                        current_pat_client_id_code, target_date_range\n",
    "                    )\n",
    "                    patient_vector.append(df_pat_target)\n",
    "\n",
    "                if main_options.get(\"annotations_mrc\"):\n",
    "                    df_pat_target = get_current_pat_annotations_mrc_cs(\n",
    "                        current_pat_client_id_code, target_date_range\n",
    "                    )\n",
    "                    patient_vector.append(df_pat_target)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 1, \"core_02\")\n",
    "\n",
    "                if main_options.get(\"core_02\"):\n",
    "                    df_pat_target = get_core_02(\n",
    "                        current_pat_client_id_code, target_date_range\n",
    "                    )\n",
    "                    patient_vector.append(df_pat_target)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 2, \"bed\")\n",
    "\n",
    "                if main_options.get(\"bed\"):\n",
    "                    df_pat_target = get_bed(\n",
    "                        current_pat_client_id_code, target_date_range\n",
    "                    )\n",
    "                    patient_vector.append(df_pat_target)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 3, \"vte_status\")\n",
    "\n",
    "                if main_options.get(\"vte_status\"):\n",
    "                    df_pat_target = get_vte_status(\n",
    "                        current_pat_client_id_code, target_date_range\n",
    "                    )\n",
    "                    patient_vector.append(df_pat_target)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 4, \"hosp_site\")\n",
    "\n",
    "                if main_options.get(\"hosp_site\"):\n",
    "                    df_pat_target = get_hosp_site(\n",
    "                        current_pat_client_id_code, target_date_range\n",
    "                    )\n",
    "                    patient_vector.append(df_pat_target)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 1, \"core_resus\")\n",
    "\n",
    "                if main_options.get(\"core_resus\"):\n",
    "                    df_pat_target = get_core_resus(\n",
    "                        current_pat_client_id_code, target_date_range\n",
    "                    )\n",
    "                    patient_vector.append(df_pat_target)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 2, \"news\")\n",
    "\n",
    "                if main_options.get(\"news\"):\n",
    "                    df_pat_target = get_news(\n",
    "                        current_pat_client_id_code, target_date_range\n",
    "                    )\n",
    "                    patient_vector.append(df_pat_target)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 2, \"concatenating\")\n",
    "\n",
    "                target_date_vector = enum_target_date_vector(\n",
    "                    target_date_range, current_pat_client_id_code\n",
    "                )\n",
    "\n",
    "                patient_vector.append(target_date_vector)\n",
    "\n",
    "                pat_concatted = pd.concat(patient_vector, axis=1)\n",
    "\n",
    "                pat_concatted.drop(\"client_idcode\", axis=1, inplace=True)\n",
    "\n",
    "                pat_concatted.insert(0, \"client_idcode\", current_pat_client_id_code)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 2, \"saving...\")\n",
    "\n",
    "                output_path = (\n",
    "                    current_pat_line_path\n",
    "                    + current_pat_client_id_code\n",
    "                    + \"/\"\n",
    "                    + str(current_pat_client_id_code)\n",
    "                    + \"_\"\n",
    "                    + str(target_date_range)\n",
    "                    + \".csv\"\n",
    "                )\n",
    "\n",
    "                if not remote_dump:\n",
    "\n",
    "                    pat_concatted.to_csv(output_path)\n",
    "                else:\n",
    "\n",
    "                    if multi_process:\n",
    "\n",
    "                        write_remote(output_path, pat_concatted, stfp_obj)\n",
    "                    else:\n",
    "                        with sftp_client.open(output_path, \"w\") as file:\n",
    "                            pat_concatted.to_csv(file)\n",
    "\n",
    "                update_pbar(\n",
    "                    p_bar_entry,\n",
    "                    start_time,\n",
    "                    2,\n",
    "                    f\"Done {len(pat_concatted.columns)} cols in {int(time.time() - start_time)}s, {int(len(pat_concatted.columns)/int(time.time() - start_time))} p/s\",\n",
    "                )\n",
    "\n",
    "                # print(time.time() - start_time, current_pat_client_id_code)\n",
    "            except RuntimeError:\n",
    "                print(\"Caught runtime error... is torch?\")\n",
    "                print(RuntimeError)\n",
    "                print(\"sleeping 1h\")\n",
    "                time.sleep(3600)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(traceback.format_exc())\n",
    "                print(f\"Reproduce on {current_pat_client_id_code, target_date_range}\")\n",
    "                template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
    "                message = template.format(type(e).__name__, e.args)\n",
    "                print(message)\n",
    "\n",
    "        else:\n",
    "            skipped_counter = skipped_counter + 1\n",
    "            pass\n",
    "            # print(f\"{current_pat_client_id_code} done already\")\n",
    "            # pat_concatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in tqdm(range(0,len(all_patient_list))):\n",
    "# for k in tqdm(range(0,4)):\n",
    "def main_multi(arg_list):\n",
    "\n",
    "    current_pat_client_id_code, target_date_range = arg_list[0], arg_list[1]\n",
    "\n",
    "    global skipped_counter\n",
    "    global start_time\n",
    "    start_time = time.time()\n",
    "\n",
    "    if (\n",
    "        current_pat_client_id_code + \"_\" + str(target_date_range)\n",
    "        not in stripped_list_start\n",
    "    ):\n",
    "\n",
    "        stripped_list = [\n",
    "            x.replace(\".csv\", \"\") for x in list_dir_wrapper(current_pat_lines_path)\n",
    "        ]\n",
    "\n",
    "        if (\n",
    "            current_pat_client_id_code + \"_\" + str(target_date_range)\n",
    "            not in stripped_list\n",
    "        ):\n",
    "            # print(start_time, current_pat_client_id_code)\n",
    "            try:\n",
    "                # current_pat_client_id_code = all_patient_list[k]\n",
    "\n",
    "                patient_vector = []\n",
    "\n",
    "                p_bar_entry = current_pat_client_id_code + \"_\" + str(target_date_range)\n",
    "                #\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 0, \"demo\")\n",
    "\n",
    "                if main_options.get(\"demo\"):\n",
    "                    current_pat_demo = get_demo(\n",
    "                        current_pat_client_id_code, target_date_range\n",
    "                    )\n",
    "                    patient_vector.append(current_pat_demo)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 1, \"bmi\")\n",
    "\n",
    "                if main_options.get(\"bmi\"):\n",
    "                    bmi_features = get_bmi_features(\n",
    "                        current_pat_client_id_code, target_date_range\n",
    "                    )\n",
    "                    patient_vector.append(bmi_features)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 2, \"bloods\")\n",
    "\n",
    "                if main_options.get(\"bloods\"):\n",
    "                    current_pat_bloods = get_current_pat_bloods(\n",
    "                        current_pat_client_id_code, target_date_range\n",
    "                    )\n",
    "                    patient_vector.append(current_pat_bloods)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 3, \"drugs\")\n",
    "\n",
    "                if main_options.get(\"drugs\"):\n",
    "                    current_pat_drugs = get_current_pat_drugs(\n",
    "                        current_pat_client_id_code, target_date_range\n",
    "                    )\n",
    "                    patient_vector.append(current_pat_drugs)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 4, \"diagnostics\")\n",
    "\n",
    "                if main_options.get(\"diagnostics\"):\n",
    "                    current_pat_diagnostics = get_current_pat_diagnostics(\n",
    "                        current_pat_client_id_code, target_date_range\n",
    "                    )\n",
    "                    patient_vector.append(current_pat_diagnostics)\n",
    "\n",
    "                # update_pbar(current_pat_client_id_code, start_time, 5, 'annotations', n_docs_to_annotate)\n",
    "\n",
    "                if main_options.get(\"annotations\"):\n",
    "                    df_pat_target = get_current_pat_annotations(\n",
    "                        current_pat_client_id_code, target_date_range\n",
    "                    )\n",
    "                    patient_vector.append(df_pat_target)\n",
    "\n",
    "                if main_options.get(\"annotations_mrc\"):\n",
    "                    df_pat_target = get_current_pat_annotations_mrc_cs(\n",
    "                        current_pat_client_id_code, target_date_range\n",
    "                    )\n",
    "                    patient_vector.append(df_pat_target)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 1, \"core_02\")\n",
    "\n",
    "                if main_options.get(\"core_02\"):\n",
    "                    df_pat_target = get_core_02(\n",
    "                        current_pat_client_id_code, target_date_range\n",
    "                    )\n",
    "                    patient_vector.append(df_pat_target)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 2, \"bed\")\n",
    "\n",
    "                if main_options.get(\"bed\"):\n",
    "                    df_pat_target = get_bed(\n",
    "                        current_pat_client_id_code, target_date_range\n",
    "                    )\n",
    "                    patient_vector.append(df_pat_target)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 3, \"vte_status\")\n",
    "\n",
    "                if main_options.get(\"vte_status\"):\n",
    "                    df_pat_target = get_vte_status(\n",
    "                        current_pat_client_id_code, target_date_range\n",
    "                    )\n",
    "                    patient_vector.append(df_pat_target)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 4, \"hosp_site\")\n",
    "\n",
    "                if main_options.get(\"hosp_site\"):\n",
    "                    df_pat_target = get_hosp_site(\n",
    "                        current_pat_client_id_code, target_date_range\n",
    "                    )\n",
    "                    patient_vector.append(df_pat_target)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 1, \"core_resus\")\n",
    "\n",
    "                if main_options.get(\"core_resus\"):\n",
    "                    df_pat_target = get_core_resus(\n",
    "                        current_pat_client_id_code, target_date_range\n",
    "                    )\n",
    "                    patient_vector.append(df_pat_target)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 2, \"news\")\n",
    "\n",
    "                if main_options.get(\"news\"):\n",
    "                    df_pat_target = get_news(\n",
    "                        current_pat_client_id_code, target_date_range\n",
    "                    )\n",
    "                    patient_vector.append(df_pat_target)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 2, \"concatenating\")\n",
    "\n",
    "                target_date_vector = enum_target_date_vector(\n",
    "                    target_date_range, current_pat_client_id_code\n",
    "                )\n",
    "\n",
    "                patient_vector.append(target_date_vector)\n",
    "\n",
    "                pat_concatted = pd.concat(patient_vector, axis=1)\n",
    "\n",
    "                pat_concatted.drop(\"client_idcode\", axis=1, inplace=True)\n",
    "\n",
    "                pat_concatted.insert(0, \"client_idcode\", current_pat_client_id_code)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 2, \"saving...\")\n",
    "\n",
    "                if not remote_dump:\n",
    "\n",
    "                    pat_concatted.to_csv(\n",
    "                        current_pat_line_path\n",
    "                        + str(current_pat_client_id_code)\n",
    "                        + \"_\"\n",
    "                        + str(target_date_range)\n",
    "                        + \".csv\"\n",
    "                    )\n",
    "                else:\n",
    "                    with sftp_client.open(\n",
    "                        current_pat_line_path\n",
    "                        + str(current_pat_client_id_code)\n",
    "                        + \"_\"\n",
    "                        + str(target_date_range)\n",
    "                        + \".csv\",\n",
    "                        \"w\",\n",
    "                    ) as file:\n",
    "                        pat_concatted.to_csv(file)\n",
    "\n",
    "                update_pbar(\n",
    "                    p_bar_entry,\n",
    "                    start_time,\n",
    "                    2,\n",
    "                    f\"Done {len(pat_concatted.columns)} cols in {int(time.time() - start_time)}s, {int(len(pat_concatted.columns)/int(time.time() - start_time))} p/s\",\n",
    "                )\n",
    "\n",
    "                # print(time.time() - start_time, current_pat_client_id_code)\n",
    "            except RuntimeError:\n",
    "                print(\"Caught runtime error... is torch?\")\n",
    "                print(RuntimeError)\n",
    "                print(\"sleeping 1h\")\n",
    "                time.sleep(3600)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(traceback.format_exc())\n",
    "                print(f\"Reproduce on {current_pat_client_id_code, target_date_range}\")\n",
    "                template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
    "                message = template.format(type(e).__name__, e.args)\n",
    "                print(message)\n",
    "\n",
    "        else:\n",
    "            skipped_counter = skipped_counter + 1\n",
    "            pass\n",
    "            # print(f\"{current_pat_client_id_code} done already\")\n",
    "            # pat_concatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_execution_threshold_low = 10\n",
    "slow_execution_threshold_high = 30\n",
    "slow_execution_threshold_extreme = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_pat_lines_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_pat_line_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_dir_wrapper(current_pat_line_path, sftp_client)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stripped_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patient_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stripped_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# if(remote_dump==False):\n",
    "#     stripped_list = [x.replace(\".csv\",\"\") for x in list_dir_wrapper(current_pat_line_path)]\n",
    "#     all_patient_list = [fruit for fruit in all_patient_list if fruit not in stripped_list]\n",
    "\n",
    "# else:\n",
    "#     stripped_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stripped_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_list_bool = False\n",
    "\n",
    "if priority_list_bool:\n",
    "    df_old_done = pd.read_csv(\n",
    "        \"/data/AS/Samora/HFE/HFE/v18/current_pat_lines_parts/current_pat_lines__part_0_merged.csv\",\n",
    "        usecols=[\"client_idcode\", \"Hemochromatosis (disorder)_count_subject_present\"],\n",
    "    )\n",
    "\n",
    "    priority_list = df_old_done[\n",
    "        df_old_done[\"Hemochromatosis (disorder)_count_subject_present\"] > 0\n",
    "    ][\"client_idcode\"].to_list()\n",
    "\n",
    "    all_patient_list = priority_list  # + all_patient_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# medcat_model_pack_316666b47dfaac07.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "if aliencat:\n",
    "    cat = CAT.load_model_pack(\n",
    "        \"/home/aliencat/samora/HFE/HFE/medcat_models/medcat_model_pack_316666b47dfaac07.zip\"\n",
    "    )\n",
    "elif dgx:\n",
    "    cat = CAT.load_model_pack(\n",
    "        \"/data/AS/Samora/HFE/HFE/v18/\"\n",
    "        + \"medcat_models/20230328_trained_model_hfe_redone/medcat_model_pack_316666b47dfaac07\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat = CAT.load_model_pack('/data/AS/Samora/HFE/HFE/v18/' + 'medcat_models/20230328_trained_model_hfe_redone/medcat_model_pack_316666b47dfaac07');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat = CAT.load_model_pack('/data/AS/Samora/HFE/HFE/v18/' + 'medcat_models/20230214_trained_model_hfe_redone/medcat_model_pack_bec3865f4a29ee20');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed()\n",
    "random.shuffle(all_patient_list)\n",
    "\n",
    "skipped_counter = 0\n",
    "t = trange(\n",
    "    len(all_patient_list),\n",
    "    desc=\"Bar desc\",\n",
    "    leave=True,\n",
    "    colour=\"GREEN\",\n",
    "    position=0,\n",
    "    total=len(all_patient_list),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patient_list_bu = all_patient_list.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_patient_list = all_patient_list[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_list = [\n",
    "    x.replace(\".csv\", \"\") for x in list_dir_wrapper(current_pat_lines_path, sftp_client)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stripped_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if strip_list:\n",
    "    stripped_list_start_copy = stripped_list.copy()\n",
    "\n",
    "    container_list = []\n",
    "\n",
    "    if remote_dump:\n",
    "        ssh_client = paramiko.SSHClient()\n",
    "        ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "        ssh_client.connect(hostname=hostname, username=username, password=password)\n",
    "\n",
    "        sftp_client = ssh_client.open_sftp()\n",
    "\n",
    "        for i in range(0, len(stripped_list)):\n",
    "            try:\n",
    "                if (\n",
    "                    len(sftp_client.listdir(current_pat_lines_path + stripped_list[i]))\n",
    "                    > 300\n",
    "                ):\n",
    "                    container_list.append(stripped_list[i])\n",
    "            except:\n",
    "                pass\n",
    "    #             stripped_list_start_copy.remove(stripped_list_start[i])\n",
    "\n",
    "    else:\n",
    "        for i in tqdm(range(0, len(stripped_list))):\n",
    "            if len(list_dir_wrapper(current_pat_lines_path + stripped_list[i])) > 300:\n",
    "                container_list.append(stripped_list[i])\n",
    "\n",
    "    stripped_list_start = container_list.copy()\n",
    "    stripped_list = container_list.copy()\n",
    "    if remote_dump:\n",
    "        sftp_client.close()\n",
    "        ssh_client.close()\n",
    "else:\n",
    "    stripped_list = []\n",
    "    stripped_list_start = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_arg_list = []\n",
    "for i in range(0, len(all_patient_list)):\n",
    "    for j in range(0, len(combinations)):\n",
    "        all_arg_list.append([all_patient_list[i], combinations[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_arg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed()\n",
    "# random.shuffle(all_arg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = trange(len(all_arg_list), desc='Bar desc', leave=True, colour=\"GREEN\", position=0, total=len(all_arg_list))\n",
    "# skipped_counter = 0\n",
    "# main_multi(all_arg_list[300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_additional_listdir = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_batch(\n",
    "    current_pat_client_id_code,\n",
    "    target_date_range,\n",
    "    batch_demo=None,\n",
    "    batch_smoking=None,\n",
    "    batch_core_02=None,\n",
    "    batch_bednumber=None,\n",
    "    batch_vte=None,\n",
    "    batch_hospsite=None,\n",
    "    batch_resus=None,\n",
    "    batch_news=None,\n",
    "    batch_bmi=None,\n",
    "    batch_diagnostics=None,\n",
    "    batch_epr=None,\n",
    "    batch_mct=None,\n",
    "    batch_bloods=None,\n",
    "    batch_drugs=None,\n",
    "    sftp_obj=None,\n",
    "):\n",
    "    global skipped_counter\n",
    "    global start_time\n",
    "    start_time = time.time()\n",
    "\n",
    "    already_done = False\n",
    "\n",
    "    if current_pat_client_id_code not in stripped_list_start:\n",
    "\n",
    "        if skip_additional_listdir:\n",
    "            stripped_list = stripped_list_start\n",
    "        else:\n",
    "\n",
    "            if (\n",
    "                len(\n",
    "                    list_dir_wrapper(\n",
    "                        current_pat_lines_path + current_pat_client_id_code, sftp_obj\n",
    "                    )\n",
    "                )\n",
    "                >= 336\n",
    "            ):\n",
    "                already_done = True\n",
    "                stripped_list_start.append(current_pat_client_id_code)\n",
    "            stripped_list = stripped_list_start.copy()\n",
    "\n",
    "            # stripped_list = []\n",
    "        #             stripped_list = [x for x in list_dir_wrapper(current_pat_lines_path)]\n",
    "\n",
    "        if current_pat_client_id_code not in stripped_list and not already_done:\n",
    "\n",
    "            try:\n",
    "                patient_vector = []\n",
    "\n",
    "                p_bar_entry = current_pat_client_id_code + \"_\" + str(target_date_range)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 0, \"demo\")\n",
    "\n",
    "                if main_options.get(\"demo\"):\n",
    "                    current_pat_demo = get_demo(\n",
    "                        current_pat_client_id_code, target_date_range, batch_demo\n",
    "                    )\n",
    "                    patient_vector.append(current_pat_demo)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 1, \"bmi\")\n",
    "\n",
    "                if main_options.get(\"bmi\"):\n",
    "                    bmi_features = get_bmi_features(\n",
    "                        current_pat_client_id_code, target_date_range, batch_bmi\n",
    "                    )\n",
    "                    patient_vector.append(bmi_features)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 2, \"bloods\")\n",
    "\n",
    "                if main_options.get(\"bloods\"):\n",
    "                    current_pat_bloods = get_current_pat_bloods(\n",
    "                        current_pat_client_id_code, target_date_range, batch_bloods\n",
    "                    )\n",
    "                    patient_vector.append(current_pat_bloods)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 3, \"drugs\")\n",
    "\n",
    "                if main_options.get(\"drugs\"):\n",
    "                    current_pat_drugs = get_current_pat_drugs(\n",
    "                        current_pat_client_id_code, target_date_range, batch_drugs\n",
    "                    )\n",
    "                    patient_vector.append(current_pat_drugs)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 4, \"diagnostics\")\n",
    "\n",
    "                if main_options.get(\"diagnostics\"):\n",
    "                    current_pat_diagnostics = get_current_pat_diagnostics(\n",
    "                        current_pat_client_id_code, target_date_range, batch_diagnostics\n",
    "                    )\n",
    "                    patient_vector.append(current_pat_diagnostics)\n",
    "\n",
    "                if main_options.get(\"annotations\"):\n",
    "                    df_pat_target = get_current_pat_annotations(\n",
    "                        current_pat_client_id_code,\n",
    "                        target_date_range,\n",
    "                        batch_epr,\n",
    "                        sftp_obj,\n",
    "                    )\n",
    "                    patient_vector.append(df_pat_target)\n",
    "\n",
    "                if main_options.get(\"annotations_mrc\"):\n",
    "                    df_pat_target = get_current_pat_annotations_mrc_cs(\n",
    "                        current_pat_client_id_code,\n",
    "                        target_date_range,\n",
    "                        batch_mct,\n",
    "                        sftp_obj,\n",
    "                    )\n",
    "                    patient_vector.append(df_pat_target)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 1, \"core_02\")\n",
    "\n",
    "                if main_options.get(\"core_02\"):\n",
    "                    df_pat_target = get_core_02(\n",
    "                        current_pat_client_id_code, target_date_range, batch_core_02\n",
    "                    )\n",
    "                    patient_vector.append(df_pat_target)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 2, \"bed\")\n",
    "\n",
    "                if main_options.get(\"bed\"):\n",
    "                    df_pat_target = get_bed(\n",
    "                        current_pat_client_id_code, target_date_range, batch_bednumber\n",
    "                    )\n",
    "                    patient_vector.append(df_pat_target)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 3, \"vte_status\")\n",
    "\n",
    "                if main_options.get(\"vte_status\"):\n",
    "                    df_pat_target = get_vte_status(\n",
    "                        current_pat_client_id_code, target_date_range, batch_vte\n",
    "                    )\n",
    "                    patient_vector.append(df_pat_target)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 4, \"hosp_site\")\n",
    "\n",
    "                if main_options.get(\"hosp_site\"):\n",
    "                    df_pat_target = get_hosp_site(\n",
    "                        current_pat_client_id_code, target_date_range, batch_hospsite\n",
    "                    )\n",
    "                    patient_vector.append(df_pat_target)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 1, \"core_resus\")\n",
    "\n",
    "                if main_options.get(\"core_resus\"):\n",
    "                    df_pat_target = get_core_resus(\n",
    "                        current_pat_client_id_code, target_date_range, batch_resus\n",
    "                    )\n",
    "                    patient_vector.append(df_pat_target)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 2, \"news\")\n",
    "\n",
    "                if main_options.get(\"news\"):\n",
    "                    df_pat_target = get_news(\n",
    "                        current_pat_client_id_code, target_date_range, batch_news\n",
    "                    )\n",
    "                    patient_vector.append(df_pat_target)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 2, \"concatenating\")\n",
    "\n",
    "                target_date_vector = enum_target_date_vector(\n",
    "                    target_date_range, current_pat_client_id_code\n",
    "                )\n",
    "\n",
    "                patient_vector.append(target_date_vector)\n",
    "\n",
    "                pat_concatted = pd.concat(patient_vector, axis=1)\n",
    "\n",
    "                pat_concatted.drop(\"client_idcode\", axis=1, inplace=True)\n",
    "\n",
    "                pat_concatted.insert(0, \"client_idcode\", current_pat_client_id_code)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 2, \"saving...\")\n",
    "\n",
    "                output_path = (\n",
    "                    current_pat_line_path\n",
    "                    + current_pat_client_id_code\n",
    "                    + \"/\"\n",
    "                    + str(current_pat_client_id_code)\n",
    "                    + \"_\"\n",
    "                    + str(target_date_range)\n",
    "                    + \".csv\"\n",
    "                )\n",
    "\n",
    "                if not remote_dump:\n",
    "\n",
    "                    pat_concatted.to_csv(output_path)\n",
    "                else:\n",
    "\n",
    "                    if multi_process:\n",
    "\n",
    "                        write_remote(output_path, pat_concatted, sftp_obj)\n",
    "                    else:\n",
    "                        with sftp_client.open(output_path, \"w\") as file:\n",
    "                            pat_concatted.to_csv(file)\n",
    "\n",
    "                # display(type(pat_concatted))\n",
    "                try:\n",
    "                    update_pbar(\n",
    "                        p_bar_entry,\n",
    "                        start_time,\n",
    "                        2,\n",
    "                        f\"Done {len(pat_concatted.columns)} cols in {int(time.time() - start_time)}s, {int((len(pat_concatted.columns)+1)/int(time.time() - start_time)+1)} p/s\",\n",
    "                    )\n",
    "                except:\n",
    "                    update_pbar(\n",
    "                        p_bar_entry,\n",
    "                        start_time,\n",
    "                        2,\n",
    "                        f\"Columns n={len(pat_concatted.columns)}\",\n",
    "                    )\n",
    "                    pass\n",
    "\n",
    "                # display(pat_concatted)\n",
    "                # print(time.time() - start_time, current_pat_client_id_code)\n",
    "            except RuntimeError:\n",
    "                print(\"Caught runtime error... is torch?\")\n",
    "                print(RuntimeError)\n",
    "                print(\"sleeping 1h\")\n",
    "                time.sleep(3600)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(traceback.format_exc())\n",
    "                print(f\"Reproduce on {current_pat_client_id_code, target_date_range}\")\n",
    "                template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
    "                message = template.format(type(e).__name__, e.args)\n",
    "                print(message)\n",
    "\n",
    "        else:\n",
    "            if not multi_process:\n",
    "                skipped_counter = skipped_counter + 1\n",
    "            else:\n",
    "                with skipped_counter.get_lock():\n",
    "                    skipped_counter.value += 1\n",
    "            pass\n",
    "            # print(f\"{current_pat_client_id_code} done already\")\n",
    "            # pat_concatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_start_year, global_start_month, global_end_year, global_end_month = (\n",
    "    \"1995\",\n",
    "    \"01\",\n",
    "    \"2023\",\n",
    "    \"11\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122",
   "metadata": {},
   "source": [
    "#get_pat batches\n",
    "\n",
    "search_term = 'CORE_SmokingStatus'\n",
    "\n",
    "batch_smoking = get_pat_batch_obs(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "search_term = 'CORE_SpO2'\n",
    "\n",
    "batch_core_02 = get_pat_batch_obs(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "search_term = 'CORE_BedNumber3'\n",
    "\n",
    "batch_bednumber = get_pat_batch_obs(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "search_term = 'CORE_VTE_STATUS'\n",
    "\n",
    "batch_vte = get_pat_batch_obs(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "search_term = 'CORE_HospitalSite'\n",
    "\n",
    "batch_hospsite = get_pat_batch_obs(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "search_term = 'CORE_RESUS_STATUS'\n",
    "\n",
    "batch_resus = get_pat_batch_obs(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "search_term = None # inside function\n",
    "batch_news = get_pat_batch_news(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "search_term = None # inside function\n",
    "batch_bmi = get_pat_batch_bmi(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "search_term = None # inside function\n",
    "batch_diagnostics = get_pat_batch_diagnostics(current_pat_client_id_code, search_term)\n",
    "\n",
    "search_term = None # inside function\n",
    "batch_drugs = get_pat_batch_drugs(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "search_term = None # inside function\n",
    "batch_epr = get_pat_batch_epr_docs(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "search_term = None # inside function\n",
    "batch_mct = get_pat_batch_mct_docs(current_pat_client_id_code, search_term)\n",
    "\n",
    "search_term = None # inside function\n",
    "batch_demo = get_pat_batch_demo(current_pat_client_id_code, search_term)\n",
    "\n",
    "search_term = None # inside function\n",
    "batch_bloods =  get_pat_batch_bloods(current_pat_client_id_code, search_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123",
   "metadata": {},
   "source": [
    "batch_bloods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124",
   "metadata": {},
   "source": [
    "target_date_range = (2022, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125",
   "metadata": {},
   "source": [
    "target_date_range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126",
   "metadata": {},
   "source": [
    "#pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127",
   "metadata": {},
   "source": [
    "#%%prun\n",
    "\n",
    "\n",
    "main_batch(current_pat_client_id_code,\n",
    "               target_date_range,\n",
    "               batch_demo = batch_demo,\n",
    "               batch_smoking = batch_smoking,\n",
    "               batch_core_02 = batch_core_02,\n",
    "               batch_bednumber = batch_bednumber,\n",
    "               batch_vte = batch_vte,\n",
    "               batch_hospsite = batch_hospsite,\n",
    "               batch_resus = batch_resus,\n",
    "               batch_news = batch_news,\n",
    "               batch_bmi = batch_bmi,\n",
    "               batch_diagnostics = batch_diagnostics,\n",
    "               batch_epr = batch_epr,\n",
    "               batch_mct = batch_mct,\n",
    "               batch_bloods = batch_bloods,\n",
    "               batch_drugs = batch_drugs\n",
    "            \n",
    "              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotate_only = False\n",
    "\n",
    "if annotate_only:\n",
    "    random.seed()\n",
    "    random.shuffle(all_patient_list)\n",
    "\n",
    "    skipped_counter = 0\n",
    "    t = trange(\n",
    "        len(all_patient_list),\n",
    "        desc=\"Bar desc\",\n",
    "        leave=True,\n",
    "        colour=\"GREEN\",\n",
    "        position=0,\n",
    "        total=len(all_patient_list),\n",
    "    )\n",
    "\n",
    "    for i in t:\n",
    "        global_start_year, global_start_month, global_end_year, global_end_month = (\n",
    "            \"1995\",\n",
    "            \"01\",\n",
    "            \"2023\",\n",
    "            \"11\",\n",
    "        )\n",
    "\n",
    "        current_pat_doc_batch = cohort_searcher_with_terms_and_search(\n",
    "            index_name=\"epr_documents\",\n",
    "            fields_list=\"\"\"client_idcode document_guid\tdocument_description\tbody_analysed updatetime clientvisit_visitidcode\"\"\".split(),\n",
    "            term_name=\"client_idcode.keyword\",\n",
    "            entered_list=[all_patient_list[i]],\n",
    "            search_string=f\"updatetime:[{global_start_year}-{global_start_month} TO {global_end_year}-{global_end_month}] \",\n",
    "        )\n",
    "\n",
    "        current_pat_doc_mct_batch = cohort_searcher_with_terms_and_search(\n",
    "            index_name=\"observations\",\n",
    "            fields_list=\"\"\"observation_guid client_idcode\tobscatalogmasteritem_displayname\tobservation_valuetext_analysed\tobservationdocument_recordeddtm clientvisit_visitidcode\"\"\".split(),\n",
    "            term_name=\"client_idcode.keyword\",\n",
    "            entered_list=[all_patient_list[i]],\n",
    "            search_string='obscatalogmasteritem_displayname:(\"AoMRC_ClinicalSummary_FT\") AND '\n",
    "            + f\"observationdocument_recordeddtm:[{global_start_year}-{global_start_month} TO {global_end_year}-{global_end_month}]\",\n",
    "        )\n",
    "\n",
    "        for j in range(0, len(combinations)):\n",
    "            try:\n",
    "                if all_patient_list[i] not in stripped_list:\n",
    "                    get_current_pat_annotations_batch_to_file(\n",
    "                        all_patient_list[i], combinations[j], current_pat_doc_batch\n",
    "                    )\n",
    "\n",
    "                    get_current_pat_annotations_mct_batch_to_file(\n",
    "                        all_patient_list[i], combinations[j], current_pat_doc_mct_batch\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(all_patient_list[i], combinations[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_pat_line_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_remote(path, csv_file, sftp_obj=None):\n",
    "    # print(\"writing remote\")\n",
    "    if not share_sftp:\n",
    "        ssh_client = paramiko.SSHClient()\n",
    "        ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "        ssh_client.connect(hostname=hostname, username=username, password=password)\n",
    "\n",
    "        sftp_client = ssh_client.open_sftp()\n",
    "        sftp_obj = sftp_client\n",
    "\n",
    "    with sftp_obj.open(path, \"w\") as file:\n",
    "        csv_file.to_csv(file)\n",
    "\n",
    "    if not share_sftp:\n",
    "        sftp_obj.close()\n",
    "        sftp_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed()\n",
    "random.shuffle(all_patient_list)\n",
    "\n",
    "skipped_counter = 0\n",
    "t = trange(\n",
    "    len(all_patient_list),\n",
    "    desc=\"Bar desc\",\n",
    "    leave=True,\n",
    "    colour=\"GREEN\",\n",
    "    position=0,\n",
    "    total=len(all_patient_list),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pat_maker(i):\n",
    "    global skipped_counter\n",
    "    global stripped_list\n",
    "\n",
    "    current_pat_client_id_code = all_patient_list[i]\n",
    "\n",
    "    p_bar_entry = current_pat_client_id_code\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    update_pbar(p_bar_entry, start_time, 0, f\"Pat_maker called on {i}...\")\n",
    "\n",
    "    # time.sleep(random.randint(1, 50))\n",
    "    # i, sftp_obj = i[0], i[1]\n",
    "    if remote_dump:\n",
    "        ssh_client = paramiko.SSHClient()\n",
    "        ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "        ssh_client.connect(\n",
    "            hostname=hostname, username=username, password=password, timeout=60\n",
    "        )\n",
    "\n",
    "        sftp_obj = ssh_client.open_sftp()\n",
    "    else:\n",
    "        sftp_obj = None\n",
    "\n",
    "    # get_pat batches\n",
    "\n",
    "    stripped_list = stripped_list_start.copy()\n",
    "\n",
    "    if current_pat_client_id_code not in stripped_list_start:\n",
    "\n",
    "        update_pbar(p_bar_entry, start_time, 0, \"Getting batches...\")\n",
    "\n",
    "        search_term = None  # inside function\n",
    "        batch_epr = get_pat_batch_epr_docs(current_pat_client_id_code, search_term)\n",
    "\n",
    "        search_term = None  # inside function\n",
    "        batch_mct = get_pat_batch_mct_docs(current_pat_client_id_code, search_term)\n",
    "\n",
    "        if not annot_first:\n",
    "\n",
    "            search_term = \"CORE_SmokingStatus\"\n",
    "\n",
    "            batch_smoking = get_pat_batch_obs(current_pat_client_id_code, search_term)\n",
    "\n",
    "            search_term = \"CORE_SpO2\"\n",
    "\n",
    "            batch_core_02 = get_pat_batch_obs(current_pat_client_id_code, search_term)\n",
    "\n",
    "            search_term = \"CORE_BedNumber3\"\n",
    "\n",
    "            batch_bednumber = get_pat_batch_obs(current_pat_client_id_code, search_term)\n",
    "\n",
    "            search_term = \"CORE_VTE_STATUS\"\n",
    "\n",
    "            batch_vte = get_pat_batch_obs(current_pat_client_id_code, search_term)\n",
    "\n",
    "            search_term = \"CORE_HospitalSite\"\n",
    "\n",
    "            batch_hospsite = get_pat_batch_obs(current_pat_client_id_code, search_term)\n",
    "\n",
    "            search_term = \"CORE_RESUS_STATUS\"\n",
    "\n",
    "            batch_resus = get_pat_batch_obs(current_pat_client_id_code, search_term)\n",
    "\n",
    "            search_term = None  # inside function\n",
    "            batch_news = get_pat_batch_news(current_pat_client_id_code, search_term)\n",
    "\n",
    "            search_term = None  # inside function\n",
    "            batch_bmi = get_pat_batch_bmi(current_pat_client_id_code, search_term)\n",
    "\n",
    "            search_term = None  # inside function\n",
    "            batch_diagnostics = get_pat_batch_diagnostics(\n",
    "                current_pat_client_id_code, search_term\n",
    "            )\n",
    "\n",
    "            search_term = None  # inside function\n",
    "            batch_drugs = get_pat_batch_drugs(current_pat_client_id_code, search_term)\n",
    "\n",
    "            search_term = None  # inside function\n",
    "            batch_demo = get_pat_batch_demo(current_pat_client_id_code, search_term)\n",
    "\n",
    "            search_term = None  # inside function\n",
    "            batch_bloods = get_pat_batch_bloods(current_pat_client_id_code, search_term)\n",
    "\n",
    "        update_pbar(\n",
    "            p_bar_entry, start_time, 0, f\"Done batches in {time.time()-start_time}\"\n",
    "        )\n",
    "\n",
    "        run_on_pat = False\n",
    "\n",
    "        only_check_last = True\n",
    "\n",
    "        last_check = all_patient_list[i] not in stripped_list\n",
    "\n",
    "        skip_check = last_check\n",
    "\n",
    "        for j in range(0, len(combinations)):\n",
    "            try:\n",
    "                if only_check_last:\n",
    "                    run_on_pat = last_check\n",
    "                else:\n",
    "                    run_on_pat = all_patient_list[i] not in stripped_list\n",
    "\n",
    "                if run_on_pat:\n",
    "                    if annot_first:\n",
    "\n",
    "                        get_current_pat_annotations_batch_to_file(\n",
    "                            all_patient_list[i],\n",
    "                            combinations[j],\n",
    "                            batch_epr,\n",
    "                            sftp_obj,\n",
    "                            skip_check=skip_check,\n",
    "                        )\n",
    "\n",
    "                        get_current_pat_annotations_mct_batch_to_file(\n",
    "                            all_patient_list[i],\n",
    "                            combinations[j],\n",
    "                            batch_mct,\n",
    "                            sftp_obj,\n",
    "                            skip_check=skip_check,\n",
    "                        )\n",
    "\n",
    "                    else:\n",
    "                        main_batch(\n",
    "                            all_patient_list[i],\n",
    "                            combinations[j],\n",
    "                            batch_demo=batch_demo,\n",
    "                            batch_smoking=batch_smoking,\n",
    "                            batch_core_02=batch_core_02,\n",
    "                            batch_bednumber=batch_bednumber,\n",
    "                            batch_vte=batch_vte,\n",
    "                            batch_hospsite=batch_hospsite,\n",
    "                            batch_resus=batch_resus,\n",
    "                            batch_news=batch_news,\n",
    "                            batch_bmi=batch_bmi,\n",
    "                            batch_diagnostics=batch_diagnostics,\n",
    "                            batch_epr=batch_epr,\n",
    "                            batch_mct=batch_mct,\n",
    "                            batch_bloods=batch_bloods,\n",
    "                            batch_drugs=batch_drugs,\n",
    "                            sftp_obj=sftp_obj,\n",
    "                        )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\n",
    "                    f\"Exception in patmaker on {all_patient_list[i], combinations[j]}\"\n",
    "                )\n",
    "                print(traceback.format_exc())\n",
    "        if remote_dump:\n",
    "            sftp_obj.close()\n",
    "            ssh_client.close()\n",
    "    else:\n",
    "        if not multi_process:\n",
    "            skipped_counter = skipped_counter + 1\n",
    "            update_pbar(str(i), start_time, 0, f\"Skipped {i}\")\n",
    "        else:\n",
    "            with skipped_counter.get_lock():\n",
    "                skipped_counter.value += 1\n",
    "            update_pbar(str(i), start_time, 0, f\"Skipped {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed()\n",
    "random.shuffle(all_patient_list)\n",
    "all_patient_list = [x for x in all_patient_list if x not in stripped_list_start]\n",
    "\n",
    "skipped_counter = 0\n",
    "t = trange(\n",
    "    len(all_patient_list),\n",
    "    desc=\"Bar desc\",\n",
    "    leave=True,\n",
    "    colour=\"GREEN\",\n",
    "    position=0,\n",
    "    total=len(all_patient_list),\n",
    ")\n",
    "\n",
    "all_patient_list = [x for x in all_patient_list if x not in stripped_list_start]\n",
    "\n",
    "skipped_counter = 0\n",
    "t = trange(\n",
    "    len(all_patient_list),\n",
    "    desc=\"Bar desc\",\n",
    "    leave=True,\n",
    "    colour=\"GREEN\",\n",
    "    position=0,\n",
    "    total=len(all_patient_list),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pat_maker(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed()\n",
    "# random.shuffle(all_patient_list)\n",
    "\n",
    "# skipped_counter = 0\n",
    "# t = trange(10, desc='Bar desc', leave=True, colour=\"GREEN\", position=0, total=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pat_maker(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssh_client = paramiko.SSHClient()\n",
    "# ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "# ssh_client.connect(hostname=hostname, username=username, password=password, timeout=10)\n",
    "\n",
    "# sftp_client = ssh_client.open_sftp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# could annotate every document in the batch and store result in a dictionary of dates, use dict lookup to fill files in loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Value\n",
    "\n",
    "skipped_counter = None\n",
    "\n",
    "\n",
    "def init(args):\n",
    "    \"\"\"store the counter for later use\"\"\"\n",
    "    global skipped_counter\n",
    "    skipped_counter = args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init?\n",
    "stripped_list = stripped_list_start.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_process = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# %%prun\n",
    "pat_maker(0)\n",
    "stripped_list_start.append(all_patient_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipped_counter = 0\n",
    "t = trange(\n",
    "    len(all_patient_list),\n",
    "    desc=\"Bar desc\",\n",
    "    leave=True,\n",
    "    colour=\"GREEN\",\n",
    "    position=0,\n",
    "    total=len(all_patient_list),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_process = True\n",
    "\n",
    "# if(multi_process):\n",
    "#     #sftp_client.close()\n",
    "#     ssh_client.close()\n",
    "\n",
    "if multi_process:\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        skipped_counter = Value(\"i\", 0)\n",
    "\n",
    "        pool = Pool(processes=3, initializer=init, initargs=(skipped_counter,))\n",
    "        # pool = eventlet.GreenPool(size=1000)\n",
    "        for patient in pool.imap(pat_maker, [x for x in range(len(all_patient_list))]):\n",
    "\n",
    "            _\n",
    "        pool.close()\n",
    "else:\n",
    "    for i in t:\n",
    "        pat_maker(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "152",
   "metadata": {},
   "source": [
    "# Threadpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "if multi_process:\n",
    "    if __name__ == \"__main__\":\n",
    "        skipped_counter = Value(\"i\", 0)\n",
    "\n",
    "        pool = ThreadPool(processes=8, initializer=init, initargs=(skipped_counter,))\n",
    "        # pool = ThreadPool(80)\n",
    "        results = pool.map(pat_maker, [x for x in range(0, len(all_patient_list))])\n",
    "    pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
