{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a22939",
   "metadata": {},
   "outputs": [],
   "source": [
    "aliencat = False\n",
    "dgx = True\n",
    "\n",
    "\n",
    "\n",
    "batch_mode = True\n",
    "remote_dump = True\n",
    "\n",
    "negated_presence_annotations = True\n",
    "store_annot = True\n",
    "\n",
    "share_sftp = True\n",
    "multi_process = True\n",
    "\n",
    "annot_first = True\n",
    "strip_list = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6865fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#time.sleep(86000 * 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f46de73",
   "metadata": {},
   "source": [
    "#%%javascript\n",
    "#IPython.notebook.kernel.execute('nb_name = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a94fe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install 'paramiko-3.1.0-py3-none-any.whl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc3d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import paramiko\n",
    "from os.path import exists\n",
    "import random\n",
    "from datetime import datetime, timedelta, timezone\n",
    "#nb_full_path = os.path.join(os.getcwd(), nb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1112bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_name = 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13d8b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not aliencat or dgx):\n",
    "\n",
    "    import sys\n",
    "    import logging\n",
    "\n",
    "    nblog = open(f\"{nb_name}.log\", \"w\")\n",
    "    nblog = open(f\"{nb_name}.log\", \"a+\")\n",
    "    sys.stdout.echo = nblog\n",
    "    sys.stderr.echo = nblog\n",
    "\n",
    "    get_ipython().log.handlers[0].stream = nblog\n",
    "    get_ipython().log.setLevel(logging.INFO)\n",
    "\n",
    "    get_ipython().run_line_magic('autosave', '5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981b1ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tqdm import trange\n",
    "from colorama import Fore, Back, Style\n",
    "color_bars = [Fore.RED,\n",
    "    Fore.GREEN,\n",
    "    Fore.BLUE,\n",
    "    Fore.MAGENTA,\n",
    "    Fore.YELLOW,\n",
    "    Fore.CYAN,\n",
    "    Fore.WHITE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b7f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.utils import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7b9870",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from csv import writer\n",
    "import warnings\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "#import tqdm\n",
    "import re\n",
    "import sys  \n",
    "import numpy as np\n",
    "import os, sys\n",
    "sys.path.insert(0,'/home/aliencat/samora/gloabl_files')\n",
    "sys.path.insert(0,'/data/AS/Samora/gloabl_files')\n",
    "#from cogstack_v8 import *\n",
    "from cogstack_v8_lite import *\n",
    "from credentials import *\n",
    "from os.path import exists\n",
    "#import cogstack_v8\n",
    "from COGStats import *\n",
    "from scipy import stats\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b28fd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "from datetime import datetime\n",
    "def convert_date(date_string):\n",
    "    date_string = date_string.split(\"T\")[0]\n",
    "    date_object = datetime.strptime(date_string, \"%Y-%m-%d\")\n",
    "    return date_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f534e3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_free_gpu():\n",
    "    gpu_stats = subprocess.check_output([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=memory.used,memory.free\"])\n",
    "    gpu_df = pd.read_csv(StringIO(gpu_stats.decode('utf-8')),\n",
    "                         names=['memory.used', 'memory.free'],\n",
    "                         skiprows=1)\n",
    "    print('GPU usage:\\n{}'.format(gpu_df))\n",
    "    gpu_df['memory.free'] = gpu_df['memory.free'].map(lambda x: x.rstrip(' [MiB]'))\n",
    "    idx = gpu_df['memory.free'].astype(int).idxmax()\n",
    "    print('Returning GPU{} with {} free MiB'.format(idx, gpu_df.iloc[idx]['memory.free']))\n",
    "    return int(idx), gpu_df.iloc[idx]['memory.free']\n",
    "import os\n",
    "import subprocess\n",
    "from io import StringIO\n",
    "\n",
    "gpu_index,free_mem  = get_free_gpu()\n",
    "\n",
    "if(int(free_mem)>4000):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_index)\n",
    "    print(f\"Setting gpu with {free_mem} free\")\n",
    "else:\n",
    "    print(f\"Setting NO gpu, most free memory: {free_mem} !\")\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a32a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medcat.cat import CAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99a08ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1177e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#suffix='_time'\n",
    "suffix = ''\n",
    "\n",
    "\n",
    "treatment_doc_filename = f'treatment_docs_v3.csv'\n",
    "treatment_control_ratio_n = 1 #1:n \n",
    "pre_annotation_path = f'current_pat_annots_parts{suffix}/'\n",
    "pre_annotation_path_mrc = f'current_pat_annots_mrc_parts{suffix}/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(pre_annotation_path)\n",
    "print(pre_annotation_path_mrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4850d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "if(remote_dump==False):\n",
    "    \n",
    "    pre_path = '/mnt/hdd1/samora/HFE_time_v1/'\n",
    "    \n",
    "    pre_annotation_path = pre_path + pre_annotation_path\n",
    "    \n",
    "    pre_annotation_path_mrc = pre_path + pre_annotation_path_mrc\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    Path(pre_annotation_path).mkdir(parents=True, exist_ok=True)\n",
    "    Path(pre_annotation_path_mrc).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(pre_annotation_path)\n",
    "    print(pre_annotation_path_mrc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f65d139",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_pat_line_path = f\"current_pat_lines_parts{suffix}/\"\n",
    "from pathlib import Path\n",
    "if(remote_dump==False):\n",
    "    \n",
    "    current_pat_line_path = pre_path + current_pat_line_path\n",
    "    \n",
    "    current_pat_lines_path = current_pat_line_path\n",
    "    \n",
    "    Path(current_pat_line_path).mkdir(parents=True, exist_ok=True)\n",
    "  \n",
    "\n",
    "\n",
    "print(current_pat_line_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a856ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get treatment docs if not already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528fe445",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_exists = exists(treatment_doc_filename)\n",
    "if (file_exists==False):\n",
    "    docs = cohort_searcher_no_terms(index_name = \"epr_documents\", \n",
    "                        fields_list = \"\"\"client_idcode document_guid\tdocument_description\tbody_analysed updatetime clientvisit_visitidcode\"\"\".split(),\n",
    "                        search_string = \"\"\" \"Haemochromatosis\" \n",
    "                        OR \"Hemochromatosis\" \n",
    "                        OR \"HFE\" \n",
    "                        OR \"HHC\" \n",
    "                        OR \"c282y\" \n",
    "                        OR \"h63d\" \n",
    "                        OR \"S65C\" \n",
    "                        OR \"Cys282Tyr\" \n",
    "                        OR \"p.C282Y\" \n",
    "                        OR \"HHemochromatosis\" \n",
    "                        OR \"HLAH\"\n",
    "                        OR \"Bronze diabetes\"\n",
    "                        OR \"Bronzed cirrhosis\"\n",
    "                        OR \"282y\"\n",
    "                        OR \"282C/Y\"\n",
    "                        OR \"rs1799945\"\n",
    "                        OR \"rs1800562\"\n",
    "                        OR \"rs1800730\"\n",
    "                        OR \"c.187C>G\"\n",
    "                        OR \"c.845G>A\"\n",
    "                        OR \"c.193A>T\"\n",
    "                        OR \"p.His63Asp\"\n",
    "                        OR \"p. Cys282Tyr\"\n",
    "                        OR \"p.Ser65Cys\"\n",
    "                        OR \"Transfusional haemosiderosis\"\n",
    "                        OR \"Gly320Val\"\n",
    "                        OR \"Troisier\"\n",
    "                        OR \"Iron Storage Disorder\"\n",
    "                        OR \"C282Y/H63D\"\n",
    "                        \n",
    "                         \n",
    "                         \n",
    "                        \"\"\")\n",
    "    docs.to_csv(treatment_doc_filename)\n",
    "else:\n",
    "    docs = pd.read_csv(treatment_doc_filename)\n",
    "    \n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9c51ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d423b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs['client_idcode'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cde0cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_filter = False\n",
    "if(use_filter):\n",
    "    json_filter_path = '/data/AS/Samora/HFE/HFE/v18/MedCAT_Export_With_Text_2022-12-24_21_30_48.json'\n",
    "    import json\n",
    "\n",
    "    with open(json_filter_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        \n",
    "    len(json_data['projects'][0])\n",
    "    json_cuis = json_data['projects'][0]['cuis'].split(\",\")\n",
    "    cat.cdb.filter_by_cui(json_cuis)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795acd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_client_id_list = list(docs['client_idcode'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6509a914",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(treatment_client_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225093c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "use_controls = False\n",
    "if(use_controls):\n",
    "    # Get control docs default 1:1\n",
    "\n",
    "    all_idcodes = pd.read_csv('all_client_idcodes_epr_unique.csv')['client_idcode']\n",
    "\n",
    "    \n",
    "    print(len(all_idcodes), len(treatment_client_id_list))\n",
    "\n",
    "    full_control_client_id_list = list(set(all_idcodes) - set(treatment_client_id_list))\n",
    "    \n",
    "    full_control_client_id_list.sort() # ensure sort for repeatability\n",
    "\n",
    "    len(full_control_client_id_list) - len(all_idcodes)\n",
    "\n",
    "    n_treatments = len(treatment_client_id_list) * treatment_control_ratio_n\n",
    "    print(f\"{n_treatments} selected as controls\") #Soft control selection, many treatments will be false positives\n",
    "    treatment_control_sample = pd.Series(full_control_client_id_list).sample(n_treatments, random_state=42)\n",
    "\n",
    "    treatment_control_sample\n",
    "\n",
    "    all_patient_list_control = list(treatment_control_sample.values)\n",
    "    \n",
    "    with open('control_list.pkl', 'wb') as f:\n",
    "        pickle.dump(all_patient_list_control, f)\n",
    "        \n",
    "    print(all_patient_list_control[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf66069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patient_list = list(treatment_client_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccdf7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(use_controls):\n",
    "    all_patient_list = all_patient_list + all_patient_list_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d1e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_concatted_master_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5639c756",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_builder = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623948ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_threshold = 200\n",
    "\n",
    "failed_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b8ffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(all_patient_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45920ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_patient_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ebab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exist_check(path, sftp_obj=None):\n",
    "        if(remote_dump):\n",
    "            return sftp_exists(path, sftp_obj)\n",
    "        else:\n",
    "            return exists(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dbf09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"remote_dump {remote_dump}\")\n",
    "print(pre_annotation_path)\n",
    "print(pre_annotation_path_mrc)\n",
    "print(current_pat_line_path)\n",
    "\n",
    "if(remote_dump):\n",
    "\n",
    "\n",
    "    pre_path = '/mnt/hdd1/samora/HFE_time_v1/'\n",
    "\n",
    "    def sftp_exists(path, sftp_obj=None):\n",
    "        try:\n",
    "            if(share_sftp == False):\n",
    "                ssh_client = paramiko.SSHClient()\n",
    "                ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "                ssh_client.connect(hostname=hostname, username=username, password=password)\n",
    "\n",
    "                sftp_obj = ssh_client.open_sftp()\n",
    "            \n",
    "            sftp_obj.stat(path)\n",
    "            \n",
    "            if(share_sftp == False):\n",
    "                sftp_obj.close()\n",
    "                sftp_obj.close()\n",
    "            return True\n",
    "        except FileNotFoundError:\n",
    "            return False\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    # Set the hostname, username, and password for the remote machine\n",
    "    \n",
    "    if(not aliencat or dgx):\n",
    "        hostname = '%HOSTIPADDRESS%'\n",
    "        \n",
    "    if(aliencat and not dgx):\n",
    "        hostname = 'localhost'\n",
    "    \n",
    "    username = '%USERNAME%'\n",
    "    password = '%PASSWORD%'\n",
    "\n",
    "    # Create an SSH client and connect to the remote machine\n",
    "    ssh_client = paramiko.SSHClient()\n",
    "    ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh_client.connect(hostname=hostname, username=username, password=password)\n",
    "\n",
    "    sftp_client = ssh_client.open_sftp()\n",
    "\n",
    "    if(remote_dump):\n",
    "        try:\n",
    "            sftp_client.chdir(pre_path)  # Test if remote_path exists\n",
    "        except IOError:\n",
    "            sftp_client.mkdir(pre_path)  # Create remote_path\n",
    "\n",
    "\n",
    "\n",
    "    pre_annotation_path = f\"{pre_path}{pre_annotation_path}\"\n",
    "    pre_annotation_path_mrc = f\"{pre_path}{pre_annotation_path_mrc}\"\n",
    "    current_pat_line_path = f\"{pre_path}{current_pat_line_path}\"\n",
    "    current_pat_lines_path = current_pat_line_path\n",
    "    \n",
    "    \n",
    "    if(remote_dump==False):\n",
    "        Path(current_pat_annot_path).mkdir(parents=True, exist_ok=True)\n",
    "        Path(pre_annotation_path_mrc).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            sftp_client.chdir(pre_annotation_path)  # Test if remote_path exists\n",
    "        except IOError:\n",
    "            sftp_client.mkdir(pre_annotation_path)  # Create remote_path\n",
    "\n",
    "        try:\n",
    "            sftp_client.chdir(pre_annotation_path_mrc)  # Test if remote_path exists\n",
    "        except IOError:\n",
    "            sftp_client.mkdir(pre_annotation_path_mrc)  # Create remote_path\n",
    "            \n",
    "        try:\n",
    "            sftp_client.chdir(current_pat_line_path)  # Test if remote_path exists\n",
    "        except IOError:\n",
    "            sftp_client.mkdir(current_pat_line_path)  # Create remote_path\n",
    "else:\n",
    "    sftp_client = None\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b377b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_dir_wrapper(path, sftp_obj=None):\n",
    "    #global sftp_client\n",
    "    if(remote_dump):\n",
    "        if(share_sftp == False):\n",
    "            ssh_client = paramiko.SSHClient()\n",
    "            ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "            ssh_client.connect(hostname=hostname, username=username, password=password)\n",
    "\n",
    "            sftp_client = ssh_client.open_sftp()\n",
    "            sftp_obj = sftp_client\n",
    "        elif(sftp_obj ==None):\n",
    "            sftp_obj = sftp_client\n",
    "            \n",
    "        res = sftp_obj.listdir(path)\n",
    "        \n",
    "        \n",
    "        return res\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        return os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77a033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_results(file_data, path, sftp_obj=None):\n",
    "    if(remote_dump):\n",
    "        if(share_sftp == False):\n",
    "            ssh_client = paramiko.SSHClient()\n",
    "            ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "            ssh_client.connect(hostname=hostname, username=username, password=password)\n",
    "\n",
    "            sftp_client = ssh_client.open_sftp()\n",
    "            sftp_obj = sftp_client\n",
    "        \n",
    "        \n",
    "        with sftp_obj.open(path, 'w') as file:\n",
    "    \n",
    "            pickle.dump(file_data, file)\n",
    "        if(share_sftp == False):\n",
    "            sftp_obj.close()\n",
    "            sftp_obj.close()\n",
    "        \n",
    "    else:\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(file_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b255b078",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = [x for x in range(1,13)]\n",
    "years = [x for x in range(1995, 2023)]\n",
    "import itertools\n",
    "combinations = list(itertools.product(years, months))\n",
    "len(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2929fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a19b710",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_options = {'demo':True,\n",
    "                'bmi':True,\n",
    "                'bloods':True,\n",
    "                'drugs':True,\n",
    "                'diagnostics':True,\n",
    "                \n",
    "                'core_02':True,\n",
    "                'bed':True,\n",
    "                'vte_status':True,\n",
    "                'hosp_site':True,\n",
    "                'core_resus':True,\n",
    "                'news':True,\n",
    "                \n",
    "                'annotations':True,\n",
    "                'annotations_mrc': True,\n",
    "                \n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eefdc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_demographics3(patlist, target_date_range):\n",
    "    #print(\"get demo3 non batch call --debug\")\n",
    "    \n",
    "    start_year, start_month, end_year, end_month = get_start_end_year_month(target_date_range)\n",
    "    \n",
    "    \n",
    "    demo = cohort_searcher_with_terms_and_search(index_name=\"epr_documents\", \n",
    "                                             fields_list=[\"client_idcode\", \"client_firstname\", \"client_lastname\", \"client_dob\", \"client_gendercode\", \"client_racecode\", \"client_deceaseddtm\", \"updatetime\"], \n",
    "                                             term_name=\"client_idcode.keyword\", \n",
    "                                             entered_list=patlist,\n",
    "                                            search_string= f'updatetime:[{start_year}-{start_month} TO {end_year}-{end_month}] '\n",
    "                                                )\n",
    "    demo[\"updatetime\"] = pd.to_datetime(demo[\"updatetime\"], utc=True)\n",
    "    demo = demo.sort_values([\"client_idcode\", \"updatetime\"]) #.drop_duplicates(subset = [\"client_idcode\"], keep = \"last\", inplace = True)\n",
    "    if(len(demo)> 1):\n",
    "        try:\n",
    "            \n",
    "            return demo.iloc[-1].to_frame()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    elif len(demo)==1:\n",
    "        return demo\n",
    "    \n",
    "    else:\n",
    "        demo = pd.DataFrame(data=None, columns=None)\n",
    "        demo['client_idcode'] = patlist\n",
    "        return demo\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eaf67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_end_year_month(target_date_range):\n",
    "    \n",
    "    start_year = target_date_range[0]\n",
    "    start_month = target_date_range[1]\n",
    "    end_year = start_year\n",
    "    \n",
    "\n",
    "    if(target_date_range[1] == 12):\n",
    "        end_year = start_year + 1\n",
    "        end_month = 1\n",
    "    else:\n",
    "        end_year = target_date_range[0]\n",
    "        end_month = start_month + 1\n",
    "        \n",
    "    if(start_month < 10):\n",
    "        start_month = '0'+str(start_month)\n",
    "        \n",
    "    if(end_month < 10):\n",
    "        end_month = '0'+str(end_month)   \n",
    "        \n",
    "    return start_year, start_month, end_year, end_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84709e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should one impute from the global mean\n",
    "# impute from local mean? \n",
    "# correct stratification. by year and month...?\n",
    "# by 6 months?\n",
    "# dynamic and user spec time strat?\n",
    "# encode month and year separately or every date/month\n",
    "# c. 240\n",
    "# with hfe annotation in vector, for positive can easily split by < >  date\n",
    "# instead filter datetime at search level, since we want to produce individual vectors\n",
    "# loop over vector production procedure for entry in date list for total 336 * npat = 336 * 20000 = 6,720,000 rows\n",
    "# for c. 7000 included variables, n cells = 7000 * 6,720,000 = 47,040,000,000\n",
    "# do we have updatetime for all data classes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea8b31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_empty_date_vector():\n",
    "    months = [x for x in range(1,13)]\n",
    "    years = [x for x in range(1995, 2023)]\n",
    "    combinations = list(itertools.product(years, months))\n",
    "    combinations = [str(item) + '_' + 'date_time_stamp' for item in combinations]\n",
    "    \n",
    "    \n",
    "    return pd.DataFrame(data=0.0, index=np.arange(1), columns = combinations).astype(float) #untested float cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a65982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_timestamp_to_tuple(timestamp):\n",
    "    # parse the timestamp string into a datetime object\n",
    "    dt = datetime.strptime(timestamp, '%Y-%m-%dT%H:%M:%S.%f%z')\n",
    "    \n",
    "    # extract the year and month from the datetime object\n",
    "    year = dt.year\n",
    "    month = dt.month\n",
    "    \n",
    "    # return the tuple of year and month\n",
    "    return (year, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5295b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enum_target_date_vector(target_date_range, current_pat_client_id_code):\n",
    "    \n",
    "    empty_date_vector = get_empty_date_vector()\n",
    "    \n",
    "    empty_date_vector.at[0,str(target_date_range)+\"_date_time_stamp\"] = 1\n",
    "    \n",
    "    empty_date_vector['client_idcode'] = current_pat_client_id_code\n",
    "    \n",
    "    return empty_date_vector\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14a3088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problems...:\n",
    "\n",
    "#post code field map code to region for feature\n",
    "#Religion code\n",
    "#Language code\n",
    "#occuptation code not specified\n",
    "#country of birth empty\n",
    "#marital status, populated but relevance?\n",
    "#title code, populated\n",
    "#address city populated\n",
    "#                                                           \n",
    "#OR Weight (kg)                                                                                                                 \n",
    "#OR \\\"Height in cm\\\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff2937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Good candidates:\n",
    "#City of birth many results\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a93885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"remote_dump {remote_dump}\")\n",
    "print(pre_annotation_path)\n",
    "print(pre_annotation_path_mrc)\n",
    "print(current_pat_line_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69940a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_pat_annotations_mrc_cs(current_pat_client_id_code, target_date_range, pat_batch, sftp_obj=None):\n",
    "    global start_time\n",
    "    \n",
    "#     current_annot_file_path = pre_annotation_path_mrc + current_pat_client_id_code+\"_\"+str(target_date_range)\n",
    "    \n",
    "    current_annot_file_path = pre_annotation_path_mrc  + current_pat_client_id_code + \"/\" + current_pat_client_id_code  +\"_\"+str(target_date_range)\n",
    "    \n",
    "    \n",
    "    file_exists = exist_check(current_annot_file_path, sftp_obj)\n",
    "    \n",
    "    if(file_exists==False):\n",
    "    \n",
    "        start_year, start_month, end_year, end_month = get_start_end_year_month(target_date_range)\n",
    "\n",
    "        \n",
    "        if(batch_mode):\n",
    "            current_pat_docs = filter_dataframe_by_timestamp(pat_batch, start_year, start_month, end_year, end_month, 'observationdocument_recordeddtm')\n",
    "\n",
    "        else:\n",
    "        \n",
    "            current_pat_docs = cohort_searcher_with_terms_and_search(index_name=\"observations\", \n",
    "                                                                       fields_list=\"\"\"observation_guid client_idcode\tobscatalogmasteritem_displayname\tobservation_valuetext_analysed\tobservationdocument_recordeddtm clientvisit_visitidcode\"\"\".split(),\n",
    "                                                                       term_name=\"client_idcode.keyword\", \n",
    "                                                                       entered_list=[current_pat_client_id_code], \n",
    "                                                                       search_string=\"obscatalogmasteritem_displayname:(\\\"AoMRC_ClinicalSummary_FT\\\") AND \" + f'observationdocument_recordeddtm:[{start_year}-{start_month} TO {end_year}-{end_month}]')\n",
    "\n",
    "\n",
    "        n_docs_to_annotate = len(current_pat_docs)\n",
    "        update_pbar(current_pat_client_id_code+\"_\"+str(target_date_range), start_time, 5, 'annotations_mrc', n_docs_to_annotate = n_docs_to_annotate)\n",
    "    else:\n",
    "        n_docs_to_annotate = \"Reading preannotated mrc...\"\n",
    "    annotation_map = {'True':1,\n",
    "                     'Presence':1 ,\n",
    "                     'Recent': 1,\n",
    "                     'Past':0,\n",
    "                     'Subject/Experiencer':1,\n",
    "                     'Other':0,\n",
    "                     'Hypothetical':0,\n",
    "                     'Patient': 1}\n",
    "\n",
    "    #remove filter from cdb?\n",
    "    #print(\"getting annotations\")\n",
    "    \n",
    "#     file_exists = exists(pre_annotation_path_mrc + current_pat_client_id_code)\n",
    "    \n",
    "    \n",
    "    if(file_exists==False):\n",
    "        with io.capture_output() as captured:\n",
    "            pats_anno_annotations = cat.get_entities_multi_texts(current_pat_docs['observation_valuetext_analysed'].dropna());#, n_process=1\n",
    "            \n",
    "            dump_results(pats_anno_annotations, current_annot_file_path, sftp_obj)\n",
    "                \n",
    "#                 with open(pre_annotation_path_mrc + current_pat_client_id_code, 'wb') as f:\n",
    "#                     pickle.dump(pats_anno_annotations, f)\n",
    "                    \n",
    "    else:\n",
    "        if(remote_dump):\n",
    "            if(share_sftp == False):\n",
    "                ssh_client = paramiko.SSHClient()\n",
    "                ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "                ssh_client.connect(hostname=hostname, username=username, password=password)\n",
    "\n",
    "                sftp_client = ssh_client.open_sftp()\n",
    "                sftp_obj = sftp_client\n",
    "            \n",
    "            \n",
    "            with sftp_obj.open(current_annot_file_path, 'r') as file:\n",
    "    \n",
    "                pats_anno_annotations = pickle.load(file)\n",
    "        \n",
    "            if(share_sftp == False):\n",
    "                sftp_obj.close()\n",
    "                sftp_obj.close()\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            with open(current_annot_file_path, 'rb') as f:\n",
    "                pats_anno_annotations = pickle.load(f)                \n",
    "                    \n",
    "    n_docs_to_annotate = len(pats_anno_annotations)\n",
    "    \n",
    "    \n",
    "    #print(f\"Annotated {current_pat_client_id_code}\")\n",
    "    #length of chars in documents summed\n",
    "    #average number of documents as a divisor for mention counts\n",
    "    #we want to keep the fact that lots of documents is a bad sign... \n",
    "    #Lots of mentions of something could indicate severity etc\n",
    "\n",
    "    #pats_anno_annotations = cat.get_entities(current_pat_docs['body_analysed'])\n",
    "    update_pbar(current_pat_client_id_code+\"_\"+str(target_date_range), start_time, 5, 'annotations_mrc', n_docs_to_annotate = n_docs_to_annotate)\n",
    "\n",
    "\n",
    "    sum_count = 0\n",
    "    for i in range(0, len(pats_anno_annotations)):\n",
    "        sum_count = sum_count + len(list(pats_anno_annotations[i]['entities'].keys()))\n",
    "\n",
    "    sum_count_index_list = [x for x in range(0, sum_count)]\n",
    "    all_doc_entities = {'entities': dict.fromkeys(sum_count_index_list, {})}\n",
    "    sum_count_index = 0\n",
    "    for i in range(0, len(pats_anno_annotations)):\n",
    "\n",
    "        key_list = list(pats_anno_annotations[i]['entities'].keys())\n",
    "        for j in range(0, len(key_list)):\n",
    "\n",
    "            all_doc_entities['entities'][sum_count_index] = pats_anno_annotations[i]['entities'].get(key_list[j])\n",
    "            sum_count_index = sum_count_index + 1\n",
    "\n",
    "\n",
    "    pats_anno_annotations = all_doc_entities\n",
    "\n",
    "    all_cui_list = []\n",
    "\n",
    "    all_meta_anno = False\n",
    "    confidence_threshold_presence = 0.8\n",
    "    confidence_threshold_subject = 0.8\n",
    "    confidence_threshold_concept_accuracy = 0.8\n",
    "\n",
    "    cui_list_pretty_names = []\n",
    "    doc_keys = list(pats_anno_annotations.keys())\n",
    "    for i in range(0, len(doc_keys)):\n",
    "        current_pats_entry = pats_anno_annotations.get('entities')\n",
    "        current_pats_entry_keys = list(pats_anno_annotations.get('entities').keys())\n",
    "        for j in range(0, len(current_pats_entry_keys)):\n",
    "            all_cui_list.append(current_pats_entry.get(current_pats_entry_keys[j])['cui'])\n",
    "            cui_list_pretty_names.append(current_pats_entry.get(current_pats_entry_keys[j])['pretty_name'])\n",
    "    #print(\"len(all_cui_list)\", len(all_cui_list))      \n",
    "    #print(\"len(set(all_cui_list))\", len(set(all_cui_list)))\n",
    "\n",
    "    cui_list = all_cui_list\n",
    "\n",
    "#     cui_list_pretty_names = []\n",
    "#     for i in range(0, len(cui_list)):\n",
    "#         cui_list_pretty_names.append(cat.cdb.cui2preferred_name.get(cui_list[i]))\n",
    "\n",
    "    #print(\"len(set(cui_list_pretty_names))\", len(set(cui_list_pretty_names)))\n",
    "\n",
    "    cui_list_pretty_names = list(set(cui_list_pretty_names))\n",
    "\n",
    "    #cui_list_pretty_names.remove(None)\n",
    "\n",
    "\n",
    "\n",
    "    cui_list_pretty_names_meta_list = []\n",
    "    if(all_meta_anno):\n",
    "        cui_list_pretty_names_meta = [x + '_meta' for x in cui_list_pretty_names]\n",
    "\n",
    "        cui_list_pretty_names_meta_list = []\n",
    "\n",
    "        meta_key_list = ['Time', 'Presence', 'Subject/Experiencer']\n",
    "\n",
    "        meta_sub_key_list = ['value'] #,'confidence', 'name']\n",
    "\n",
    "        for i in range(0, len(cui_list_pretty_names)):\n",
    "            for j in range(0, len(meta_key_list)):\n",
    "                for k in range(0, len(meta_sub_key_list)):\n",
    "                    cui_list_pretty_names_meta_list.append(cui_list_pretty_names[i] + \"_\"+meta_key_list[j]+\"_\"+meta_sub_key_list[k])\n",
    "\n",
    "        print(\"len(set(cui_list_pretty_names_meta_list))\", len(set(cui_list_pretty_names_meta_list)))    \n",
    "\n",
    "    cui_list_pretty_names_count_list = []\n",
    "    for i in range(0, len(cui_list_pretty_names)):\n",
    "            cui_list_pretty_names_count_list.append(cui_list_pretty_names[i] +\"_count_mrc_cs\")\n",
    "            cui_list_pretty_names_count_list.append(cui_list_pretty_names[i] +\"_count_subject_present_mrc_cs\")\n",
    "            \n",
    "            if(negated_presence_annotations):\n",
    "                cui_list_pretty_names_count_list.append(cui_list_pretty_names[i] +\"_count_subject_not_present_mrc_cs\")\n",
    "                cui_list_pretty_names_count_list.append(cui_list_pretty_names[i] +\"_count_relative_not_present_mrc_cs\")\n",
    "\n",
    "\n",
    "    all_columns_to_append =   cui_list_pretty_names_count_list\n",
    "    if(all_meta_anno):\n",
    "        all_columns_to_append.append(cui_list_pretty_names_meta_list)\n",
    "\n",
    "    dummy_data = np.empty((1,len(all_columns_to_append)));\n",
    "    dummy_data[:] = np.nan\n",
    "    df_pat_entry = pd.DataFrame(data = dummy_data,columns=all_columns_to_append)\n",
    "\n",
    "    #df = pd.read_csv(file_name) #call outside\n",
    "    df_pat = df_pat_entry.copy() \n",
    "    df_pat['client_idcode'] = current_pat_client_id_code \n",
    "    df_pat['n_docs'] = n_docs_to_annotate\n",
    "\n",
    "    df_pat.reset_index(inplace=True)\n",
    "    #df_pat['n'] = [i for i in range(0, len(df_pat))]\n",
    "    df_pat = df_pat[['client_idcode']].copy()\n",
    "    df_pat_target = df_pat.copy(deep=True)\n",
    "    #print(\"Reindexing df_pat_target\")\n",
    "    df_pat_target = df_pat_target.reindex(list(df_pat_target.columns) + all_columns_to_append, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    a = list(df_pat_target.columns)\n",
    "    b = cui_list_pretty_names_meta_list\n",
    "\n",
    "\n",
    "\n",
    "    #print(\"filling df pat target with nans\")\n",
    "    df_pat_target[[x for x in a if (x not in b)]] = df_pat_target[[x for x in a if (x not in b)]].fillna(0)\n",
    "\n",
    "    #break\n",
    "    df_pat_target['client_idcode'].iloc[0] = current_pat_client_id_code\n",
    "\n",
    "    df_pat_target = df_pat_target.copy()\n",
    "\n",
    "    list_targ=[x for x in range(0, len(df_pat_target))]\n",
    "\n",
    "    columns = list(cui_list_pretty_names_meta_list)\n",
    "\n",
    "\n",
    "    df_pat_target = df_pat_target.copy()#[0:1]\n",
    "\n",
    "\n",
    "\n",
    "    entry_counter = 0\n",
    "    meta_counter = 0\n",
    "    i = 0\n",
    "    #print(\"Starting annotation frame builder...\")\n",
    "    #for i in tqdm(range(0, len(df_pat_target))):\n",
    "        #ci = df_pat_target['client_idcode'].iloc[i]\n",
    "\n",
    "    #annotations = cat.get_entities(current_pat_docs['body_analysed']) #docs.get(ci)\n",
    "    annotations = pats_anno_annotations\n",
    "\n",
    "\n",
    "    if annotations is not None:\n",
    "        annotation_keys = list(annotations['entities'].keys())\n",
    "\n",
    "        for j in range(0, len(annotation_keys)):\n",
    "\n",
    "            cui = annotations['entities'][annotation_keys[j]].get(\"cui\")\n",
    "            if(cui in cui_list): \n",
    "                current_col_name = annotations['entities'][annotation_keys[j]].get('pretty_name')\n",
    "                current_col_meta = annotations['entities'][annotation_keys[j]].get('meta_anns')\n",
    "\n",
    "                df_pat_target.at[i, current_col_name+\"_count_mrc_cs\"] = df_pat_target.loc[i][current_col_name+\"_count_mrc_cs\"] + 1\n",
    "\n",
    "                if(current_col_meta is not None):\n",
    "                    \n",
    "                    if(current_col_meta['Presence']['value']=='True' and \n",
    "                       current_col_meta['Subject/Experiencer']['value']=='Patient' and \n",
    "                       current_col_meta['Presence']['confidence']> confidence_threshold_presence and\n",
    "                       current_col_meta['Subject/Experiencer']['confidence']> confidence_threshold_presence and\n",
    "                       annotations['entities'][annotation_keys[j]]['acc'] >confidence_threshold_concept_accuracy):\n",
    "\n",
    "                            df_pat_target.at[i, current_col_name+\"_count_subject_present_mrc_cs\"] = df_pat_target.loc[i][current_col_name+\"_count_subject_present_mrc_cs\"] + 1\n",
    "                    \n",
    "                    elif(current_col_meta['Presence']['value']=='True' and \n",
    "                       current_col_meta['Subject/Experiencer']['value']=='Relative' and \n",
    "                       current_col_meta['Presence']['confidence']> confidence_threshold_presence and\n",
    "                       current_col_meta['Subject/Experiencer']['confidence']> confidence_threshold_presence and\n",
    "                       annotations['entities'][annotation_keys[j]]['acc'] >confidence_threshold_concept_accuracy):\n",
    "                            if(current_col_name+\"_count_relative_present_mrc_cs\" in df_pat_target.columns):\n",
    "                                \n",
    "                                df_pat_target.at[i, current_col_name+\"_count_relative_present_mrc_cs\"] = df_pat_target.loc[i][current_col_name+\"_count_relative_present_mrc_cs\"] + 1\n",
    "                    \n",
    "                            else:\n",
    "                                df_pat_target[current_col_name+\"_count_relative_present_mrc_cs\"] = 1\n",
    "                                \n",
    "                    if(negated_presence_annotations):\n",
    "                        \n",
    "                        if(current_col_meta is not None):\n",
    "                    \n",
    "                            if(current_col_meta['Presence']['value']=='False' and \n",
    "                               current_col_meta['Subject/Experiencer']['value']=='Patient' and \n",
    "                               current_col_meta['Presence']['confidence']> confidence_threshold_presence and\n",
    "                               current_col_meta['Subject/Experiencer']['confidence']> confidence_threshold_presence and\n",
    "                               annotations['entities'][annotation_keys[j]]['acc'] >confidence_threshold_concept_accuracy):\n",
    "\n",
    "                                    df_pat_target.at[i, current_col_name+\"_count_subject_not_present_mrc_cs\"] = df_pat_target.loc[i][current_col_name+\"_count_subject_not_present_mrc_cs\"] + 1\n",
    "\n",
    "\n",
    "                            elif(current_col_meta['Presence']['value']=='True' and \n",
    "                               current_col_meta['Subject/Experiencer']['value']=='Relative' and \n",
    "                               current_col_meta['Presence']['confidence']> confidence_threshold_presence and\n",
    "                               current_col_meta['Subject/Experiencer']['confidence']> confidence_threshold_presence and\n",
    "                               annotations['entities'][annotation_keys[j]]['acc'] >confidence_threshold_concept_accuracy):\n",
    "                                    if(current_col_name+\"_count_relative_not_present_mrc_cs\" in df_pat_target.columns):\n",
    "\n",
    "                                        df_pat_target.at[i, current_col_name+\"_count_relative_not_present_mrc_cs\"] = df_pat_target.loc[i][current_col_name+\"_count_relative_not_present_mrc_cs\"] + 1\n",
    "\n",
    "                                    else:\n",
    "                                        df_pat_target[current_col_name+\"_count_relative_not_present_mrc_cs\"] = 1\n",
    "                                \n",
    "                                \n",
    "                    else:\n",
    "                        #OLD: set to nan instead of zero for impute and medcat precision persistence\n",
    "                        #Dont set anything here as this will overwrite existing 1 entries?\n",
    "                        pass\n",
    "                        #df_pat_target.at[i, current_col_name+\"_count_subject_present\"] = np.nan\n",
    "\n",
    "\n",
    "                if(all_meta_anno):\n",
    "                    if(len(list(current_col_meta.keys()))>0):\n",
    "\n",
    "                        for key in current_col_meta.keys():\n",
    "\n",
    "                            sub_anot = current_col_meta.get(key)\n",
    "                            if(len(list(sub_anot.keys()))>0):\n",
    "                                for sub_key in sub_anot.keys():\n",
    "                                    if(sub_key in meta_sub_key_list):\n",
    "                                        try:\n",
    "\n",
    "                                            key_result = sub_anot.get(sub_key)\n",
    "                                            if(type(key_result) is str):\n",
    "                                                key_result = annotation_map.get(key_result)\n",
    "\n",
    "                                            #print(current_col_name+'_'+str(key)+'_'+str(sub_key), key_result, sub_anot.get(sub_key))\n",
    "\n",
    "                                            df_pat_target.at[i, current_col_name+'_'+str(key)+'_'+str(sub_key)] = key_result\n",
    "\n",
    "\n",
    "                                            meta_counter = meta_counter+1\n",
    "                                        except Exception as e:\n",
    "                                            print(e)\n",
    "                                            pass\n",
    "\n",
    "\n",
    "\n",
    "                meta_counter  = meta_counter  + 1\n",
    "                entry_counter = entry_counter + 1\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "    update_pbar(current_pat_client_id_code+\"_\"+str(target_date_range), start_time, 5, 'annotations_mrc', n_docs_to_annotate = n_docs_to_annotate)\n",
    "    #df_pat_target.drop(\"n\", axis=1, inplace=True)\n",
    "\n",
    "    #df_pat_target.to_csv(entry_file_name)\n",
    "    #print(f\"Made {entry_counter} entry_counter  entries\")\n",
    "    #print(f\"Made {meta_counter} meta_counter entries\")\n",
    "            #print(\"done\")\n",
    "    return df_pat_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e842a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_pat_annotations(current_pat_client_id_code, target_date_range, pat_batch, sftp_obj=None):\n",
    "    global start_time\n",
    "    \n",
    "#     current_annotation_file_path = pre_annotation_path + current_pat_client_id_code+\"_\"+str(target_date_range)\n",
    "    \n",
    "    current_annotation_file_path = pre_annotation_path + current_pat_client_id_code + \"/\" +  current_pat_client_id_code+\"_\"+str(target_date_range)\n",
    "    \n",
    "    \n",
    "    file_exists = exist_check(current_annotation_file_path, sftp_obj)\n",
    "    \n",
    "    if(file_exists == False):\n",
    "    \n",
    "        start_year, start_month, end_year, end_month = get_start_end_year_month(target_date_range)\n",
    "        \n",
    "        if(batch_mode):\n",
    "            current_pat_docs = filter_dataframe_by_timestamp(pat_batch, start_year, start_month, end_year, end_month, 'updatetime')\n",
    "\n",
    "        else:\n",
    "\n",
    "            current_pat_docs = cohort_searcher_with_terms_and_search(index_name=\"epr_documents\", \n",
    "                                                                  fields_list = \"\"\"client_idcode document_guid\tdocument_description\tbody_analysed updatetime clientvisit_visitidcode\"\"\".split(),\n",
    "                                                                  term_name = \"client_idcode.keyword\", \n",
    "                                                                  entered_list = [current_pat_client_id_code],\n",
    "                                                                search_string = f'updatetime:[{start_year}-{start_month} TO {end_year}-{end_month}] ')\n",
    "\n",
    "\n",
    "        n_docs_to_annotate = len(current_pat_docs)\n",
    "        update_pbar(current_pat_client_id_code+\"_\"+str(target_date_range), start_time, 5, 'annotations', n_docs_to_annotate = n_docs_to_annotate)\n",
    "\n",
    "    else:\n",
    "        n_docs_to_annotate = \"Reading preannotated...\"\n",
    "        \n",
    "        \n",
    "    annotation_map = {'True':1,\n",
    "                     'Presence':1 ,\n",
    "                     'Recent': 1,\n",
    "                     'Past':0,\n",
    "                     'Subject/Experiencer':1,\n",
    "                     'Other':0,\n",
    "                     'Hypothetical':0,\n",
    "                     'Patient': 1}\n",
    "\n",
    "    #remove filter from cdb?\n",
    "    #print(\"getting annotations\")\n",
    "    \n",
    "#     file_exists = exists(pre_annotation_path + current_pat_client_id_code)\n",
    "    \n",
    "    \n",
    "    if(file_exists==False):\n",
    "        with io.capture_output() as captured:\n",
    "            pats_anno_annotations = cat.get_entities_multi_texts(current_pat_docs['body_analysed'].dropna());#, n_process=1\n",
    "        if(store_annot):\n",
    "            dump_results(pats_anno_annotations, current_annotation_file_path, sftp_obj)\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        if(remote_dump):\n",
    "            if(share_sftp == False):\n",
    "                ssh_client = paramiko.SSHClient()\n",
    "                ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "                ssh_client.connect(hostname=hostname, username=username, password=password)\n",
    "\n",
    "                sftp_client = ssh_client.open_sftp()\n",
    "                sftp_obj = sftp_client\n",
    "            \n",
    "\n",
    "            with sftp_obj.open(current_annotation_file_path, 'r') as file:\n",
    "\n",
    "                pats_anno_annotations = pickle.load(file)\n",
    "            \n",
    "            if(share_sftp == False):\n",
    "                sftp_obj.close()\n",
    "                sftp_obj.close()\n",
    "            \n",
    "        else:\n",
    "\n",
    "            with open(current_annotation_file_path, 'rb') as f:\n",
    "                pats_anno_annotations = pickle.load(f)\n",
    "   \n",
    "    n_docs_to_annotate = len(pats_anno_annotations)\n",
    "    \n",
    "    #print(f\"Annotated {current_pat_client_id_code}\")\n",
    "    #length of chars in documents summed\n",
    "    #average number of documents as a divisor for mention counts\n",
    "    #we want to keep the fact that lots of documents is a bad sign... \n",
    "    #Lots of mentions of something could indicate severity etc\n",
    "\n",
    "    #pats_anno_annotations = cat.get_entities(current_pat_docs['body_analysed'])\n",
    "    update_pbar(current_pat_client_id_code+\"_\"+str(target_date_range), start_time, 5, 'annotations', n_docs_to_annotate = n_docs_to_annotate)\n",
    "\n",
    "\n",
    "    sum_count = 0\n",
    "    for i in range(0, len(pats_anno_annotations)):\n",
    "        sum_count = sum_count + len(list(pats_anno_annotations[i]['entities'].keys()))\n",
    "\n",
    "    sum_count_index_list = [x for x in range(0, sum_count)]\n",
    "    all_doc_entities = {'entities': dict.fromkeys(sum_count_index_list, {})}\n",
    "    sum_count_index = 0\n",
    "    for i in range(0, len(pats_anno_annotations)):\n",
    "\n",
    "        key_list = list(pats_anno_annotations[i]['entities'].keys())\n",
    "        for j in range(0, len(key_list)):\n",
    "\n",
    "            all_doc_entities['entities'][sum_count_index] = pats_anno_annotations[i]['entities'].get(key_list[j])\n",
    "            sum_count_index = sum_count_index + 1\n",
    "\n",
    "\n",
    "    pats_anno_annotations = all_doc_entities\n",
    "\n",
    "    all_cui_list = []\n",
    "\n",
    "    all_meta_anno = False\n",
    "    confidence_threshold_presence = 0.8\n",
    "    confidence_threshold_subject = 0.8\n",
    "    confidence_threshold_concept_accuracy = 0.8\n",
    "\n",
    "    cui_list_pretty_names = []\n",
    "    doc_keys = list(pats_anno_annotations.keys())\n",
    "    for i in range(0, len(doc_keys)):\n",
    "        current_pats_entry = pats_anno_annotations.get('entities')\n",
    "        current_pats_entry_keys = list(pats_anno_annotations.get('entities').keys())\n",
    "        for j in range(0, len(current_pats_entry_keys)):\n",
    "            all_cui_list.append(current_pats_entry.get(current_pats_entry_keys[j])['cui'])\n",
    "            cui_list_pretty_names.append(current_pats_entry.get(current_pats_entry_keys[j])['pretty_name'])\n",
    "    #print(\"len(all_cui_list)\", len(all_cui_list))      \n",
    "    #print(\"len(set(all_cui_list))\", len(set(all_cui_list)))\n",
    "\n",
    "    cui_list = all_cui_list\n",
    "\n",
    "#     cui_list_pretty_names = []\n",
    "#     for i in range(0, len(cui_list)):\n",
    "#         cui_list_pretty_names.append(cat.cdb.cui2preferred_name.get(cui_list[i]))\n",
    "\n",
    "    #print(\"len(set(cui_list_pretty_names))\", len(set(cui_list_pretty_names)))\n",
    "\n",
    "    cui_list_pretty_names = list(set(cui_list_pretty_names))\n",
    "\n",
    "    #cui_list_pretty_names.remove(None)\n",
    "\n",
    "\n",
    "\n",
    "    cui_list_pretty_names_meta_list = []\n",
    "    if(all_meta_anno):\n",
    "        cui_list_pretty_names_meta = [x + '_meta' for x in cui_list_pretty_names]\n",
    "\n",
    "        cui_list_pretty_names_meta_list = []\n",
    "\n",
    "        meta_key_list = ['Time', 'Presence', 'Subject/Experiencer']\n",
    "\n",
    "        meta_sub_key_list = ['value'] #,'confidence', 'name']\n",
    "\n",
    "        for i in range(0, len(cui_list_pretty_names)):\n",
    "            for j in range(0, len(meta_key_list)):\n",
    "                for k in range(0, len(meta_sub_key_list)):\n",
    "                    cui_list_pretty_names_meta_list.append(cui_list_pretty_names[i] + \"_\"+meta_key_list[j]+\"_\"+meta_sub_key_list[k])\n",
    "\n",
    "        print(\"len(set(cui_list_pretty_names_meta_list))\", len(set(cui_list_pretty_names_meta_list)))    \n",
    "\n",
    "    cui_list_pretty_names_count_list = []\n",
    "    for i in range(0, len(cui_list_pretty_names)):\n",
    "            cui_list_pretty_names_count_list.append(cui_list_pretty_names[i] +\"_count\")\n",
    "            cui_list_pretty_names_count_list.append(cui_list_pretty_names[i] +\"_count_subject_present\")\n",
    "            \n",
    "            if(negated_presence_annotations):\n",
    "                cui_list_pretty_names_count_list.append(cui_list_pretty_names[i] +\"_count_subject_not_present\")\n",
    "                cui_list_pretty_names_count_list.append(cui_list_pretty_names[i] +\"_count_relative_not_present\")\n",
    "\n",
    "\n",
    "    all_columns_to_append =   cui_list_pretty_names_count_list\n",
    "    if(all_meta_anno):\n",
    "        all_columns_to_append.append(cui_list_pretty_names_meta_list)\n",
    "\n",
    "    dummy_data = np.empty((1,len(all_columns_to_append)));\n",
    "    dummy_data[:] = np.nan\n",
    "    df_pat_entry = pd.DataFrame(data = dummy_data,columns=all_columns_to_append)\n",
    "\n",
    "    #df = pd.read_csv(file_name) #call outside\n",
    "    df_pat = df_pat_entry.copy() \n",
    "    df_pat['client_idcode'] = current_pat_client_id_code \n",
    "    df_pat['n_docs'] = n_docs_to_annotate\n",
    "\n",
    "    df_pat.reset_index(inplace=True)\n",
    "    #df_pat['n'] = [i for i in range(0, len(df_pat))]\n",
    "    df_pat = df_pat[['client_idcode']].copy()\n",
    "    df_pat_target = df_pat.copy(deep=True)\n",
    "    #print(\"Reindexing df_pat_target\")\n",
    "    df_pat_target = df_pat_target.reindex(list(df_pat_target.columns) +all_columns_to_append, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    a = list(df_pat_target.columns)\n",
    "    b = cui_list_pretty_names_meta_list\n",
    "\n",
    "\n",
    "\n",
    "    #print(\"filling df pat target with nans\")\n",
    "    df_pat_target[[x for x in a if (x not in b)]] = df_pat_target[[x for x in a if (x not in b)]].fillna(0)\n",
    "\n",
    "    #break\n",
    "    df_pat_target['client_idcode'].iloc[0] = current_pat_client_id_code\n",
    "\n",
    "    df_pat_target = df_pat_target.copy()\n",
    "\n",
    "    list_targ=[x for x in range(0, len(df_pat_target))]\n",
    "\n",
    "    columns = list(cui_list_pretty_names_meta_list)\n",
    "\n",
    "\n",
    "    df_pat_target = df_pat_target.copy()#[0:1]\n",
    "\n",
    "\n",
    "\n",
    "    entry_counter = 0\n",
    "    meta_counter = 0\n",
    "    i = 0\n",
    "    #print(\"Starting annotation frame builder...\")\n",
    "    #for i in tqdm(range(0, len(df_pat_target))):\n",
    "        #ci = df_pat_target['client_idcode'].iloc[i]\n",
    "\n",
    "    #annotations = cat.get_entities(current_pat_docs['body_analysed']) #docs.get(ci)\n",
    "    annotations = pats_anno_annotations\n",
    "\n",
    "\n",
    "    if annotations is not None:\n",
    "        annotation_keys = list(annotations['entities'].keys())\n",
    "\n",
    "        for j in range(0, len(annotation_keys)):\n",
    "\n",
    "            cui = annotations['entities'][annotation_keys[j]].get(\"cui\")\n",
    "            if(cui in cui_list): \n",
    "                current_col_name = annotations['entities'][annotation_keys[j]].get('pretty_name')\n",
    "                current_col_meta = annotations['entities'][annotation_keys[j]].get('meta_anns')\n",
    "\n",
    "                df_pat_target.at[i, current_col_name+\"_count\"] = df_pat_target.loc[i][current_col_name+\"_count\"] + 1\n",
    "\n",
    "                if(current_col_meta is not None):\n",
    "                    \n",
    "                    if(current_col_meta['Presence']['value']=='True' and \n",
    "                       current_col_meta['Subject/Experiencer']['value']=='Patient' and \n",
    "                       current_col_meta['Presence']['confidence']> confidence_threshold_presence and\n",
    "                       current_col_meta['Subject/Experiencer']['confidence']> confidence_threshold_presence and\n",
    "                       annotations['entities'][annotation_keys[j]]['acc'] >confidence_threshold_concept_accuracy):\n",
    "                     \n",
    "                            df_pat_target.at[i, current_col_name+\"_count_subject_present\"] = df_pat_target.loc[i][current_col_name+\"_count_subject_present\"] + 1\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                \n",
    "                \n",
    "                \n",
    "                    elif(current_col_meta['Presence']['value']=='True' and \n",
    "                       current_col_meta['Subject/Experiencer']['value']=='Relative' and \n",
    "                       current_col_meta['Presence']['confidence']> confidence_threshold_presence and\n",
    "                       current_col_meta['Subject/Experiencer']['confidence']> confidence_threshold_presence and\n",
    "                       annotations['entities'][annotation_keys[j]]['acc'] >confidence_threshold_concept_accuracy):\n",
    "                            if(current_col_name+\"_count_relative_present\" in df_pat_target.columns):\n",
    "                                \n",
    "                                df_pat_target.at[i, current_col_name+\"_count_relative_present\"] = df_pat_target.loc[i][current_col_name+\"_count_relative_present\"] + 1\n",
    "                    \n",
    "                            else:\n",
    "                                df_pat_target[current_col_name+\"_count_relative_present\"] = 1\n",
    "                                \n",
    "                    if(negated_presence_annotations):\n",
    "                        \n",
    "                        if(current_col_meta is not None):\n",
    "                    \n",
    "                            if(current_col_meta['Presence']['value']=='False' and \n",
    "                               current_col_meta['Subject/Experiencer']['value']=='Patient' and \n",
    "                               current_col_meta['Presence']['confidence']> confidence_threshold_presence and\n",
    "                               current_col_meta['Subject/Experiencer']['confidence']> confidence_threshold_presence and\n",
    "                               annotations['entities'][annotation_keys[j]]['acc'] >confidence_threshold_concept_accuracy):\n",
    "\n",
    "                                    df_pat_target.at[i, current_col_name+\"_count_subject_not_present\"] = df_pat_target.loc[i][current_col_name+\"_count_subject_not_present\"] + 1\n",
    "\n",
    "\n",
    "                            elif(current_col_meta['Presence']['value']=='True' and \n",
    "                               current_col_meta['Subject/Experiencer']['value']=='Relative' and \n",
    "                               current_col_meta['Presence']['confidence']> confidence_threshold_presence and\n",
    "                               current_col_meta['Subject/Experiencer']['confidence']> confidence_threshold_presence and\n",
    "                               annotations['entities'][annotation_keys[j]]['acc'] >confidence_threshold_concept_accuracy):\n",
    "                                    if(current_col_name+\"_count_relative_not_present\" in df_pat_target.columns):\n",
    "\n",
    "                                        df_pat_target.at[i, current_col_name+\"_count_relative_not_present\"] = df_pat_target.loc[i][current_col_name+\"_count_relative_not_present\"] + 1\n",
    "\n",
    "                                    else:\n",
    "                                        df_pat_target[current_col_name+\"_count_relative_not_present\"] = 1\n",
    "                        \n",
    "                                \n",
    "                                \n",
    "                    else:\n",
    "                        #OLD: set to nan instead of zero for impute and medcat precision persistence\n",
    "                        #Dont set anything here as this will overwrite existing 1 entries?\n",
    "                        pass\n",
    "                        #df_pat_target.at[i, current_col_name+\"_count_subject_present\"] = np.nan\n",
    "\n",
    "\n",
    "                if(all_meta_anno):\n",
    "                    if(len(list(current_col_meta.keys()))>0):\n",
    "\n",
    "                        for key in current_col_meta.keys():\n",
    "\n",
    "                            sub_anot = current_col_meta.get(key)\n",
    "                            if(len(list(sub_anot.keys()))>0):\n",
    "                                for sub_key in sub_anot.keys():\n",
    "                                    if(sub_key in meta_sub_key_list):\n",
    "                                        try:\n",
    "\n",
    "                                            key_result = sub_anot.get(sub_key)\n",
    "                                            if(type(key_result) is str):\n",
    "                                                key_result = annotation_map.get(key_result)\n",
    "\n",
    "                                            #print(current_col_name+'_'+str(key)+'_'+str(sub_key), key_result, sub_anot.get(sub_key))\n",
    "\n",
    "                                            df_pat_target.at[i, current_col_name+'_'+str(key)+'_'+str(sub_key)] = key_result\n",
    "\n",
    "\n",
    "                                            meta_counter = meta_counter+1\n",
    "                                        except Exception as e:\n",
    "                                            print(e)\n",
    "                                            pass\n",
    "\n",
    "\n",
    "\n",
    "                meta_counter  = meta_counter  + 1\n",
    "                entry_counter = entry_counter + 1\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "    update_pbar(current_pat_client_id_code+\"_\"+str(target_date_range), start_time, 5, 'annotations', n_docs_to_annotate = n_docs_to_annotate)\n",
    "    #df_pat_target.drop(\"n\", axis=1, inplace=True)\n",
    "\n",
    "    #df_pat_target.to_csv(entry_file_name)\n",
    "    #print(f\"Made {entry_counter} entry_counter  entries\")\n",
    "    #print(f\"Made {meta_counter} meta_counter entries\")\n",
    "            #print(\"done\")\n",
    "    return df_pat_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48a31b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pbar(current_pat_client_id_code, start_time, stage_int, stage_str, **n_docs_to_annotate):\n",
    "    global colour_val\n",
    "    global t\n",
    "    global skipped_counter\n",
    "    \n",
    "    #global skipped_counter\n",
    "    #colour_val = color_bars[stage_int] + stage_str\n",
    "    colour_val = Fore.GREEN +  Style.BRIGHT + stage_str\n",
    "    \n",
    "    if(multi_process):\n",
    "        \n",
    "        counter_disp = skipped_counter.value\n",
    "    \n",
    "    else:\n",
    "        counter_disp = skipped_counter\n",
    "    \n",
    "    t.set_description(f\"s: {counter_disp} | {current_pat_client_id_code} | task: {colour_val} | {n_docs_to_annotate}\" )\n",
    "    if((time.time() - start_time)>slow_execution_threshold_low):\n",
    "        t.colour = Fore.YELLOW\n",
    "        colour_val = Fore.YELLOW + stage_str\n",
    "        t.set_description(f\"s: {counter_disp} | {current_pat_client_id_code} | task: {colour_val} | {n_docs_to_annotate}\" )\n",
    "        \n",
    "    elif((time.time() - start_time)>slow_execution_threshold_high):\n",
    "        t.colour = Fore.RED +  Style.BRIGHT\n",
    "        colour_val = Fore.RED +  Style.BRIGHT + stage_str\n",
    "        t.set_description(f\"s: {counter_disp} | {current_pat_client_id_code} | task: {colour_val} | {n_docs_to_annotate}\" )\n",
    "        \n",
    "    elif((time.time() - start_time)>slow_execution_threshold_extreme):\n",
    "        t.colour = Fore.RED +  Style.DIM\n",
    "        colour_val = Fore.RED +  Style.DIM + stage_str\n",
    "        t.set_description(f\"s: {counter_disp} | {current_pat_client_id_code} | task: {colour_val} | {n_docs_to_annotate}\" )\n",
    "        \n",
    "    else:\n",
    "        t.colour =  Fore.GREEN +  Style.DIM\n",
    "        colour_val = Fore.GREEN +  Style.DIM + stage_str\n",
    "        t.set_description(f\"s: {counter_disp} | {current_pat_client_id_code} | task: {colour_val} | {n_docs_to_annotate}\" )\n",
    "        \n",
    "        \n",
    "\n",
    "    t.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2e7675",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_pat_lines_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b58ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_list_start = [x.replace(\".csv\",\"\") for x in list_dir_wrapper(current_pat_lines_path, sftp_client)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875dcfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stripped_list_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0fcff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_list = [x.replace(\".csv\",\"\") for x in list_dir_wrapper(current_pat_lines_path, sftp_client)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f81446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30bb13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_pat_annotations_to_file(current_pat_client_id_code, target_date_range):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    file_exists = exist_check(pre_annotation_path + current_pat_client_id_code+\"_\"+str(target_date_range))\n",
    "    \n",
    "    if(file_exists == False):\n",
    "    \n",
    "        start_year, start_month, end_year, end_month = get_start_end_year_month(target_date_range)\n",
    "\n",
    "        current_pat_docs = cohort_searcher_with_terms_and_search(index_name=\"epr_documents\", \n",
    "                                                                  fields_list = \"\"\"client_idcode document_guid\tdocument_description\tbody_analysed updatetime clientvisit_visitidcode\"\"\".split(),\n",
    "                                                                  term_name = \"client_idcode.keyword\", \n",
    "                                                                  entered_list = [current_pat_client_id_code],\n",
    "                                                                search_string = f'updatetime:[{start_year}-{start_month} TO {end_year}-{end_month}] ')\n",
    "\n",
    "\n",
    "\n",
    "        n_docs_to_annotate = len(current_pat_docs)\n",
    "        update_pbar(current_pat_client_id_code+\"_\"+str(target_date_range), start_time, 5, 'annotations', n_docs_to_annotate = n_docs_to_annotate)\n",
    "\n",
    "    annotation_map = {'True':1,\n",
    "                     'Presence':1 ,\n",
    "                     'Recent': 1,\n",
    "                     'Past':0,\n",
    "                     'Subject/Experiencer':1,\n",
    "                     'Other':0,\n",
    "                     'Hypothetical':0,\n",
    "                     'Patient': 1}\n",
    "\n",
    "    #remove filter from cdb?\n",
    "    #print(\"getting annotations\")\n",
    "    \n",
    "#     file_exists = exists(pre_annotation_path + current_pat_client_id_code)\n",
    "    \n",
    "    \n",
    "    if(file_exists==False):\n",
    "        with io.capture_output() as captured:\n",
    "            pats_anno_annotations = cat.get_entities_multi_texts(current_pat_docs['body_analysed'].dropna());#, n_process=1\n",
    "        if(store_annot):\n",
    "            dump_results(pats_anno_annotations, pre_annotation_path + current_pat_client_id_code+\"_\"+str(target_date_range))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f72cd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataframe_by_timestamp(df, start_year, start_month, end_year, end_month, timestamp_string):\n",
    "        # Convert timestamp column to datetime format\n",
    "    df[timestamp_string] = pd.to_datetime(df[timestamp_string], utc=True)\n",
    "\n",
    "\n",
    "    #imputed from elastic. Mirror:\n",
    "    start_day = 1\n",
    "    end_day = 1\n",
    "    hour = 23\n",
    "    minute = 59\n",
    "    second = 59\n",
    "\n",
    "\n",
    "    # Filter based on year and month ranges\n",
    "    filtered_df = df[(df[timestamp_string] >= str(datetime(start_year, int(start_month), start_day, hour, minute, second))) & \n",
    "                     (df[timestamp_string] <= str(datetime(end_year, int(end_month), end_day, hour, minute, second)))\n",
    "                    ]\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84dd8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_pat_annotations_mct_batch_to_file(current_pat_client_id_code, target_date_range, pat_doc_batch, sftp_obj = None, skip_check=False):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    current_annot_file_path = pre_annotation_path_mrc + current_pat_client_id_code + \"/\" + current_pat_client_id_code+\"_\"+str(target_date_range)\n",
    "    \n",
    "    if(skip_check):\n",
    "        file_exists = False\n",
    "    else:\n",
    "        file_exists = exist_check(current_annot_file_path, sftp_obj)\n",
    "\n",
    "    if(file_exists == False):\n",
    "    \n",
    "        start_year, start_month, end_year, end_month = get_start_end_year_month(target_date_range)\n",
    "\n",
    "        current_pat_docs = filter_dataframe_by_timestamp(pat_doc_batch, start_year, start_month, end_year, end_month, 'observationdocument_recordeddtm')\n",
    "\n",
    "\n",
    "        n_docs_to_annotate = len(current_pat_docs)\n",
    "        update_pbar(current_pat_client_id_code+\"_\"+str(target_date_range), start_time, 5, 'annotations_mct', n_docs_to_annotate = n_docs_to_annotate)\n",
    "\n",
    "    \n",
    "    if(file_exists==False):\n",
    "        with io.capture_output() as captured:\n",
    "            pats_anno_annotations = cat.get_entities_multi_texts(current_pat_docs['observation_valuetext_analysed'].dropna());#, n_process=1\n",
    "        if(store_annot):\n",
    "            dump_results(pats_anno_annotations, current_annot_file_path, sftp_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debe26a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_pat_annotations_batch_to_file(current_pat_client_id_code, target_date_range, pat_doc_batch, sftp_obj=None, skip_check=False):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    current_annotation_file_path = pre_annotation_path + current_pat_client_id_code + \"/\" + current_pat_client_id_code+\"_\"+str(target_date_range)\n",
    "    \n",
    "    if(skip_check):\n",
    "        file_exists = False\n",
    "    else:\n",
    "        file_exists = exist_check(current_annotation_file_path, sftp_obj)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if(file_exists == False):\n",
    "    \n",
    "        start_year, start_month, end_year, end_month = get_start_end_year_month(target_date_range)\n",
    "\n",
    "        current_pat_docs = filter_dataframe_by_timestamp(pat_doc_batch, start_year, start_month, end_year, end_month, 'updatetime')\n",
    "\n",
    "\n",
    "        n_docs_to_annotate = len(current_pat_docs)\n",
    "        update_pbar(current_pat_client_id_code+\"_\"+str(target_date_range), start_time, 5, 'annotations', n_docs_to_annotate = n_docs_to_annotate)\n",
    "\n",
    "    annotation_map = {'True':1,\n",
    "                     'Presence':1 ,\n",
    "                     'Recent': 1,\n",
    "                     'Past':0,\n",
    "                     'Subject/Experiencer':1,\n",
    "                     'Other':0,\n",
    "                     'Hypothetical':0,\n",
    "                     'Patient': 1}\n",
    "\n",
    "    #remove filter from cdb?\n",
    "    #print(\"getting annotations\")\n",
    "    \n",
    "#     file_exists = exists(pre_annotation_path + current_pat_client_id_code)\n",
    "    \n",
    "    \n",
    "    if(file_exists==False):\n",
    "        with io.capture_output() as captured:\n",
    "            pats_anno_annotations = cat.get_entities_multi_texts(current_pat_docs['body_analysed'].dropna());#, n_process=1\n",
    "        if(store_annot):\n",
    "            dump_results(pats_anno_annotations, current_annotation_file_path, sftp_obj)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69789c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_demographics3_batch(patlist, target_date_range, pat_batch):\n",
    "    \n",
    "    \n",
    "    start_year, start_month, end_year, end_month = get_start_end_year_month(target_date_range)\n",
    "    \n",
    "    \n",
    "    if(batch_mode):\n",
    "        \n",
    "        demo = filter_dataframe_by_timestamp(pat_batch, start_year, start_month, end_year, end_month, 'updatetime')\n",
    "\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        demo = cohort_searcher_with_terms_and_search(index_name=\"epr_documents\", \n",
    "                                             fields_list=[\"client_idcode\", \"client_firstname\", \"client_lastname\", \"client_dob\", \"client_gendercode\", \"client_racecode\", \"client_deceaseddtm\", \"updatetime\"], \n",
    "                                             term_name=\"client_idcode.keyword\", \n",
    "                                             entered_list=patlist,\n",
    "                                            search_string= f'updatetime:[{start_year}-{start_month} TO {end_year}-{end_month}] '\n",
    "                                                )\n",
    "    \n",
    "    \n",
    "    demo[\"updatetime\"] = pd.to_datetime(demo[\"updatetime\"], utc=True)\n",
    "    demo = demo.sort_values([\"client_idcode\", \"updatetime\"]) #.drop_duplicates(subset = [\"client_idcode\"], keep = \"last\", inplace = True)\n",
    "    \n",
    "    #if more than one in the range return the nearest the end of the period\n",
    "    if(len(demo)> 1):\n",
    "        try:\n",
    "            #print(\"case1\")\n",
    "            return demo.tail(1)\n",
    "            #return demo.iloc[-1].to_frame()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "    #if only one return it        \n",
    "    elif len(demo)==1:\n",
    "        return demo\n",
    "    \n",
    "    #otherwise return only the client id\n",
    "    else:\n",
    "        demo = pd.DataFrame(data=None, columns=None)\n",
    "        demo['client_idcode'] = patlist\n",
    "        return demo\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67679e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_demo(current_pat_client_id_code, target_date_range, pat_batch):\n",
    "\n",
    "    \n",
    "    \n",
    "    current_pat_demo = get_demographics3_batch([current_pat_client_id_code], target_date_range, pat_batch)\n",
    "    \n",
    "    #display(current_pat_demo)\n",
    "    \n",
    "    #print(len(current_pat_demo.columns))\n",
    "\n",
    "    if(len(current_pat_demo.columns)>1):\n",
    "\n",
    "        current_pat_demo = append_age_at_record_series(current_pat_demo)\n",
    "\n",
    "        #demo_dataframe = pd.DataFrame(current_pat_demo).T\n",
    "\n",
    "        #demo_dataframe.reset_index(inplace=True)\n",
    "        \n",
    "        demo_dataframe = current_pat_demo.copy()\n",
    "\n",
    "        current_pat_demo = EthnicityAbstractor.abstractEthnicity(demo_dataframe, outputNameString = '_census', ethnicityColumnString='client_racecode')\n",
    "\n",
    "        dummied_dummy_ethnicity_dataframe = pd.DataFrame(data = [(np.zeros(5))], columns = ['census_white', \n",
    "                                                                                            'census_asian_or_asian_british',\n",
    "                                                                                            'census_black_african_caribbean_or_black_british',\n",
    "                                                                                            'census_mixed_or_multiple_ethnic_groups',\n",
    "                                                                                            'census_other_ethnic_group'])\n",
    "\n",
    "        cen_res = pd.get_dummies(current_pat_demo['census'], prefix = 'census')\n",
    "\n",
    "        dummied_dummy_ethnicity_dataframe[cen_res.columns[0]] = cen_res[cen_res.columns[0]]\n",
    "\n",
    "        abstrated_ethnicity_dummied = dummied_dummy_ethnicity_dataframe\n",
    "\n",
    "        #abstrated_ethnicity_dummied = pd.concat([dummied_dummy_ethnicity_dataframe, pd.get_dummies(current_pat_demo['census'], prefix = 'census')], axis=1)\n",
    "\n",
    "        current_pat_demo = pd.concat([current_pat_demo.reset_index(), abstrated_ethnicity_dummied], axis=1)\n",
    "\n",
    "        current_pat_demo.reset_index( inplace=True)\n",
    "\n",
    "        #current_pat_demo\n",
    "\n",
    "        sex_map = {'Male': 1,\n",
    "                   'Female': 0,\n",
    "                   'male': 1,\n",
    "                   'female':0}\n",
    "\n",
    "\n",
    "        #         def is_dead(line):\n",
    "        #             try:\n",
    "        #                 if(type(line)==str or type(line)==float):\n",
    "        #                     return np.isnan(line)\n",
    "        #                 else:\n",
    "        #                     return line.isnull\n",
    "        #             except Exception as e:\n",
    "        #                 print(e)\n",
    "        #                 print(type(line))\n",
    "        #                 print(line)\n",
    "        #         def is_dead(line):\n",
    "        #             return line.isna()\n",
    "\n",
    "\n",
    "        current_pat_demo['male'] = current_pat_demo['client_gendercode'].map(sex_map)\n",
    "\n",
    "        #current_pat_demo['dead'] = current_pat_demo['client_deceaseddtm'].apply(is_dead)\n",
    "\n",
    "        #current_pat_demo['dead'] = int(int((np.isnan(current_pat_demo['client_deceaseddtm'].iloc[0])))!=1)\n",
    "\n",
    "        current_pat_demo['dead'] = int(type(current_pat_demo['client_deceaseddtm'])==str)\n",
    "\n",
    "        current_pat_demo = current_pat_demo[['client_idcode',\n",
    "                                             'male',\n",
    "                                             'age',\n",
    "                                             'dead',\n",
    "                                             'census_white',\n",
    "                                             'census_asian_or_asian_british',\n",
    "                                             'census_black_african_caribbean_or_black_british',\n",
    "                                             'census_mixed_or_multiple_ethnic_groups',\n",
    "                                             'census_other_ethnic_group']].copy()\n",
    "\n",
    "        current_pat_demo['dead'] = current_pat_demo['dead'].astype(int)\n",
    "\n",
    "        current_pat_demo['age'] = current_pat_demo['age'].astype(int)\n",
    "\n",
    "        current_pat_demo['dead'] = current_pat_demo['dead'].astype(float)\n",
    "\n",
    "        current_pat_demo['age'] = current_pat_demo['age'].astype(float)\n",
    "\n",
    "        current_pat_demo['male'] = current_pat_demo['male'].astype(float)\n",
    "\n",
    "\n",
    "\n",
    "        return current_pat_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aacfb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smoking(current_pat_client_id_code, target_date_range, pat_batch):\n",
    "    start_year, start_month, end_year, end_month = get_start_end_year_month(target_date_range)\n",
    "\n",
    "    search_term = 'CORE_SmokingStatus'\n",
    "    \n",
    "    if(batch_mode):\n",
    "        current_pat_raw = filter_dataframe_by_timestamp(pat_batch, start_year, start_month, end_year, end_month, 'observationdocument_recordeddtm')\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        current_pat_raw = cohort_searcher_with_terms_and_search(index_name=\"observations\", \n",
    "                                                                   fields_list=\"\"\"observation_guid client_idcode\tobscatalogmasteritem_displayname\tobservation_valuetext_analysed\tobservationdocument_recordeddtm clientvisit_visitidcode\"\"\".split(),\n",
    "                                                                   term_name=\"client_idcode.keyword\", \n",
    "                                                                   entered_list=[current_pat_client_id_code], \n",
    "\n",
    "\n",
    "    #                                                                 search_string=f\"obscatalogmasteritem_displayname:(\\\"{search_term}\\\")\")\n",
    "                                                                 search_string=f\"obscatalogmasteritem_displayname:(\\\"{search_term}\\\") AND \" + f'observationdocument_recordeddtm:[{start_year}-{start_month} TO {end_year}-{end_month}]')\n",
    "    if(len(current_pat_raw)==0):\n",
    "\n",
    "        features = pd.DataFrame(data = [current_pat_client_id_code], columns=['client_idcode'])\n",
    "\n",
    "\n",
    "    features_data = current_pat_raw[current_pat_raw['obscatalogmasteritem_displayname']==search_term].copy()\n",
    "\n",
    "    #screen and purge dud values\n",
    "    #features_data =  features_data[(features_data['observation_valuetext_analysed'].astype(float)<20)& (features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    features_data.dropna(inplace=True)\n",
    "\n",
    "    #-----------------------------------------------------------------\n",
    "\n",
    "    features_data = current_pat_raw[current_pat_raw['obscatalogmasteritem_displayname']==search_term].copy()\n",
    "    #features_data =  features_data[(features_data['observation_valuetext_analysed'].astype(float)<20)& (features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    #features_data.dropna(inplace=True)    \n",
    "\n",
    "\n",
    "    term = 'smoking_status'.lower()\n",
    "\n",
    "    if(len(features_data) > 0):\n",
    "        features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "\n",
    "        value_array = features_data['observation_valuetext_analysed'].dropna()\n",
    "\n",
    "        features[f'{term}_current'] = value_array.str.contains(\"Current Smoker\").astype(int)\n",
    "        features[f'{term}_non'] = value_array.str.contains(\"Non-Smoker\").astype(int)\n",
    "\n",
    "    else:\n",
    "        features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        features[f'{term}_current'] = np.nan\n",
    "        features[f'{term}_non'] = np.nan\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f52990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_core_02(current_pat_client_id_code, target_date_range, pat_batch):\n",
    "    \n",
    "    start_year, start_month, end_year, end_month = get_start_end_year_month(target_date_range)\n",
    "    \n",
    "    search_term = 'CORE_SpO2'\n",
    "    \n",
    "    if(batch_mode):\n",
    "        current_pat_raw = filter_dataframe_by_timestamp(pat_batch, start_year, start_month, end_year, end_month, 'observationdocument_recordeddtm')\n",
    "        \n",
    "    else:\n",
    "        current_pat_raw = cohort_searcher_with_terms_and_search(index_name=\"observations\", \n",
    "                                                               fields_list=\"\"\"observation_guid client_idcode\tobscatalogmasteritem_displayname\tobservation_valuetext_analysed\tobservationdocument_recordeddtm clientvisit_visitidcode\"\"\".split(),\n",
    "                                                               term_name=\"client_idcode.keyword\", \n",
    "                                                               entered_list=[current_pat_client_id_code], \n",
    "\n",
    "\n",
    "\n",
    "                                                            search_string=f\"obscatalogmasteritem_displayname:(\\\"{search_term}\\\") AND \" + f'observationdocument_recordeddtm:[{start_year}-{start_month} TO {end_year}-{end_month}]')\n",
    "\n",
    "    if(len(current_pat_raw)==0):\n",
    "\n",
    "        features = pd.DataFrame(data = [current_pat_client_id_code], columns=['client_idcode'])\n",
    "\n",
    "\n",
    "    features_data = current_pat_raw[current_pat_raw['obscatalogmasteritem_displayname']==search_term].copy()\n",
    "\n",
    "    #screen and purge dud values\n",
    "    #features_data =  features_data[(features_data['observation_valuetext_analysed'].astype(float)<20)& (features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    features_data.dropna(inplace=True)\n",
    "\n",
    "    #-----------------------------------------------------------------\n",
    "\n",
    "    features_data = current_pat_raw[current_pat_raw['obscatalogmasteritem_displayname']==search_term].copy()\n",
    "    #features_data =  features_data[(features_data['observation_valuetext_analysed'].astype(float)<20)& (features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    #features_data.dropna(inplace=True)    \n",
    "\n",
    "\n",
    "    term = 'core_sp_02'.lower()\n",
    "\n",
    "    if(len(features_data) > 0):\n",
    "        features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "\n",
    "\n",
    "        all_terms = list(features_data['observation_valuetext_analysed'].dropna().unique())\n",
    "\n",
    "        for term in all_terms:\n",
    "\n",
    "            features[f'{term.replace(\"-\", \"_\").replace(\"%\", \"pct\")}'] = 1\n",
    "            #features[f'{bed_term}'] = 1\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1fd4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bed(current_pat_client_id_code, target_date_range, pat_batch):\n",
    "    \n",
    "    start_year, start_month, end_year, end_month = get_start_end_year_month(target_date_range)\n",
    "    \n",
    "    search_term = 'CORE_BedNumber3'\n",
    "    \n",
    "    if(batch_mode):\n",
    "        current_pat_raw = filter_dataframe_by_timestamp(pat_batch, start_year, start_month, end_year, end_month, 'observationdocument_recordeddtm')\n",
    "        \n",
    "    else:\n",
    "        current_pat_raw = cohort_searcher_with_terms_and_search(index_name=\"observations\", \n",
    "                                                               fields_list=\"\"\"observation_guid client_idcode\tobscatalogmasteritem_displayname\tobservation_valuetext_analysed\tobservationdocument_recordeddtm clientvisit_visitidcode\"\"\".split(),\n",
    "                                                               term_name=\"client_idcode.keyword\", \n",
    "                                                               entered_list=[current_pat_client_id_code], \n",
    "\n",
    "\n",
    "\n",
    "                                                            search_string=f\"obscatalogmasteritem_displayname:(\\\"{search_term}\\\") AND \" + f'observationdocument_recordeddtm:[{start_year}-{start_month} TO {end_year}-{end_month}]')\n",
    "\n",
    "    if(len(current_pat_raw)==0):\n",
    "\n",
    "        features = pd.DataFrame(data = [current_pat_client_id_code], columns=['client_idcode'])\n",
    "\n",
    "\n",
    "    features_data = current_pat_raw[current_pat_raw['obscatalogmasteritem_displayname']==search_term].copy()\n",
    "\n",
    "    #screen and purge dud values\n",
    "    #features_data =  features_data[(features_data['observation_valuetext_analysed'].astype(float)<20)& (features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    features_data.dropna(inplace=True)\n",
    "\n",
    "    #-----------------------------------------------------------------\n",
    "\n",
    "    features_data = current_pat_raw[current_pat_raw['obscatalogmasteritem_displayname']==search_term].copy()\n",
    "    #features_data =  features_data[(features_data['observation_valuetext_analysed'].astype(float)<20)& (features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    #features_data.dropna(inplace=True)    \n",
    "\n",
    "\n",
    "    term = 'bed'.lower()\n",
    "\n",
    "    if(len(features_data) > 0):\n",
    "        features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "\n",
    "\n",
    "        all_bed_terms = list(features_data['observation_valuetext_analysed'].dropna().unique())\n",
    "\n",
    "        for bed_term in all_bed_terms:\n",
    "\n",
    "            features[f'bed_{bed_term}'] = 1\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f6737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vte_status(current_pat_client_id_code,target_date_range, pat_batch):\n",
    "    \n",
    "    start_year, start_month, end_year, end_month = get_start_end_year_month(target_date_range)\n",
    "    \n",
    "    search_term = 'CORE_VTE_STATUS'\n",
    "    \n",
    "    if(batch_mode):\n",
    "        current_pat_raw = filter_dataframe_by_timestamp(pat_batch, start_year, start_month, end_year, end_month, 'observationdocument_recordeddtm')\n",
    "        \n",
    "    else:\n",
    "        current_pat_raw = cohort_searcher_with_terms_and_search(index_name=\"observations\", \n",
    "                                                               fields_list=\"\"\"observation_guid client_idcode\tobscatalogmasteritem_displayname\tobservation_valuetext_analysed\tobservationdocument_recordeddtm clientvisit_visitidcode\"\"\".split(),\n",
    "                                                               term_name=\"client_idcode.keyword\", \n",
    "                                                               entered_list=[current_pat_client_id_code], \n",
    "                                                               search_string=f\"obscatalogmasteritem_displayname:(\\\"{search_term}\\\") AND \" + f'observationdocument_recordeddtm:[{start_year}-{start_month} TO {end_year}-{end_month}]')\n",
    "\n",
    "    if(len(current_pat_raw)==0):\n",
    "\n",
    "        features = pd.DataFrame(data = [current_pat_client_id_code], columns=['client_idcode'])\n",
    "\n",
    "\n",
    "    features_data = current_pat_raw[current_pat_raw['obscatalogmasteritem_displayname']==search_term].copy()\n",
    "\n",
    "    #screen and purge dud values\n",
    "    #features_data =  features_data[(features_data['observation_valuetext_analysed'].astype(float)<20)& (features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    features_data.dropna(inplace=True)\n",
    "\n",
    "    #-----------------------------------------------------------------\n",
    "\n",
    "    features_data = current_pat_raw[current_pat_raw['obscatalogmasteritem_displayname']==search_term].copy()\n",
    "    #features_data =  features_data[(features_data['observation_valuetext_analysed'].astype(float)<20)& (features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    #features_data.dropna(inplace=True)    \n",
    "\n",
    "    term = 'VTE_Status'.lower()\n",
    "\n",
    "    if(len(features_data) > 0):\n",
    "        features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "\n",
    "\n",
    "\n",
    "        di = {'High risk of VTE High risk of bleeding':1,\n",
    "              'High risk of VTE Low risk of bleeding':0}\n",
    "\n",
    "        value_array = features_data['observation_valuetext_analysed'].map(di)\n",
    "\n",
    "\n",
    "        value_array = value_array.astype(float)\n",
    "\n",
    "        features[f'{term}_mean'] = value_array.mean()\n",
    "        features[f'{term}_median'] = value_array.median()\n",
    "        features[f'{term}_std'] = value_array.std()\n",
    "        features[f'{term}_max'] = max(value_array)\n",
    "        features[f'{term}_min'] = min(value_array)\n",
    "        features[f'{term}_n'] = value_array.shape[0]\n",
    "    else:\n",
    "        features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        features[f'{term}_mean'] = np.nan\n",
    "        features[f'{term}_median'] = np.nan\n",
    "        features[f'{term}_std'] = np.nan\n",
    "        features[f'{term}_max'] = np.nan\n",
    "        features[f'{term}_min'] = np.nan\n",
    "        features[f'{term}_n'] = np.nan   \n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0866414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hosp_site(current_pat_client_id_code, target_date_range, pat_batch):\n",
    "    \n",
    "    start_year, start_month, end_year, end_month = get_start_end_year_month(target_date_range)\n",
    "    \n",
    "    \n",
    "    search_term = 'CORE_HospitalSite'\n",
    "    if(batch_mode):\n",
    "        current_pat_raw = filter_dataframe_by_timestamp(pat_batch, start_year, start_month, end_year, end_month, 'observationdocument_recordeddtm')\n",
    "        \n",
    "    else:\n",
    "        current_pat_raw = cohort_searcher_with_terms_and_search(index_name=\"observations\", \n",
    "                                                               fields_list=\"\"\"observation_guid client_idcode\tobscatalogmasteritem_displayname\tobservation_valuetext_analysed\tobservationdocument_recordeddtm clientvisit_visitidcode\"\"\".split(),\n",
    "                                                               term_name=\"client_idcode.keyword\", \n",
    "                                                               entered_list=[current_pat_client_id_code], \n",
    "                                                               search_string=f\"obscatalogmasteritem_displayname:(\\\"{search_term}\\\") AND \" + f'observationdocument_recordeddtm:[{start_year}-{start_month} TO {end_year}-{end_month}]')\n",
    "\n",
    "    if(len(current_pat_raw)==0):\n",
    "\n",
    "        features = pd.DataFrame(data = [current_pat_client_id_code], columns=['client_idcode'])\n",
    "\n",
    "\n",
    "    features_data = current_pat_raw[current_pat_raw['obscatalogmasteritem_displayname']==search_term].copy()\n",
    "\n",
    "    #screen and purge dud values\n",
    "    #features_data =  features_data[(features_data['observation_valuetext_analysed'].astype(float)<20)& (features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    features_data.dropna(inplace=True)\n",
    "\n",
    "    #-----------------------------------------------------------------\n",
    "\n",
    "    features_data = current_pat_raw[current_pat_raw['obscatalogmasteritem_displayname']==search_term].copy()\n",
    "    #features_data =  features_data[(features_data['observation_valuetext_analysed'].astype(float)<20)& (features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    #features_data.dropna(inplace=True)    \n",
    "\n",
    "    term = 'hosp_site'.lower()\n",
    "\n",
    "    if(len(features_data) > 0):\n",
    "        features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        #value_array = features_data['observation_valuetext_analysed'].astype(float)\n",
    "        value_array = features_data['observation_valuetext_analysed'].dropna()\n",
    "\n",
    "        features[f'{term}_dh'] = value_array.str.contains(\"DH\").astype(int)\n",
    "        features[f'{term}_ph'] = value_array.str.contains(\"PRUH\").astype(int)\n",
    "\n",
    "    else:\n",
    "        features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        features[f'{term}_dh'] = np.nan\n",
    "        features[f'{term}_ph'] = np.nan\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b3d122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_core_resus(current_pat_client_id_code, target_date_range, pat_batch):\n",
    "    \n",
    "    start_year, start_month, end_year, end_month = get_start_end_year_month(target_date_range)\n",
    "    \n",
    "    if(batch_mode):\n",
    "        current_pat_raw = filter_dataframe_by_timestamp(pat_batch, start_year, start_month, end_year, end_month, 'observationdocument_recordeddtm')\n",
    "        \n",
    "    else:\n",
    "        current_pat_raw = cohort_searcher_with_terms_and_search(index_name=\"observations\", \n",
    "                                                               fields_list=\"\"\"observation_guid client_idcode\tobscatalogmasteritem_displayname\tobservation_valuetext_analysed\tobservationdocument_recordeddtm clientvisit_visitidcode\"\"\".split(),\n",
    "                                                               term_name=\"client_idcode.keyword\", \n",
    "                                                               entered_list=[current_pat_client_id_code], \n",
    "                                                               search_string=\"obscatalogmasteritem_displayname:(\\\"CORE_RESUS_STATUS\\\") AND \" + f'observationdocument_recordeddtm:[{start_year}-{start_month} TO {end_year}-{end_month}]')\n",
    "\n",
    "\n",
    "    if(len(current_pat_raw)==0):\n",
    "\n",
    "        features = pd.DataFrame(data = [current_pat_client_id_code], columns=['client_idcode'])\n",
    "\n",
    "\n",
    "    features_data = current_pat_raw[current_pat_raw['obscatalogmasteritem_displayname']=='CORE_RESUS_STATUS'].copy()\n",
    "\n",
    "    #screen and purge dud values\n",
    "    #features_data =  features_data[(features_data['observation_valuetext_analysed'].astype(float)<20)& (features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    #features_data.dropna(inplace=True)\n",
    "\n",
    "    #-----------------------------------------------------------------\n",
    "\n",
    "    #features_data = current_pat_raw[current_pat_raw['obscatalogmasteritem_displayname']=='CORE_RESUS_STATUS'].copy()\n",
    "    #features_data =  features_data[(features_data['observation_valuetext_analysed'].astype(float)<20)& (features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    #features_data.dropna(inplace=True)    \n",
    "\n",
    "    term = 'CORE_RESUS_STATUS'.lower()\n",
    "\n",
    "    if(len(features_data) > 0):\n",
    "        features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        features[f'{term}_For cardiopulmonary resuscitation'] = len(features_data[features_data['observation_valuetext_analysed'] == 'For cardiopulmonary resuscitation'])\n",
    "        features[f'{term}_Not for cardiopulmonary resuscitation'] = len(features_data[features_data['observation_valuetext_analysed'] == 'Not for cardiopulmonary resuscitation'])\n",
    "    else:\n",
    "        features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        features[f'{term}_For cardiopulmonary resuscitation'] = len(features_data[features_data['observation_valuetext_analysed'] == 'For cardiopulmonary resuscitation'])\n",
    "        features[f'{term}_Not for cardiopulmonary resuscitation'] = len(features_data[features_data['observation_valuetext_analysed'] == 'Not for cardiopulmonary resuscitation'])\n",
    "\n",
    "\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b01c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(current_pat_client_id_code, target_date_range, pat_batch):\n",
    "    \n",
    "    start_year, start_month, end_year, end_month = get_start_end_year_month(target_date_range)\n",
    "    \n",
    "    if(batch_mode):\n",
    "        current_pat_raw_news = filter_dataframe_by_timestamp(pat_batch, start_year, start_month, end_year, end_month, 'observationdocument_recordeddtm')\n",
    "        \n",
    "    else:\n",
    "        current_pat_raw_news = cohort_searcher_with_terms_and_search(index_name=\"observations\", \n",
    "                                                                   fields_list=\"\"\"observation_guid client_idcode\tobscatalogmasteritem_displayname\tobservation_valuetext_analysed\tobservationdocument_recordeddtm clientvisit_visitidcode\"\"\".split(),\n",
    "                                                                   term_name=\"client_idcode.keyword\", \n",
    "                                                                   entered_list=[current_pat_client_id_code], \n",
    "                                                                   search_string='obscatalogmasteritem_displayname:(\\\"NEWS\\\" OR \\\"NEWS2\\\") AND ' + f'observationdocument_recordeddtm:[{start_year}-{start_month} TO {end_year}-{end_month}]')\n",
    "\n",
    "\n",
    "    #if(len(current_pat_raw_news)==0):\n",
    "\n",
    "    news_features = pd.DataFrame(data = [current_pat_client_id_code], columns=['client_idcode'])\n",
    "\n",
    "\n",
    "    news_features_data = current_pat_raw_news[current_pat_raw_news['obscatalogmasteritem_displayname']=='NEWS2_Score'].copy()\n",
    "\n",
    "    #screen and purge dud values\n",
    "    news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "\n",
    "    if(len(news_features_data) > 0):\n",
    "        news_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        value_array = news_features_data['observation_valuetext_analysed'].dropna().astype(float)\n",
    "        news_features['news_score_mean'] = value_array.mean()\n",
    "        news_features['news_score_median'] = value_array.median()\n",
    "        news_features['news_score_std'] = value_array.std()\n",
    "        news_features['news_score_max'] = max(value_array)\n",
    "        news_features['news_score_min'] = min(value_array)\n",
    "        news_features['news_score_n'] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features['news_score_mean'] = np.nan\n",
    "        news_features['news_score_median'] = np.nan\n",
    "        news_features['news_score_std'] = np.nan\n",
    "        news_features['news_score_max'] = np.nan\n",
    "        news_features['news_score_min'] = np.nan\n",
    "        news_features['news_score_n'] = np.nan\n",
    "\n",
    "    #-----------------------------------------------------------------\n",
    "    news_features_data = current_pat_raw_news[current_pat_raw_news['obscatalogmasteritem_displayname']=='NEWS_Systolic_BP'].copy()\n",
    "    #news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    news_features_data.dropna(subset=['observation_valuetext_analysed'],inplace=True)   \n",
    "\n",
    "    if(len(news_features_data) > 0):\n",
    "        #news_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        value_array = news_features_data['observation_valuetext_analysed'].dropna().dropna().astype(float)\n",
    "        news_features['news_systolic_bp_mean'] = value_array.mean()\n",
    "        news_features['news_systolic_bp_median'] = value_array.median()\n",
    "        news_features['news_systolic_bp_std'] = value_array.std()\n",
    "        news_features['news_systolic_bp_max'] = max(value_array)\n",
    "        news_features['news_systolic_bp_min'] = min(value_array)\n",
    "        news_features['news_systolic_bp_n'] = value_array.shape[0]\n",
    "    else:\n",
    "        \n",
    "        news_features['news_systolic_bp_mean'] = np.nan\n",
    "        news_features['news_systolic_bp_median'] = np.nan\n",
    "        news_features['news_systolic_bp_std'] = np.nan\n",
    "        news_features['news_systolic_bp_max'] = np.nan\n",
    "        news_features['news_systolic_bp_min'] = np.nan\n",
    "        news_features['news_systolic_bp_n'] = np.nan\n",
    "\n",
    "    #-----------------------------------------------------------------\n",
    "    news_features_data = current_pat_raw_news[current_pat_raw_news['obscatalogmasteritem_displayname']=='NEWS_Diastolic_BP'].copy()\n",
    "    #news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    news_features_data.dropna(subset=['observation_valuetext_analysed'],inplace=True)   \n",
    "\n",
    "    if(len(news_features_data) > 0):\n",
    "        #news_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        value_array = news_features_data['observation_valuetext_analysed'].dropna().astype(float)\n",
    "        news_features['news_diastolic_bp_mean'] = value_array.mean()\n",
    "        news_features['news_diastolic_bp_median'] = value_array.median()\n",
    "        news_features['news_diastolic_bp_std'] = value_array.std()\n",
    "        news_features['news_diastolic_bp_max'] = max(value_array)\n",
    "        news_features['news_diastolic_bp_min'] = min(value_array)\n",
    "        news_features['news_diastolic_bp_n'] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features['news_diastolic_bp_mean'] = np.nan\n",
    "        news_features['news_diastolic_bp_median'] = np.nan\n",
    "        news_features['news_diastolic_bp_std'] = np.nan\n",
    "        news_features['news_diastolic_bp_max'] = np.nan\n",
    "        news_features['news_diastolic_bp_min'] = np.nan\n",
    "        news_features['news_diastolic_bp_n'] = np.nan    \n",
    "\n",
    "    #-----------------------------------------------------------------\n",
    "    news_features_data = current_pat_raw_news[current_pat_raw_news['obscatalogmasteritem_displayname']=='NEWS_Respiration_Rate'].copy()\n",
    "    #news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    news_features_data.dropna(subset=['observation_valuetext_analysed'],inplace=True)   \n",
    "\n",
    "    if(len(news_features_data) > 0):\n",
    "        #news_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        value_array = news_features_data['observation_valuetext_analysed'].dropna().astype(float)\n",
    "        news_features['news_respiration_rate_mean'] = value_array.mean()\n",
    "        news_features['news_respiration_rate_median'] = value_array.median()\n",
    "        news_features['news_respiration_rate_std'] = value_array.std()\n",
    "        news_features['news_respiration_rate_max'] = max(value_array)\n",
    "        news_features['news_respiration_rate_min'] = min(value_array)\n",
    "        news_features['news_respiration_rate_n'] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features['news_respiration_rate_mean'] = np.nan\n",
    "        news_features['news_respiration_rate_median'] = np.nan\n",
    "        news_features['news_respiration_rate_std'] = np.nan\n",
    "        news_features['news_respiration_rate_max'] = np.nan\n",
    "        news_features['news_respiration_rate_min'] = np.nan\n",
    "        news_features['news_respiration_rate_n'] = np.nan    \n",
    "\n",
    "\n",
    "    #-----------------------------------------------------------------\n",
    "\n",
    "    news_features_data = current_pat_raw_news[current_pat_raw_news['obscatalogmasteritem_displayname']=='NEWS_Heart_Rate'].copy()\n",
    "\n",
    "    news_features_data.dropna(subset=['observation_valuetext_analysed'],inplace=True)   \n",
    "\n",
    "    if(len(news_features_data) > 0):\n",
    "\n",
    "        value_array = news_features_data['observation_valuetext_analysed'].dropna().astype(float)\n",
    "        news_features['news_heart_rate_mean'] = value_array.mean()\n",
    "        news_features['news_heart_rate_median'] = value_array.median()\n",
    "        news_features['news_heart_rate_std'] = value_array.std()\n",
    "        news_features['news_heart_rate_max'] = max(value_array)\n",
    "        news_features['news_heart_rate_min'] = min(value_array)\n",
    "        news_features['news_heart_rate_n'] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features['news_heart_rate_mean'] = np.nan\n",
    "        news_features['news_heart_rate_median'] = np.nan\n",
    "        news_features['news_heart_rate_std'] = np.nan\n",
    "        news_features['news_heart_rate_max'] = np.nan\n",
    "        news_features['news_heart_rate_min'] = np.nan\n",
    "        news_features['news_heart_rate_n'] = np.nan    \n",
    "\n",
    "    #-----------------------------------------------------------------\n",
    "\n",
    "    news_features_data = current_pat_raw_news[current_pat_raw_news['obscatalogmasteritem_displayname']=='NEWS_Oxygen_Saturation'].copy()\n",
    "    #news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    news_features_data.dropna(subset=['observation_valuetext_analysed'],inplace=True)     \n",
    "\n",
    "    if(len(news_features_data) > 0):\n",
    "\n",
    "        value_array = news_features_data['observation_valuetext_analysed'].dropna().astype(float)\n",
    "        news_features['news_oxygen_saturation_mean'] = value_array.mean()\n",
    "        news_features['news_oxygen_saturation_median'] = value_array.median()\n",
    "        news_features['news_oxygen_saturation_std'] = value_array.std()\n",
    "        news_features['news_oxygen_saturation_max'] = max(value_array)\n",
    "        news_features['news_oxygen_saturation_min'] = min(value_array)\n",
    "        news_features['news_oxygen_saturation_n'] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features['news_oxygen_saturation_mean'] = np.nan\n",
    "        news_features['news_oxygen_saturation_median'] = np.nan\n",
    "        news_features['news_oxygen_saturation_std'] = np.nan\n",
    "        news_features['news_oxygen_saturation_max'] = np.nan\n",
    "        news_features['news_oxygen_saturation_min'] = np.nan\n",
    "        news_features['news_oxygen_saturation_n'] = np.nan    \n",
    "\n",
    "    #-----------------------------------------------------------------\n",
    "\n",
    "    news_features_data = current_pat_raw_news[current_pat_raw_news['obscatalogmasteritem_displayname']=='NEWS Temperature'].copy()\n",
    "    #news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    news_features_data.dropna(subset=['observation_valuetext_analysed'],inplace=True)       \n",
    "\n",
    "    if(len(news_features_data) > 0):\n",
    "        #news_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        value_array = news_features_data['observation_valuetext_analysed'].dropna().astype(float)\n",
    "        news_features['news_temperature_mean'] = value_array.mean()\n",
    "        news_features['news_temperature_median'] = value_array.median()\n",
    "        news_features['news_temperature_std'] = value_array.std()\n",
    "        news_features['news_temperature_max'] = max(value_array)\n",
    "        news_features['news_temperature_min'] = min(value_array)\n",
    "        news_features['news_temperature_n'] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features['news_temperature_mean'] = np.nan\n",
    "        news_features['news_temperature_median'] = np.nan\n",
    "        news_features['news_temperature_std'] = np.nan\n",
    "        news_features['news_temperature_max'] = np.nan\n",
    "        news_features['news_temperature_min'] = np.nan\n",
    "        news_features['news_temperature_n'] = np.nan    \n",
    "\n",
    "    #-----------------------------------------------------------------\n",
    "\n",
    "    news_features_data = current_pat_raw_news[current_pat_raw_news['obscatalogmasteritem_displayname']=='NEWS_AVPU'].copy()\n",
    "    #news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    news_features_data.dropna(subset=['observation_valuetext_analysed'],inplace=True)        \n",
    "\n",
    "\n",
    "\n",
    "    if(len(news_features_data) > 0):\n",
    "        #news_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        value_array = news_features_data['observation_valuetext_analysed'].dropna().astype(float)\n",
    "        news_features['news_avpu_mean'] = value_array.mean()\n",
    "        news_features['news_avpu_median'] = value_array.median()\n",
    "        news_features['news_avpu_std'] = value_array.std()\n",
    "        news_features['news_avpu_max'] = max(value_array)\n",
    "        news_features['news_avpu_min'] = min(value_array)\n",
    "        news_features['news_avpu_n'] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features['news_avpu_mean'] = np.nan\n",
    "        news_features['news_avpu_median'] = np.nan\n",
    "        news_features['news_avpu_std'] = np.nan\n",
    "        news_features['news_avpu_max'] = np.nan\n",
    "        news_features['news_avpu_min'] = np.nan\n",
    "        news_features['news_avpu_n'] = np.nan    \n",
    "\n",
    "    #-----------------------------------------------------------------\n",
    "\n",
    "    news_features_data = current_pat_raw_news[current_pat_raw_news['obscatalogmasteritem_displayname']=='NEWS_Supplemental_Oxygen'].copy()\n",
    "    #news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    news_features_data.dropna(subset=['observation_valuetext_analysed'],inplace=True)     \n",
    "\n",
    "    term = 'supplemental_oxygen'\n",
    "\n",
    "    if(len(news_features_data) > 0):\n",
    "        #news_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        value_array = news_features_data['observation_valuetext_analysed'].dropna().astype(float)\n",
    "        news_features[f'news_{term}_mean'] = value_array.mean()\n",
    "        news_features[f'news_{term}_median'] = value_array.median()\n",
    "        news_features[f'news_{term}_std'] = value_array.std()\n",
    "        news_features[f'news_{term}_max'] = max(value_array)\n",
    "        news_features[f'news_{term}_min'] = min(value_array)\n",
    "        news_features[f'news_{term}_n'] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features[f'news_{term}_mean'] = np.nan\n",
    "        news_features[f'news_{term}_median'] = np.nan\n",
    "        news_features[f'news_{term}_std'] = np.nan\n",
    "        news_features[f'news_{term}_max'] = np.nan\n",
    "        news_features[f'news_{term}_min'] = np.nan\n",
    "        news_features[f'news_{term}_n'] = np.nan    \n",
    "\n",
    "    #-----------------------------------------------------------------\n",
    "\n",
    "    news_features_data = current_pat_raw_news[current_pat_raw_news['obscatalogmasteritem_displayname']=='NEWS2_Sp02_Target'].copy()\n",
    "    #news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    news_features_data.dropna(subset=['observation_valuetext_analysed'],inplace=True)       \n",
    "\n",
    "    term = 'Sp02_Target'.lower()\n",
    "\n",
    "    if(len(news_features_data) > 0):\n",
    "        #news_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        value_array = news_features_data['observation_valuetext_analysed'].dropna().astype(float)\n",
    "        news_features[f'news_{term}_mean'] = value_array.mean()\n",
    "        news_features[f'news_{term}_median'] = value_array.median()\n",
    "        news_features[f'news_{term}_std'] = value_array.std()\n",
    "        news_features[f'news_{term}_max'] = max(value_array)\n",
    "        news_features[f'news_{term}_min'] = min(value_array)\n",
    "        news_features[f'news_{term}_n'] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features[f'news_{term}_mean'] = np.nan\n",
    "        news_features[f'news_{term}_median'] = np.nan\n",
    "        news_features[f'news_{term}_std'] = np.nan\n",
    "        news_features[f'news_{term}_max'] = np.nan\n",
    "        news_features[f'news_{term}_min'] = np.nan\n",
    "        news_features[f'news_{term}_n'] = np.nan  \n",
    "\n",
    "    #-----------------------------------------------------------------\n",
    "\n",
    "    news_features_data = current_pat_raw_news[current_pat_raw_news['obscatalogmasteritem_displayname']=='NEWS2_Sp02_Scale'].copy()\n",
    "    #news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    news_features_data.dropna(subset=['observation_valuetext_analysed'],inplace=True)     \n",
    "\n",
    "    term = 'Sp02_Scale'.lower()\n",
    "\n",
    "    if(len(news_features_data) > 0):\n",
    "        #news_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        value_array = news_features_data['observation_valuetext_analysed'].dropna().astype(float)\n",
    "        news_features[f'news_{term}_mean'] = value_array.mean()\n",
    "        news_features[f'news_{term}_median'] = value_array.median()\n",
    "        news_features[f'news_{term}_std'] = value_array.std()\n",
    "        news_features[f'news_{term}_max'] = max(value_array)\n",
    "        news_features[f'news_{term}_min'] = min(value_array)\n",
    "        news_features[f'news_{term}_n'] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features[f'news_{term}_mean'] = np.nan\n",
    "        news_features[f'news_{term}_median'] = np.nan\n",
    "        news_features[f'news_{term}_std'] = np.nan\n",
    "        news_features[f'news_{term}_max'] = np.nan\n",
    "        news_features[f'news_{term}_min'] = np.nan\n",
    "        news_features[f'news_{term}_n'] = np.nan  \n",
    "\n",
    "    #-----------------------------------------------------------------\n",
    "\n",
    "    news_features_data = current_pat_raw_news[current_pat_raw_news['obscatalogmasteritem_displayname']=='NEWS_Pulse_Type'].copy()\n",
    "    #news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    news_features_data.dropna(subset=['observation_valuetext_analysed'],inplace=True)     \n",
    "\n",
    "    term = 'pulse_type'.lower()\n",
    "\n",
    "    if(len(news_features_data) > 0):\n",
    "        #news_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        value_array = news_features_data['observation_valuetext_analysed'].dropna().astype(float)\n",
    "        news_features[f'news_{term}_mean'] = value_array.mean()\n",
    "        news_features[f'news_{term}_median'] = value_array.median()\n",
    "        news_features[f'news_{term}_std'] = value_array.std()\n",
    "        news_features[f'news_{term}_max'] = max(value_array)\n",
    "        news_features[f'news_{term}_min'] = min(value_array)\n",
    "        news_features[f'news_{term}_n'] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features[f'news_{term}_mean'] = np.nan\n",
    "        news_features[f'news_{term}_median'] = np.nan\n",
    "        news_features[f'news_{term}_std'] = np.nan\n",
    "        news_features[f'news_{term}_max'] = np.nan\n",
    "        news_features[f'news_{term}_min'] = np.nan\n",
    "        news_features[f'news_{term}_n'] = np.nan      \n",
    "\n",
    "    #-----------------------------------------------------------------\n",
    "\n",
    "    news_features_data = current_pat_raw_news[current_pat_raw_news['obscatalogmasteritem_displayname']=='NEWS_Pain_Score'].copy()\n",
    "    #news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    news_features_data.dropna(subset=['observation_valuetext_analysed'],inplace=True)      \n",
    "\n",
    "    term = 'Pain_Score'.lower()\n",
    "\n",
    "    if(len(news_features_data) > 0):\n",
    "        #news_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        value_array = news_features_data['observation_valuetext_analysed'].astype(float)\n",
    "        news_features[f'news_{term}_mean'] = value_array.mean()\n",
    "        news_features[f'news_{term}_median'] = value_array.median()\n",
    "        news_features[f'news_{term}_std'] = value_array.std()\n",
    "        news_features[f'news_{term}_max'] = max(value_array)\n",
    "        news_features[f'news_{term}_min'] = min(value_array)\n",
    "        news_features[f'news_{term}_n'] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features[f'news_{term}_mean'] = np.nan\n",
    "        news_features[f'news_{term}_median'] = np.nan\n",
    "        news_features[f'news_{term}_std'] = np.nan\n",
    "        news_features[f'news_{term}_max'] = np.nan\n",
    "        news_features[f'news_{term}_min'] = np.nan\n",
    "        news_features[f'news_{term}_n'] = np.nan    \n",
    "\n",
    "    #-----------------------------------------------------------------\n",
    "\n",
    "    news_features_data = current_pat_raw_news[current_pat_raw_news['obscatalogmasteritem_displayname']=='NEWS Oxygen Litres'].copy()\n",
    "    #news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    news_features_data.dropna(subset=['observation_valuetext_analysed'],inplace=True)     \n",
    "\n",
    "    term = 'oxygen_litres'.lower()\n",
    "\n",
    "    if(len(news_features_data) > 0):\n",
    "        #news_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        value_array = news_features_data['observation_valuetext_analysed'].dropna().astype(float)\n",
    "        news_features[f'news_{term}_mean'] = value_array.mean()\n",
    "        news_features[f'news_{term}_median'] = value_array.median()\n",
    "        news_features[f'news_{term}_std'] = value_array.std()\n",
    "        news_features[f'news_{term}_max'] = max(value_array)\n",
    "        news_features[f'news_{term}_min'] = min(value_array)\n",
    "        news_features[f'news_{term}_n'] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features[f'news_{term}_mean'] = np.nan\n",
    "        news_features[f'news_{term}_median'] = np.nan\n",
    "        news_features[f'news_{term}_std'] = np.nan\n",
    "        news_features[f'news_{term}_max'] = np.nan\n",
    "        news_features[f'news_{term}_min'] = np.nan\n",
    "        news_features[f'news_{term}_n'] = np.nan  \n",
    "\n",
    "    #-----------------------------------------------------------------\n",
    "\n",
    "    news_features_data = current_pat_raw_news[current_pat_raw_news['obscatalogmasteritem_displayname']=='NEWS Oxygen Delivery'].copy()\n",
    "    #news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    news_features_data.dropna(subset=['observation_valuetext_analysed'],inplace=True)     \n",
    "\n",
    "    term = 'oxygen_delivery'.lower()\n",
    "\n",
    "    if(len(news_features_data) > 0):\n",
    "        #news_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        value_array = news_features_data['observation_valuetext_analysed'].dropna().astype(float)\n",
    "        news_features[f'news_{term}_mean'] = value_array.mean()\n",
    "        news_features[f'news_{term}_median'] = value_array.median()\n",
    "        news_features[f'news_{term}_std'] = value_array.std()\n",
    "        news_features[f'news_{term}_max'] = max(value_array)\n",
    "        news_features[f'news_{term}_min'] = min(value_array)\n",
    "        news_features[f'news_{term}_n'] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features[f'news_{term}_mean'] = np.nan\n",
    "        news_features[f'news_{term}_median'] = np.nan\n",
    "        news_features[f'news_{term}_std'] = np.nan\n",
    "        news_features[f'news_{term}_max'] = np.nan\n",
    "        news_features[f'news_{term}_min'] = np.nan\n",
    "        news_features[f'news_{term}_n'] = np.nan      \n",
    "\n",
    "\n",
    "    #-----------------------------------------------------------------\n",
    "\n",
    "    news_features_data = current_pat_raw_news[current_pat_raw_news['obscatalogmasteritem_displayname']=='NEWS Oxygen Delivery'].copy()\n",
    "    #news_features_data =  news_features_data[(news_features_data['observation_valuetext_analysed'].astype(float)<20)& (news_features_data['observation_valuetext_analysed'].astype(float)>-20)].copy()\n",
    "    news_features_data.dropna(subset=['observation_valuetext_analysed'],inplace=True)    \n",
    "\n",
    "    term = 'oxygen_delivery'.lower()\n",
    "\n",
    "    if(len(news_features_data) > 0):\n",
    "        #news_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        value_array = news_features_data['observation_valuetext_analysed'].astype(float)\n",
    "        news_features[f'news_{term}_mean'] = value_array.mean()\n",
    "        news_features[f'news_{term}_median'] = value_array.median()\n",
    "        news_features[f'news_{term}_std'] = value_array.std()\n",
    "        news_features[f'news_{term}_max'] = max(value_array)\n",
    "        news_features[f'news_{term}_min'] = min(value_array)\n",
    "        news_features[f'news_{term}_n'] = value_array.shape[0]\n",
    "    else:\n",
    "        news_features[f'news_{term}_mean'] = np.nan\n",
    "        news_features[f'news_{term}_median'] = np.nan\n",
    "        news_features[f'news_{term}_std'] = np.nan\n",
    "        news_features[f'news_{term}_max'] = np.nan\n",
    "        news_features[f'news_{term}_min'] = np.nan\n",
    "        news_features[f'news_{term}_n'] = np.nan     \n",
    "\n",
    "    return news_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3859c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bmi_features(current_pat_client_id_code,target_date_range, pat_batch):\n",
    "    start_year, start_month, end_year, end_month = get_start_end_year_month(target_date_range)\n",
    "    \n",
    "    \n",
    "    if(batch_mode):\n",
    "        current_pat_raw_bmi = filter_dataframe_by_timestamp(pat_batch, start_year, start_month, end_year, end_month, 'observationdocument_recordeddtm')\n",
    "        \n",
    "    else:\n",
    "        current_pat_raw_bmi = cohort_searcher_with_terms_and_search(index_name=\"observations\", \n",
    "                                                               fields_list=\"\"\"observation_guid client_idcode\tobscatalogmasteritem_displayname\tobservation_valuetext_analysed\tobservationdocument_recordeddtm clientvisit_visitidcode\"\"\".split(),\n",
    "                                                               term_name=\"client_idcode.keyword\", \n",
    "                                                               entered_list=[current_pat_client_id_code], \n",
    "                                                               search_string=\"obscatalogmasteritem_displayname:(\\\"OBS BMI\\\" OR \\\"OBS Weight\\\" OR \\\"OBS height\\\") AND \" + f'observationdocument_recordeddtm:[{start_year}-{start_month} TO {end_year}-{end_month}]')\n",
    "\n",
    "\n",
    "    if(len(current_pat_raw_bmi[current_pat_raw_bmi['obscatalogmasteritem_displayname']=='OBS BMI Calculation']))==0:\n",
    "        bmi_features = pd.DataFrame(data = [current_pat_client_id_code], columns=['client_idcode'])\n",
    "\n",
    "\n",
    "    #get bmi features\n",
    "    #set client id to current pat\n",
    "    bmi_sample = current_pat_raw_bmi[current_pat_raw_bmi['obscatalogmasteritem_displayname']=='OBS BMI Calculation']\n",
    "    #screen and purge dud values\n",
    "    bmi_sample =  bmi_sample[(bmi_sample['observation_valuetext_analysed'].astype(float)<200)& (bmi_sample['observation_valuetext_analysed'].astype(float)>6)]\n",
    "\n",
    "\n",
    "    if(len(bmi_sample) > 0):\n",
    "        bmi_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "\n",
    "        #if(len(current_pat_raw_bmi)>0):\n",
    "\n",
    "        value_array = bmi_sample['observation_valuetext_analysed'].astype(float)\n",
    "\n",
    "        bmi_features['bmi_mean'] = value_array.mean()\n",
    "        bmi_features['bmi_median'] = value_array.median()\n",
    "        bmi_features['bmi_std'] = value_array.std()\n",
    "        bmi_features['bmi_high'] = int(bool(value_array.median() > 24.9))\n",
    "        bmi_features['bmi_low'] = int(bool(value_array.median() < 18.5))\n",
    "        bmi_features['bmi_extreme'] = int(bool(value_array.median() > 30))\n",
    "        bmi_features['bmi_max'] = max(value_array)\n",
    "        bmi_features['bmi_min'] = min(value_array)\n",
    "\n",
    "    else:\n",
    "        bmi_features = pd.DataFrame(data = [current_pat_client_id_code] , columns =['client_idcode']).copy()\n",
    "        bmi_features['bmi_mean'] = np.nan\n",
    "        bmi_features['bmi_median'] = np.nan\n",
    "        bmi_features['bmi_std'] = np.nan\n",
    "        bmi_features['bmi_high'] = np.nan\n",
    "        bmi_features['bmi_low'] = np.nan\n",
    "        bmi_features['bmi_extreme'] = np.nan\n",
    "        bmi_features['bmi_max'] = np.nan\n",
    "        bmi_features['bmi_min'] = np.nan\n",
    "\n",
    "    height_sample = current_pat_raw_bmi[current_pat_raw_bmi['obscatalogmasteritem_displayname']=='OBS Height']\n",
    "    #screen and purge dud values\n",
    "    height_sample = height_sample[(height_sample['observation_valuetext_analysed'].astype(float)<300)& (height_sample['observation_valuetext_analysed'].astype(float)>30)]    \n",
    "\n",
    "\n",
    "\n",
    "    #get height features\n",
    "    if(len(height_sample) > 0):\n",
    "\n",
    "\n",
    "\n",
    "        if(len(current_pat_raw_bmi)>0):\n",
    "\n",
    "            value_array = height_sample['observation_valuetext_analysed'].astype(float)\n",
    "\n",
    "            bmi_features['height_mean'] = value_array.mean()\n",
    "            bmi_features['height_median'] = value_array.median()\n",
    "            bmi_features['height_std'] = value_array.std()\n",
    "\n",
    "    else:\n",
    "        bmi_features['height_mean'] = np.nan\n",
    "        bmi_features['height_median'] = np.nan\n",
    "        bmi_features['height_std'] = np.nan\n",
    "\n",
    "\n",
    "    weight_sample = current_pat_raw_bmi[current_pat_raw_bmi['obscatalogmasteritem_displayname']=='OBS Weight']\n",
    "    #screen and purge dud values\n",
    "    weight_sample = weight_sample[(weight_sample['observation_valuetext_analysed'].astype(float)<800)& (weight_sample['observation_valuetext_analysed'].astype(float)>1)]    \n",
    "\n",
    "\n",
    "    #get weight features\n",
    "    if(len(weight_sample) > 0):\n",
    "\n",
    "        if(len(current_pat_raw_bmi)>0):\n",
    "\n",
    "            value_array = weight_sample['observation_valuetext_analysed'].astype(float)\n",
    "\n",
    "            bmi_features['weight_mean'] = value_array.mean()\n",
    "            bmi_features['weight_median'] = value_array.median()\n",
    "            bmi_features['weight_std'] = value_array.std()\n",
    "            bmi_features['weight_max'] = max(value_array)\n",
    "            bmi_features['weight_min'] = min(value_array)\n",
    "\n",
    "    else:\n",
    "        bmi_features['weight_mean'] = np.nan\n",
    "        bmi_features['weight_median'] = np.nan\n",
    "        bmi_features['weight_std'] = np.nan\n",
    "        bmi_features['weight_max'] = np.nan\n",
    "        bmi_features['weight_min'] = np.nan\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    return bmi_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeccb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_pat_client_id_code = '%testclientidcode%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3dcb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 1995\n",
    "start_month = '1'\n",
    "end_year = 2023\n",
    "end_month = '11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f693a11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_searcher_with_terms_and_search(index_name = \"basic_observations\", \n",
    "                                                            fields_list = [\"client_idcode\", \"basicobs_itemname_analysed\", \"basicobs_value_numeric\", \"basicobs_entered\", \"clientvisit_serviceguid\"], \n",
    "                                                            term_name = \"client_idcode.keyword\", \n",
    "                                                            entered_list = [current_pat_client_id_code], \n",
    "                                                            search_string = \"basicobs_value_numeric:* AND \"+ f'updatetime:[{start_year}-{start_month} TO {end_year}-{end_month}] ') #In kibana can we pull the mean and std of each blood test for a reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d09f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36dce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020e39c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_pat_bloods(current_pat_client_id_code, target_date_range, pat_batch):\n",
    "    \n",
    "    start_year, start_month, end_year, end_month = get_start_end_year_month(target_date_range)\n",
    "    \n",
    "    if(batch_mode):\n",
    "        current_pat_bloods = filter_dataframe_by_timestamp(pat_batch, start_year, start_month, end_year, end_month, 'basicobs_entered')\n",
    "        \n",
    "    else:\n",
    "        current_pat_bloods = cohort_searcher_with_terms_and_search(index_name = \"basic_observations\", \n",
    "                                                            fields_list = [\"client_idcode\", \"basicobs_itemname_analysed\", \"basicobs_value_numeric\", \"basicobs_entered\", \"clientvisit_serviceguid\"], \n",
    "                                                            term_name = \"client_idcode.keyword\", \n",
    "                                                            entered_list = [current_pat_client_id_code], \n",
    "                                                            search_string = \"basicobs_value_numeric:* AND \"+ f'updatetime:[{start_year}-{start_month} TO {end_year}-{end_month}] ') #In kibana can we pull the mean and std of each blood test for a reference.\n",
    "\n",
    "\n",
    "    if(batch_mode):\n",
    "        current_pat_bloods['datetime'] = current_pat_bloods['basicobs_entered'].copy()\n",
    "        \n",
    "    else:\n",
    "        current_pat_bloods['datetime'] = pd.Series(current_pat_bloods['basicobs_entered']).apply(convert_date)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    basicobs_itemname_analysed_list = list(current_pat_bloods['basicobs_itemname_analysed'].unique())\n",
    "\n",
    "    basicobs_itemname_analysed_df_dict = {elem: current_pat_bloods[current_pat_bloods.basicobs_itemname_analysed == elem] for elem in basicobs_itemname_analysed_list}\n",
    "\n",
    "    df_unique = current_pat_bloods.copy()\n",
    "\n",
    "    df_unique.drop_duplicates(subset = 'client_idcode', inplace=True)\n",
    "\n",
    "    df_unique.reset_index(inplace=True)\n",
    "\n",
    "    filtered_list = []\n",
    "\n",
    "    obs_columns_list = basicobs_itemname_analysed_list \n",
    "\n",
    "    obs_columns_set = list(set(obs_columns_list))\n",
    "\n",
    "    filtered_column_list = filtered_list\n",
    "\n",
    "    #print(\"Building obs_columns_set_columns_for_df\")\n",
    "\n",
    "    obs_columns_set_columns_for_df = []\n",
    "    for i in range(0, len(obs_columns_set)):\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i]+\"_mean\")\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i]+\"_median\")\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i]+\"_mode\")\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i]+\"_std\")\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i]+\"_num-tests\")\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i]+\"_days-since-last-test\")\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i]+\"_max\")\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i]+\"_min\")\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i]+\"_most-recent\")\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i]+\"_earliest-test\")\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i]+\"_days-between-first-last\")\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i]+\"_contains-extreme-low\")\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i]+\"_contains-extreme-high\")\n",
    "\n",
    "    orig_columns = list(df_unique.columns)\n",
    "\n",
    "    comb_cols = orig_columns + obs_columns_set_columns_for_df\n",
    "\n",
    "    df_unique = df_unique.reindex(comb_cols, axis=1)  \n",
    "\n",
    "    df_unique = df_unique.copy()\n",
    "    df_unique.drop(['index','_index', '_id', '_score',\n",
    "           'basicobs_itemname_analysed', 'basicobs_value_numeric',\n",
    "           'basicobs_entered', 'clientvisit_serviceguid', 'datetime'], inplace=True, axis=1)\n",
    "\n",
    "\n",
    "    if(batch_mode):\n",
    "        \n",
    "        today = datetime.now(timezone.utc)\n",
    "        \n",
    "    else:\n",
    "        today = datetime.today()\n",
    "        \n",
    "\n",
    "    clients_id = current_pat_client_id_code \n",
    "\n",
    "    df_unique_filtered = df_unique.copy()\n",
    "\n",
    "    i= 0\n",
    "\n",
    "    for j in (range(0, len(basicobs_itemname_analysed_list))):\n",
    "        col_name = basicobs_itemname_analysed_list[j]\n",
    "\n",
    "        filtered_df = basicobs_itemname_analysed_df_dict.get(col_name)\n",
    "\n",
    "        filtered_column_values = filtered_df.basicobs_value_numeric.astype(float)._get_numeric_data()\n",
    "\n",
    "        df_len = len(filtered_df)\n",
    "        #print(f\"df_len {df_len}\")\n",
    "        if(df_len>=1):\n",
    "                #Mean assurance*\n",
    "                agg_val = float(filtered_column_values.values[0])\n",
    "\n",
    "                df_unique_filtered.at[i,col_name+\"_mean\"] = agg_val\n",
    "\n",
    "        if(df_len>=2):\n",
    "            #try:\n",
    "            #Mean\n",
    "            agg_val = filtered_column_values.mean()\n",
    "\n",
    "            df_unique_filtered.at[i,col_name+\"_mean\"] = agg_val\n",
    "\n",
    "\n",
    "            #recent\n",
    "            agg_val = filtered_df.sort_values(by='datetime').iloc[-1]['basicobs_value_numeric']\n",
    "\n",
    "            df_unique_filtered.at[i,col_name+\"_most-recent\"] = agg_val\n",
    "\n",
    "\n",
    "            #earliest-test\n",
    "            agg_val = filtered_df.sort_values(by='datetime').iloc[0]['basicobs_value_numeric']\n",
    "            df_unique_filtered.at[i,col_name+\"_earliest-test\"] = agg_val\n",
    "\n",
    "\n",
    "            #days-since-last-test\n",
    "            date_object = filtered_df.sort_values(by='datetime').iloc[-1]['datetime']\n",
    "\n",
    "            delta = today - date_object\n",
    "\n",
    "            agg_val = delta.days\n",
    "\n",
    "            df_unique_filtered.at[i,col_name+\"_days-since-last-test\"] = agg_val\n",
    "\n",
    "            #n tests\n",
    "\n",
    "            agg_val = len(filtered_column_values)\n",
    "\n",
    "            df_unique_filtered.at[i,col_name+\"_num-tests\"] = agg_val\n",
    "\n",
    "\n",
    "\n",
    "        if(df_len>=3):\n",
    "           # try:\n",
    "            #median\n",
    "            #agg_val = np.median(filtered_df['basicobs_value_numeric'].to_numpy())\n",
    "            agg_val = filtered_column_values.median()                \n",
    "            df_unique_filtered.at[i,col_name+\"_median\"] = agg_val\n",
    "\n",
    "            #mode\n",
    "            #agg_val = stats.mode(filtered_df['basicobs_value_numeric'].to_numpy(), keepdims=True)[0][0]\n",
    "\n",
    "            agg_val = stats.mode(filtered_column_values)[0][0]\n",
    "            df_unique_filtered.at[i,col_name+\"_mode\"] = agg_val\n",
    "\n",
    "\n",
    "            #std\n",
    "            agg_val = filtered_column_values.std()                      \n",
    "            df_unique_filtered.at[i,col_name+\"_std\"] = agg_val\n",
    "\n",
    "\n",
    "            #min\n",
    "            agg_val = min(filtered_column_values)             \n",
    "            df_unique_filtered.at[i,col_name+\"_min\"] = agg_val\n",
    "\n",
    "            #max\n",
    "            agg_val = max(filtered_column_values)\n",
    "            df_unique_filtered.at[i,col_name+\"_max\"] = agg_val\n",
    "\n",
    "            #contains extreme low\n",
    "            col_name_mean = basicobs_itemname_analysed_df_dict.get(col_name).basicobs_value_numeric._get_numeric_data().mean()\n",
    "            col_name_std  = basicobs_itemname_analysed_df_dict.get(col_name).basicobs_value_numeric._get_numeric_data().std()\n",
    "\n",
    "            col_name_low  = col_name_mean - (col_name_std*3)\n",
    "\n",
    "            agg_val = int(float(min(filtered_column_values))< col_name_low)\n",
    "            df_unique_filtered.at[i,col_name+\"_contains-extreme-low\"] = agg_val\n",
    "\n",
    "\n",
    "            #contains extreme high   \n",
    "            col_name_high = col_name_mean + (col_name_std*3)\n",
    "\n",
    "\n",
    "            agg_val = int(float(max(filtered_column_values))> col_name_high)\n",
    "\n",
    "            df_unique_filtered.at[i,col_name+\"_contains-extreme-high\"] = agg_val\n",
    "\n",
    "            #days_between earliest and last\n",
    "\n",
    "            earliest = filtered_df.sort_values(by='datetime').iloc[-1]['datetime']\n",
    "\n",
    "            oldest = filtered_df.sort_values(by='datetime').iloc[-1]['datetime']\n",
    "\n",
    "            delta = earliest - oldest\n",
    "\n",
    "            agg_val = delta.days\n",
    "\n",
    "            df_unique_filtered.at[i,col_name+\"_days-between-first-last\"] = agg_val     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #current_pat_bloods = df_unique_filtered\n",
    "    return df_unique_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e98cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_pat_drugs(current_pat_client_id_code, target_date_range, pat_batch):\n",
    "    start_year, start_month, end_year, end_month = get_start_end_year_month(target_date_range)\n",
    "    \n",
    "    \n",
    "    #Drugs\n",
    "    if(batch_mode):\n",
    "        drugs = filter_dataframe_by_timestamp(pat_batch, start_year, start_month, end_year, end_month, 'order_entered')\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        drugs = cohort_searcher_with_terms_and_search(index_name=\"order\", \n",
    "                                              fields_list = \"\"\"client_idcode order_guid\torder_name\torder_summaryline order_holdreasontext\torder_entered clientvisit_visitidcode\"\"\".split(),\n",
    "                                              term_name = \"client_idcode.keyword\", \n",
    "                                              entered_list = [current_pat_client_id_code], \n",
    "                                              search_string = \"order_typecode:\\\"medication\\\" AND \"+ f'updatetime:[{start_year}-{start_month} TO {end_year}-{end_month}] ')\n",
    "\n",
    "\n",
    "\n",
    "    current_pat_diagnostics = drugs.copy()\n",
    "\n",
    "    \n",
    "\n",
    "    if(batch_mode):\n",
    "        current_pat_diagnostics['datetime'] = current_pat_diagnostics['order_entered'].copy()\n",
    "        \n",
    "    else:\n",
    "        current_pat_diagnostics['datetime'] = pd.Series(current_pat_diagnostics['order_entered']).dropna().apply(convert_date)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    order_name_list = list(current_pat_diagnostics['order_name'].unique())\n",
    "\n",
    "    order_name_df_dict = {elem: current_pat_diagnostics[current_pat_diagnostics.order_name == elem] for elem in order_name_list}\n",
    "\n",
    "    df_unique = current_pat_diagnostics.copy()\n",
    "\n",
    "    df_unique.drop_duplicates(subset = 'client_idcode', inplace=True)\n",
    "\n",
    "    df_unique.reset_index(inplace=True)\n",
    "\n",
    "    filtered_list = []\n",
    "\n",
    "    obs_columns_list = order_name_list \n",
    "\n",
    "    obs_columns_set = list(set(obs_columns_list))\n",
    "\n",
    "    filtered_column_list = filtered_list\n",
    "\n",
    "    #print(\"Building obs_columns_set_columns_for_df\")\n",
    "\n",
    "    obs_columns_set_columns_for_df = []\n",
    "    for i in range(0, len(obs_columns_set)):\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i]+\"_num-drug-order\")\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i]+\"_days-since-last-drug-order\")\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i]+\"_days-between-first-last-drug\")\n",
    "\n",
    "    orig_columns = list(df_unique.columns)\n",
    "\n",
    "    comb_cols = orig_columns + obs_columns_set_columns_for_df\n",
    "\n",
    "    df_unique = df_unique.reindex(comb_cols, axis=1)  \n",
    "\n",
    "    df_unique = df_unique.copy()\n",
    "    df_unique.drop(['_index', '_id', '_score', 'order_guid', 'order_name',\n",
    "           'order_summaryline', 'order_holdreasontext', 'order_entered',\n",
    "           'clientvisit_visitidcode'], inplace=True, axis=1)\n",
    "\n",
    "\n",
    "    if(batch_mode):\n",
    "        \n",
    "        today = datetime.now(timezone.utc)\n",
    "        \n",
    "    else:\n",
    "        today = datetime.today()\n",
    "\n",
    "    clients_id = current_pat_client_id_code \n",
    "\n",
    "    df_unique_filtered = df_unique.copy()\n",
    "\n",
    "    i= 0\n",
    "\n",
    "    for j in (range(0, len(obs_columns_list))):\n",
    "        col_name = obs_columns_list[j]\n",
    "\n",
    "        filtered_df = order_name_df_dict.get(col_name)\n",
    "\n",
    "        df_len = len(filtered_df)\n",
    "        if(df_len>=1):\n",
    "            #n tests\n",
    "\n",
    "            agg_val = len(filtered_df)\n",
    "\n",
    "            df_unique_filtered.at[i,col_name+\"_num-drug-order\"] = agg_val\n",
    "\n",
    "            #days-since-last-test\n",
    "            date_object = filtered_df.sort_values(by='datetime').iloc[-1]['datetime']\n",
    "\n",
    "            delta = today - date_object\n",
    "\n",
    "            agg_val = delta.days\n",
    "\n",
    "            df_unique_filtered.at[i,col_name+\"_days-since-last-drug-order\"] = agg_val\n",
    "\n",
    "\n",
    "        if(df_len>=2):\n",
    "\n",
    "\n",
    "            #days_between earliest and last\n",
    "\n",
    "            earliest = filtered_df.sort_values(by='datetime').iloc[-1]['datetime']\n",
    "\n",
    "            oldest = filtered_df.sort_values(by='datetime').iloc[-1]['datetime']\n",
    "\n",
    "            delta = earliest - oldest\n",
    "\n",
    "            agg_val = delta.days\n",
    "\n",
    "            df_unique_filtered.at[i,col_name+\"_days-between-first-last-drug\"] = agg_val     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return df_unique_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11306fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_pat_diagnostics(current_pat_client_id_code,target_date_range, pat_batch):\n",
    "    \n",
    "    start_year, start_month, end_year, end_month = get_start_end_year_month(target_date_range)\n",
    "    \n",
    "    \n",
    "    #Diagnostic tests\n",
    "    if(batch_mode):\n",
    "        diagnostics = filter_dataframe_by_timestamp(pat_batch, start_year, start_month, end_year, end_month, 'order_entered')\n",
    "        \n",
    "    else:\n",
    "        diagnostics = cohort_searcher_with_terms_and_search(index_name=\"order\", \n",
    "                                          fields_list = \"\"\"client_idcode order_guid\torder_name\torder_summaryline order_holdreasontext\torder_entered clientvisit_visitidcode\"\"\".split(),\n",
    "                                          term_name = \"client_idcode.keyword\", \n",
    "                                          entered_list = [current_pat_client_id_code], \n",
    "                                          search_string = \"order_typecode:\\\"diagnostic\\\"AND \"+ f'updatetime:[{start_year}-{start_month} TO {end_year}-{end_month}] ')\n",
    "\n",
    "\n",
    "    current_pat_diagnostics = diagnostics.copy()\n",
    "\n",
    "\n",
    "    if(batch_mode):\n",
    "        current_pat_diagnostics['datetime'] = current_pat_diagnostics['order_entered'].copy()\n",
    "        \n",
    "    else:\n",
    "        current_pat_diagnostics['datetime'] = pd.Series(current_pat_diagnostics['order_entered']).dropna().apply(convert_date)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    order_name_list = list(current_pat_diagnostics['order_name'].unique())\n",
    "\n",
    "    order_name_df_dict = {elem: current_pat_diagnostics[current_pat_diagnostics.order_name == elem] for elem in order_name_list}\n",
    "\n",
    "    df_unique = current_pat_diagnostics.copy()\n",
    "\n",
    "    df_unique.drop_duplicates(subset = 'client_idcode', inplace=True)\n",
    "\n",
    "    df_unique.reset_index(inplace=True)\n",
    "\n",
    "    filtered_list = []\n",
    "\n",
    "    obs_columns_list = order_name_list \n",
    "\n",
    "    obs_columns_set = list(set(obs_columns_list))\n",
    "\n",
    "    filtered_column_list = filtered_list\n",
    "\n",
    "    #print(\"Building obs_columns_set_columns_for_df\")\n",
    "\n",
    "    obs_columns_set_columns_for_df = []\n",
    "    for i in range(0, len(obs_columns_set)):\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i]+\"_num-diagnostic-order\")\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i]+\"_days-since-last-diagnostic-order\")\n",
    "        obs_columns_set_columns_for_df.append(obs_columns_set[i]+\"_days-between-first-last-diagnostic\")\n",
    "\n",
    "    orig_columns = list(df_unique.columns)\n",
    "\n",
    "    comb_cols = orig_columns + obs_columns_set_columns_for_df\n",
    "\n",
    "    df_unique = df_unique.reindex(comb_cols, axis=1)  \n",
    "\n",
    "    df_unique = df_unique.copy()\n",
    "    df_unique.drop(['_index', '_id', '_score', 'order_guid', 'order_name',\n",
    "           'order_summaryline', 'order_holdreasontext', 'order_entered',\n",
    "           'clientvisit_visitidcode'], inplace=True, axis=1)\n",
    "\n",
    "\n",
    "    if(batch_mode):\n",
    "        \n",
    "        today = datetime.now(timezone.utc)\n",
    "        \n",
    "    else:\n",
    "        today = datetime.today()\n",
    "\n",
    "    clients_id = current_pat_client_id_code \n",
    "\n",
    "    df_unique_filtered = df_unique.copy()\n",
    "\n",
    "    i= 0\n",
    "\n",
    "    for j in (range(0, len(obs_columns_list))):\n",
    "        col_name = obs_columns_list[j]\n",
    "\n",
    "        filtered_df = order_name_df_dict.get(col_name)\n",
    "\n",
    "        df_len = len(filtered_df)\n",
    "        if(df_len>=1):\n",
    "            #n tests\n",
    "\n",
    "            agg_val = len(filtered_df)\n",
    "\n",
    "            df_unique_filtered.at[i,col_name+\"_num-diagnostic-order\"] = agg_val\n",
    "\n",
    "            #days-since-last-test\n",
    "            date_object = filtered_df.sort_values(by='datetime').iloc[-1]['datetime']\n",
    "\n",
    "            delta = today - date_object\n",
    "\n",
    "            agg_val = delta.days\n",
    "\n",
    "            df_unique_filtered.at[i,col_name+\"_days-since-last-diagnostic-order\"] = agg_val\n",
    "\n",
    "\n",
    "        if(df_len>=2):\n",
    "\n",
    "            #days_between earliest and last\n",
    "\n",
    "            earliest = filtered_df.sort_values(by='datetime').iloc[-1]['datetime']\n",
    "\n",
    "            oldest = filtered_df.sort_values(by='datetime').iloc[-1]['datetime']\n",
    "\n",
    "            delta = earliest - oldest\n",
    "\n",
    "            agg_val = delta.days\n",
    "\n",
    "            df_unique_filtered.at[i,col_name+\"_days-between-first-last-diagnostic\"] = agg_val     \n",
    "\n",
    "\n",
    "    return df_unique_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f266ccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pat_batch_obs(current_pat_client_id_code, search_term):\n",
    "    \n",
    "\n",
    "    batch_target = cohort_searcher_with_terms_and_search(index_name=\"observations\", \n",
    "                                                                       fields_list=\"\"\"observation_guid client_idcode\tobscatalogmasteritem_displayname\tobservation_valuetext_analysed\tobservationdocument_recordeddtm clientvisit_visitidcode\"\"\".split(),\n",
    "                                                                       term_name=\"client_idcode.keyword\", \n",
    "                                                                       entered_list=[current_pat_client_id_code], \n",
    "                                                                     search_string=f\"obscatalogmasteritem_displayname:(\\\"{search_term}\\\") AND \" + f'observationdocument_recordeddtm:[{global_start_year}-{global_start_month} TO {global_end_year}-{global_end_month}]')\n",
    "\n",
    "    return batch_target\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9101d343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pat_batch_news(current_pat_client_id_code, search_term):\n",
    "    batch_target = cohort_searcher_with_terms_and_search(index_name=\"observations\", \n",
    "                                                                   fields_list=\"\"\"observation_guid client_idcode\tobscatalogmasteritem_displayname\tobservation_valuetext_analysed\tobservationdocument_recordeddtm clientvisit_visitidcode\"\"\".split(),\n",
    "                                                                   term_name=\"client_idcode.keyword\", \n",
    "                                                                   entered_list=[current_pat_client_id_code], \n",
    "                                                                   search_string='obscatalogmasteritem_displayname:(\\\"NEWS\\\" OR \\\"NEWS2\\\") AND ' + f'observationdocument_recordeddtm:[{global_start_year}-{global_start_month} TO {global_end_year}-{global_end_month}]')\n",
    "\n",
    "    return batch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adeda90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pat_batch_bmi(current_pat_client_id_code, search_term):\n",
    "    batch_target = cohort_searcher_with_terms_and_search(index_name=\"observations\", \n",
    "                                                               fields_list=\"\"\"observation_guid client_idcode\tobscatalogmasteritem_displayname\tobservation_valuetext_analysed\tobservationdocument_recordeddtm clientvisit_visitidcode\"\"\".split(),\n",
    "                                                               term_name=\"client_idcode.keyword\", \n",
    "                                                               entered_list=[current_pat_client_id_code], \n",
    "                                                               search_string=\"obscatalogmasteritem_displayname:(\\\"OBS BMI\\\" OR \\\"OBS Weight\\\" OR \\\"OBS height\\\") AND \" + f'observationdocument_recordeddtm:[{global_start_year}-{global_start_month} TO {global_end_year}-{global_end_month}]')\n",
    "\n",
    "    return batch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ec85ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pat_batch_bloods(current_pat_client_id_code, search_term):\n",
    "    batch_target = cohort_searcher_with_terms_and_search(index_name = \"basic_observations\", \n",
    "                                                            fields_list = [\"client_idcode\", \"basicobs_itemname_analysed\", \"basicobs_value_numeric\", \"basicobs_entered\", \"clientvisit_serviceguid\"], \n",
    "                                                            term_name = \"client_idcode.keyword\", \n",
    "                                                            entered_list = [current_pat_client_id_code], \n",
    "                                                            search_string = \"basicobs_value_numeric:* AND \"+ f'updatetime:[{global_start_year}-{global_start_month} TO {global_end_year}-{global_end_month}] ') #In kibana can we pull the mean and std of each blood test for a reference.\n",
    "\n",
    "    return batch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c19fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pat_batch_drugs(current_pat_client_id_code, search_term):\n",
    "    batch_target = cohort_searcher_with_terms_and_search(index_name=\"order\", \n",
    "                                              fields_list = \"\"\"client_idcode order_guid\torder_name\torder_summaryline order_holdreasontext\torder_entered clientvisit_visitidcode\"\"\".split(),\n",
    "                                              term_name = \"client_idcode.keyword\", \n",
    "                                              entered_list = [current_pat_client_id_code], \n",
    "                                              search_string = \"order_typecode:\\\"medication\\\" AND \"+ f'updatetime:[{global_start_year}-{global_start_month} TO {global_end_year}-{global_end_month}] ')\n",
    "\n",
    "    return batch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1380ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pat_batch_diagnostics(current_pat_client_id_code, search_term):\n",
    "    batch_target = cohort_searcher_with_terms_and_search(index_name=\"order\", \n",
    "                                          fields_list = \"\"\"client_idcode order_guid\torder_name\torder_summaryline order_holdreasontext\torder_entered clientvisit_visitidcode\"\"\".split(),\n",
    "                                          term_name = \"client_idcode.keyword\", \n",
    "                                          entered_list = [current_pat_client_id_code], \n",
    "                                          search_string = \"order_typecode:\\\"diagnostic\\\"AND \"+ f'updatetime:[{global_start_year}-{global_start_month} TO {global_end_year}-{global_end_month}] ')\n",
    "\n",
    "    return batch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786eb093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pat_batch_epr_docs(current_pat_client_id_code, search_term):\n",
    "    batch_target = cohort_searcher_with_terms_and_search(index_name=\"epr_documents\", \n",
    "                                                                      fields_list = \"\"\"client_idcode document_guid\tdocument_description\tbody_analysed updatetime clientvisit_visitidcode\"\"\".split(),\n",
    "                                                                      term_name = \"client_idcode.keyword\", \n",
    "                                                                      entered_list = [current_pat_client_id_code],\n",
    "                                                                    search_string = f'updatetime:[{global_start_year}-{global_start_month} TO {global_end_year}-{global_end_month}] ')\n",
    "    return batch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422037bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pat_batch_mct_docs(current_pat_client_id_code, search_term):\n",
    "    batch_target = cohort_searcher_with_terms_and_search(index_name=\"observations\", \n",
    "                                                                       fields_list=\"\"\"observation_guid client_idcode\tobscatalogmasteritem_displayname\tobservation_valuetext_analysed\tobservationdocument_recordeddtm clientvisit_visitidcode\"\"\".split(),\n",
    "                                                                       term_name=\"client_idcode.keyword\", \n",
    "                                                                       entered_list=[current_pat_client_id_code], \n",
    "                                                                       search_string=\"obscatalogmasteritem_displayname:(\\\"AoMRC_ClinicalSummary_FT\\\") AND \" + f'observationdocument_recordeddtm:[{global_start_year}-{global_start_month} TO {global_end_year}-{global_end_month}]')\n",
    "    return batch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d0a423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pat_batch_demo(current_pat_client_id_code, search_term):\n",
    "    batch_target =cohort_searcher_with_terms_and_search(index_name=\"epr_documents\", \n",
    "                                             fields_list=[\"client_idcode\", \"client_firstname\", \"client_lastname\", \"client_dob\", \"client_gendercode\", \"client_racecode\", \"client_deceaseddtm\", \"updatetime\"], \n",
    "                                             term_name=\"client_idcode.keyword\", \n",
    "                                             entered_list=[current_pat_client_id_code],\n",
    "                                            search_string= f'updatetime:[{global_start_year}-{global_start_month} TO {global_end_year}-{global_end_month}] '\n",
    "                                                )\n",
    "    return batch_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3107b6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for k in tqdm(range(0,len(all_patient_list))):\n",
    "#for k in tqdm(range(0,4)):   \n",
    "def main(current_pat_client_id_code, target_date_range):\n",
    "    global skipped_counter\n",
    "    global start_time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    done_list = []\n",
    "    if(current_pat_client_id_code+\"_\"+str(target_date_range) not in stripped_list_start):\n",
    "        \n",
    "        \n",
    "        stripped_list = [x.replace(\".csv\",\"\") for x in list_dir_wrapper(current_pat_lines_path)]\n",
    "\n",
    "        if(current_pat_client_id_code + \"_\" + str(target_date_range) not in stripped_list):\n",
    "            #print(start_time, current_pat_client_id_code)\n",
    "            try:\n",
    "                #current_pat_client_id_code = all_patient_list[k]\n",
    "\n",
    "                patient_vector = []\n",
    "                \n",
    "                p_bar_entry = current_pat_client_id_code + \"_\" + str(target_date_range)\n",
    "                #\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 0, 'demo')\n",
    "\n",
    "                \n",
    "                if(main_options.get('demo')):\n",
    "                    current_pat_demo = get_demo(current_pat_client_id_code, target_date_range)\n",
    "                    patient_vector.append(current_pat_demo)\n",
    "\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 1, 'bmi')\n",
    "\n",
    "                if(main_options.get('bmi')):\n",
    "                    bmi_features = get_bmi_features(current_pat_client_id_code, target_date_range)\n",
    "                    patient_vector.append(bmi_features)\n",
    "\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 2, 'bloods')\n",
    "\n",
    "                if(main_options.get('bloods')):\n",
    "                    current_pat_bloods = get_current_pat_bloods(current_pat_client_id_code, target_date_range)\n",
    "                    patient_vector.append(current_pat_bloods)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 3, 'drugs')\n",
    "\n",
    "                if(main_options.get('drugs')):\n",
    "                    current_pat_drugs = get_current_pat_drugs(current_pat_client_id_code, target_date_range)\n",
    "                    patient_vector.append(current_pat_drugs)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 4, 'diagnostics')\n",
    "\n",
    "                if(main_options.get('diagnostics')):\n",
    "                    current_pat_diagnostics = get_current_pat_diagnostics(current_pat_client_id_code, target_date_range)\n",
    "                    patient_vector.append(current_pat_diagnostics)\n",
    "\n",
    "                #update_pbar(current_pat_client_id_code, start_time, 5, 'annotations', n_docs_to_annotate)\n",
    "\n",
    "\n",
    "                if(main_options.get('annotations')):\n",
    "                    df_pat_target = get_current_pat_annotations(current_pat_client_id_code, target_date_range)\n",
    "                    patient_vector.append(df_pat_target)\n",
    "                    \n",
    "                if(main_options.get('annotations_mrc')):\n",
    "                    df_pat_target = get_current_pat_annotations_mrc_cs(current_pat_client_id_code, target_date_range)\n",
    "                    patient_vector.append(df_pat_target)\n",
    "                \n",
    "                update_pbar(p_bar_entry, start_time, 1, 'core_02')\n",
    "                \n",
    "                if(main_options.get('core_02')):\n",
    "                    df_pat_target = get_core_02(current_pat_client_id_code, target_date_range)\n",
    "                    patient_vector.append(df_pat_target)\n",
    "                    \n",
    "                update_pbar(p_bar_entry, start_time, 2, 'bed')\n",
    "                    \n",
    "                if(main_options.get('bed')):\n",
    "                    df_pat_target = get_bed(current_pat_client_id_code, target_date_range)\n",
    "                    patient_vector.append(df_pat_target)\n",
    "                    \n",
    "                update_pbar(p_bar_entry, start_time, 3, 'vte_status')\n",
    "                    \n",
    "                if(main_options.get('vte_status')):\n",
    "                    df_pat_target = get_vte_status(current_pat_client_id_code, target_date_range)\n",
    "                    patient_vector.append(df_pat_target)\n",
    "                    \n",
    "                update_pbar(p_bar_entry, start_time, 4, 'hosp_site')    \n",
    "                    \n",
    "                if(main_options.get('hosp_site')):\n",
    "                    df_pat_target = get_hosp_site(current_pat_client_id_code, target_date_range)\n",
    "                    patient_vector.append(df_pat_target)\n",
    "                    \n",
    "                update_pbar(p_bar_entry, start_time, 1, 'core_resus')     \n",
    "                    \n",
    "                if(main_options.get('core_resus')):\n",
    "                    df_pat_target = get_core_resus(current_pat_client_id_code, target_date_range)\n",
    "                    patient_vector.append(df_pat_target)\n",
    "                    \n",
    "                update_pbar(p_bar_entry, start_time, 2, 'news')      \n",
    "                    \n",
    "                if(main_options.get('news')):\n",
    "                    df_pat_target = get_news(current_pat_client_id_code, target_date_range)\n",
    "                    patient_vector.append(df_pat_target)\n",
    "                    \n",
    "                update_pbar(p_bar_entry, start_time, 2, 'concatenating')\n",
    "                \n",
    "                target_date_vector = enum_target_date_vector(target_date_range, current_pat_client_id_code)\n",
    "                \n",
    "                    \n",
    "                patient_vector.append(target_date_vector)\n",
    "                \n",
    "\n",
    "                pat_concatted = pd.concat(patient_vector, axis=1)\n",
    "\n",
    "                pat_concatted.drop('client_idcode', axis=1, inplace=True)\n",
    "\n",
    "                pat_concatted.insert(0, 'client_idcode', current_pat_client_id_code)\n",
    "                \n",
    "                update_pbar(p_bar_entry, start_time, 2, 'saving...')\n",
    "                \n",
    "                output_path = current_pat_line_path  + current_pat_client_id_code + \"/\" +str(current_pat_client_id_code) + \"_\" + str(target_date_range)+\".csv\"\n",
    "                \n",
    "                \n",
    "                if(remote_dump==False):\n",
    "                \n",
    "                    pat_concatted.to_csv(output_path)   \n",
    "                else:\n",
    "                    \n",
    "                    if(multi_process == True):\n",
    "                        \n",
    "                        write_remote(output_path, pat_concatted, stfp_obj)\n",
    "                    else:\n",
    "                        with sftp_client.open(output_path, 'w') as file:\n",
    "                            pat_concatted.to_csv(file)\n",
    "                \n",
    "                update_pbar(p_bar_entry, start_time, 2, f'Done {len(pat_concatted.columns)} cols in {int(time.time() - start_time)}s, {int(len(pat_concatted.columns)/int(time.time() - start_time))} p/s')\n",
    "                \n",
    "                #print(time.time() - start_time, current_pat_client_id_code)\n",
    "            except RuntimeError as RuntimeError_exception:\n",
    "                print(\"Caught runtime error... is torch?\")\n",
    "                print(RuntimeError)\n",
    "                print(\"sleeping 1h\")\n",
    "                time.sleep(3600)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(traceback.format_exc())\n",
    "                print(f\"Reproduce on {current_pat_client_id_code, target_date_range}\")\n",
    "                template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
    "                message = template.format(type(e).__name__, e.args)\n",
    "                print(message)\n",
    "    \n",
    "        else:\n",
    "            skipped_counter = skipped_counter + 1\n",
    "            pass\n",
    "            #print(f\"{current_pat_client_id_code} done already\")\n",
    "            #pat_concatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098cd129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for k in tqdm(range(0,len(all_patient_list))):\n",
    "#for k in tqdm(range(0,4)):   \n",
    "def main_multi(arg_list):\n",
    "    \n",
    "    current_pat_client_id_code, target_date_range = arg_list[0], arg_list[1]\n",
    "    \n",
    "    global skipped_counter\n",
    "    global start_time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    done_list = []\n",
    "    if(current_pat_client_id_code+\"_\"+str(target_date_range) not in stripped_list_start):\n",
    "        \n",
    "        \n",
    "        stripped_list = [x.replace(\".csv\",\"\") for x in list_dir_wrapper(current_pat_lines_path)]\n",
    "\n",
    "        if(current_pat_client_id_code + \"_\" + str(target_date_range) not in stripped_list):\n",
    "            #print(start_time, current_pat_client_id_code)\n",
    "            try:\n",
    "                #current_pat_client_id_code = all_patient_list[k]\n",
    "\n",
    "                patient_vector = []\n",
    "                \n",
    "                p_bar_entry = current_pat_client_id_code + \"_\" + str(target_date_range)\n",
    "                #\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 0, 'demo')\n",
    "\n",
    "                \n",
    "                if(main_options.get('demo')):\n",
    "                    current_pat_demo = get_demo(current_pat_client_id_code, target_date_range)\n",
    "                    patient_vector.append(current_pat_demo)\n",
    "\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 1, 'bmi')\n",
    "\n",
    "                if(main_options.get('bmi')):\n",
    "                    bmi_features = get_bmi_features(current_pat_client_id_code, target_date_range)\n",
    "                    patient_vector.append(bmi_features)\n",
    "\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 2, 'bloods')\n",
    "\n",
    "                if(main_options.get('bloods')):\n",
    "                    current_pat_bloods = get_current_pat_bloods(current_pat_client_id_code, target_date_range)\n",
    "                    patient_vector.append(current_pat_bloods)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 3, 'drugs')\n",
    "\n",
    "                if(main_options.get('drugs')):\n",
    "                    current_pat_drugs = get_current_pat_drugs(current_pat_client_id_code, target_date_range)\n",
    "                    patient_vector.append(current_pat_drugs)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 4, 'diagnostics')\n",
    "\n",
    "                if(main_options.get('diagnostics')):\n",
    "                    current_pat_diagnostics = get_current_pat_diagnostics(current_pat_client_id_code, target_date_range)\n",
    "                    patient_vector.append(current_pat_diagnostics)\n",
    "\n",
    "                #update_pbar(current_pat_client_id_code, start_time, 5, 'annotations', n_docs_to_annotate)\n",
    "\n",
    "\n",
    "                if(main_options.get('annotations')):\n",
    "                    df_pat_target = get_current_pat_annotations(current_pat_client_id_code, target_date_range)\n",
    "                    patient_vector.append(df_pat_target)\n",
    "                    \n",
    "                if(main_options.get('annotations_mrc')):\n",
    "                    df_pat_target = get_current_pat_annotations_mrc_cs(current_pat_client_id_code, target_date_range)\n",
    "                    patient_vector.append(df_pat_target)\n",
    "                \n",
    "                update_pbar(p_bar_entry, start_time, 1, 'core_02')\n",
    "                \n",
    "                if(main_options.get('core_02')):\n",
    "                    df_pat_target = get_core_02(current_pat_client_id_code, target_date_range)\n",
    "                    patient_vector.append(df_pat_target)\n",
    "                    \n",
    "                update_pbar(p_bar_entry, start_time, 2, 'bed')\n",
    "                    \n",
    "                if(main_options.get('bed')):\n",
    "                    df_pat_target = get_bed(current_pat_client_id_code, target_date_range)\n",
    "                    patient_vector.append(df_pat_target)\n",
    "                    \n",
    "                update_pbar(p_bar_entry, start_time, 3, 'vte_status')\n",
    "                    \n",
    "                if(main_options.get('vte_status')):\n",
    "                    df_pat_target = get_vte_status(current_pat_client_id_code, target_date_range)\n",
    "                    patient_vector.append(df_pat_target)\n",
    "                    \n",
    "                update_pbar(p_bar_entry, start_time, 4, 'hosp_site')    \n",
    "                    \n",
    "                if(main_options.get('hosp_site')):\n",
    "                    df_pat_target = get_hosp_site(current_pat_client_id_code, target_date_range)\n",
    "                    patient_vector.append(df_pat_target)\n",
    "                    \n",
    "                update_pbar(p_bar_entry, start_time, 1, 'core_resus')     \n",
    "                    \n",
    "                if(main_options.get('core_resus')):\n",
    "                    df_pat_target = get_core_resus(current_pat_client_id_code, target_date_range)\n",
    "                    patient_vector.append(df_pat_target)\n",
    "                    \n",
    "                update_pbar(p_bar_entry, start_time, 2, 'news')      \n",
    "                    \n",
    "                if(main_options.get('news')):\n",
    "                    df_pat_target = get_news(current_pat_client_id_code, target_date_range)\n",
    "                    patient_vector.append(df_pat_target)\n",
    "                    \n",
    "                update_pbar(p_bar_entry, start_time, 2, 'concatenating')\n",
    "                \n",
    "                target_date_vector = enum_target_date_vector(target_date_range, current_pat_client_id_code)\n",
    "                \n",
    "                    \n",
    "                patient_vector.append(target_date_vector)\n",
    "                \n",
    "\n",
    "                pat_concatted = pd.concat(patient_vector, axis=1)\n",
    "\n",
    "                pat_concatted.drop('client_idcode', axis=1, inplace=True)\n",
    "\n",
    "                pat_concatted.insert(0, 'client_idcode', current_pat_client_id_code)\n",
    "                \n",
    "                update_pbar(p_bar_entry, start_time, 2, 'saving...')\n",
    "                \n",
    "                if(remote_dump==False):\n",
    "                \n",
    "                    pat_concatted.to_csv(current_pat_line_path +str(current_pat_client_id_code) + \"_\" + str(target_date_range)+\".csv\")   \n",
    "                else:\n",
    "                    with sftp_client.open(current_pat_line_path +str(current_pat_client_id_code) + \"_\" + str(target_date_range)+\".csv\", 'w') as file:\n",
    "                        pat_concatted.to_csv(file)\n",
    "                \n",
    "                update_pbar(p_bar_entry, start_time, 2, f'Done {len(pat_concatted.columns)} cols in {int(time.time() - start_time)}s, {int(len(pat_concatted.columns)/int(time.time() - start_time))} p/s')\n",
    "                \n",
    "                #print(time.time() - start_time, current_pat_client_id_code)\n",
    "            except RuntimeError as RuntimeError_exception:\n",
    "                print(\"Caught runtime error... is torch?\")\n",
    "                print(RuntimeError)\n",
    "                print(\"sleeping 1h\")\n",
    "                time.sleep(3600)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(traceback.format_exc())\n",
    "                print(f\"Reproduce on {current_pat_client_id_code, target_date_range}\")\n",
    "                template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
    "                message = template.format(type(e).__name__, e.args)\n",
    "                print(message)\n",
    "    \n",
    "        else:\n",
    "            skipped_counter = skipped_counter + 1\n",
    "            pass\n",
    "            #print(f\"{current_pat_client_id_code} done already\")\n",
    "            #pat_concatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a4aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "slow_execution_threshold_low  = 10\n",
    "slow_execution_threshold_high = 30\n",
    "slow_execution_threshold_extreme = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27684f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aad653",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_pat_lines_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cff553",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_pat_line_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80fddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_dir_wrapper(current_pat_line_path, sftp_client)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6f8b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stripped_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd18fd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patient_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b329dad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stripped_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9ef92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# if(remote_dump==False):\n",
    "#     stripped_list = [x.replace(\".csv\",\"\") for x in list_dir_wrapper(current_pat_line_path)]\n",
    "#     all_patient_list = [fruit for fruit in all_patient_list if fruit not in stripped_list]\n",
    "    \n",
    "# else:\n",
    "#     stripped_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aafc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stripped_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25207d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_list_bool = False\n",
    "\n",
    "if(priority_list_bool):\n",
    "    df_old_done = pd.read_csv('/data/AS/Samora/HFE/HFE/v18/current_pat_lines_parts/current_pat_lines__part_0_merged.csv',usecols=['client_idcode', 'Hemochromatosis (disorder)_count_subject_present'])\n",
    "    \n",
    "    priority_list = df_old_done[df_old_done['Hemochromatosis (disorder)_count_subject_present']>0]['client_idcode'].to_list()\n",
    "    \n",
    "    all_patient_list = priority_list #+ all_patient_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e8034a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3b2035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from functools import partialmethod\n",
    "import tqdm as tqdm_pbar\n",
    "from tqdm import trange\n",
    "from multiprocessing import Pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac8b9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f969a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#medcat_model_pack_316666b47dfaac07.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c71149",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(aliencat):\n",
    "    cat = CAT.load_model_pack('/home/aliencat/samora/HFE/HFE/medcat_models/medcat_model_pack_316666b47dfaac07.zip')\n",
    "elif(dgx):\n",
    "    cat = CAT.load_model_pack('/data/AS/Samora/HFE/HFE/v18/' + 'medcat_models/20230328_trained_model_hfe_redone/medcat_model_pack_316666b47dfaac07');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dc20c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat = CAT.load_model_pack('/data/AS/Samora/HFE/HFE/v18/' + 'medcat_models/20230328_trained_model_hfe_redone/medcat_model_pack_316666b47dfaac07');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8c0634",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat = CAT.load_model_pack('/data/AS/Samora/HFE/HFE/v18/' + 'medcat_models/20230214_trained_model_hfe_redone/medcat_model_pack_bec3865f4a29ee20');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1df2a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed()\n",
    "random.shuffle(all_patient_list)\n",
    "\n",
    "skipped_counter = 0\n",
    "t = trange(len(all_patient_list), desc='Bar desc', leave=True, colour=\"GREEN\", position=0, total=len(all_patient_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70158abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bedf6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patient_list_bu = all_patient_list.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923d62d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_patient_list = all_patient_list[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff273f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_list = [x.replace(\".csv\",\"\") for x in list_dir_wrapper(current_pat_lines_path, sftp_client)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5227e7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stripped_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060c73ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1304a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if(strip_list):\n",
    "    stripped_list_start_copy = stripped_list.copy()\n",
    "\n",
    "    container_list = []\n",
    "    \n",
    "    if(remote_dump):\n",
    "        ssh_client = paramiko.SSHClient()\n",
    "        ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "        ssh_client.connect(hostname=hostname, username=username, password=password)\n",
    "\n",
    "        sftp_client = ssh_client.open_sftp()\n",
    "        \n",
    "        \n",
    "\n",
    "        for i in (range(0, len(stripped_list))):\n",
    "            try:\n",
    "                if(len(sftp_client.listdir(current_pat_lines_path + stripped_list[i])) >300):\n",
    "                    container_list.append(stripped_list[i])\n",
    "            except:\n",
    "                pass\n",
    "#             stripped_list_start_copy.remove(stripped_list_start[i])\n",
    "\n",
    "\n",
    "    else:\n",
    "        for i in tqdm(range(0, len(stripped_list))):\n",
    "            if(len(list_dir_wrapper(current_pat_lines_path + stripped_list[i])) >300):\n",
    "                container_list.append(stripped_list[i])\n",
    "        \n",
    "\n",
    "    stripped_list_start = container_list.copy()\n",
    "    stripped_list = container_list.copy()\n",
    "    if(remote_dump):\n",
    "        sftp_client.close()\n",
    "        ssh_client.close()\n",
    "else:\n",
    "    stripped_list = []\n",
    "    stripped_list_start = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cac0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_arg_list = []\n",
    "for i in range(0, len(all_patient_list)):\n",
    "    for j in range(0, len(combinations)):\n",
    "        all_arg_list.append([all_patient_list[i], combinations[j]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facb5c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_arg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bbd4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed()\n",
    "# random.shuffle(all_arg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7029d3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = trange(len(all_arg_list), desc='Bar desc', leave=True, colour=\"GREEN\", position=0, total=len(all_arg_list))\n",
    "# skipped_counter = 0\n",
    "# main_multi(all_arg_list[300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163406a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_additional_listdir = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf3b738",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def main_batch(current_pat_client_id_code,\n",
    "               target_date_range,\n",
    "               batch_demo = None,\n",
    "               batch_smoking = None,\n",
    "               batch_core_02 = None,\n",
    "               batch_bednumber = None,\n",
    "               batch_vte = None,\n",
    "               batch_hospsite = None,\n",
    "               batch_resus = None,\n",
    "               batch_news = None,\n",
    "               batch_bmi = None,\n",
    "               batch_diagnostics = None,\n",
    "               batch_epr = None,\n",
    "               batch_mct = None,\n",
    "               batch_bloods = None,\n",
    "               batch_drugs = None,\n",
    "            \n",
    "              sftp_obj = None):\n",
    "    global skipped_counter\n",
    "    global start_time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    \n",
    "    already_done = False\n",
    "    \n",
    "    \n",
    "    done_list = []\n",
    "    if(current_pat_client_id_code not in stripped_list_start):\n",
    "        \n",
    "        \n",
    "        if(skip_additional_listdir):\n",
    "            stripped_list = stripped_list_start\n",
    "        else:\n",
    "            \n",
    "            if(len(list_dir_wrapper(current_pat_lines_path + current_pat_client_id_code, sftp_obj)) >=336):\n",
    "                already_done = True\n",
    "                stripped_list_start.append(current_pat_client_id_code)\n",
    "            stripped_list = stripped_list_start.copy()\n",
    "            \n",
    "                \n",
    "            #stripped_list = []\n",
    "#             stripped_list = [x for x in list_dir_wrapper(current_pat_lines_path)]\n",
    "\n",
    "        if(current_pat_client_id_code not in stripped_list and already_done==False):\n",
    "        \n",
    "            try:\n",
    "                patient_vector = []\n",
    "                \n",
    "                p_bar_entry = current_pat_client_id_code + \"_\" + str(target_date_range)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 0, 'demo')\n",
    "\n",
    "                if(main_options.get('demo')):\n",
    "                    current_pat_demo = get_demo(current_pat_client_id_code, target_date_range, batch_demo)\n",
    "                    patient_vector.append(current_pat_demo)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 1, 'bmi')\n",
    "\n",
    "                if(main_options.get('bmi')):\n",
    "                    bmi_features = get_bmi_features(current_pat_client_id_code, target_date_range, batch_bmi)\n",
    "                    patient_vector.append(bmi_features)\n",
    "\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 2, 'bloods')\n",
    "\n",
    "                if(main_options.get('bloods')):\n",
    "                    current_pat_bloods = get_current_pat_bloods(current_pat_client_id_code, target_date_range, batch_bloods)\n",
    "                    patient_vector.append(current_pat_bloods)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 3, 'drugs')\n",
    "\n",
    "                if(main_options.get('drugs')):\n",
    "                    current_pat_drugs = get_current_pat_drugs(current_pat_client_id_code, target_date_range, batch_drugs)\n",
    "                    patient_vector.append(current_pat_drugs)\n",
    "\n",
    "                update_pbar(p_bar_entry, start_time, 4, 'diagnostics')\n",
    "\n",
    "                if(main_options.get('diagnostics')):\n",
    "                    current_pat_diagnostics = get_current_pat_diagnostics(current_pat_client_id_code, target_date_range, batch_diagnostics)\n",
    "                    patient_vector.append(current_pat_diagnostics)\n",
    "\n",
    "\n",
    "                if(main_options.get('annotations')):\n",
    "                    df_pat_target = get_current_pat_annotations(current_pat_client_id_code, target_date_range, batch_epr, sftp_obj)\n",
    "                    patient_vector.append(df_pat_target)\n",
    "                    \n",
    "                if(main_options.get('annotations_mrc')):\n",
    "                    df_pat_target = get_current_pat_annotations_mrc_cs(current_pat_client_id_code, target_date_range, batch_mct, sftp_obj)\n",
    "                    patient_vector.append(df_pat_target)\n",
    "                \n",
    "                update_pbar(p_bar_entry, start_time, 1, 'core_02')\n",
    "                \n",
    "                if(main_options.get('core_02')):\n",
    "                    df_pat_target = get_core_02(current_pat_client_id_code, target_date_range, batch_core_02)\n",
    "                    patient_vector.append(df_pat_target)\n",
    "                    \n",
    "                update_pbar(p_bar_entry, start_time, 2, 'bed')\n",
    "                    \n",
    "                if(main_options.get('bed')):\n",
    "                    df_pat_target = get_bed(current_pat_client_id_code, target_date_range, batch_bednumber)\n",
    "                    patient_vector.append(df_pat_target)\n",
    "                    \n",
    "                update_pbar(p_bar_entry, start_time, 3, 'vte_status')\n",
    "                    \n",
    "                if(main_options.get('vte_status')):\n",
    "                    df_pat_target = get_vte_status(current_pat_client_id_code, target_date_range, batch_vte)\n",
    "                    patient_vector.append(df_pat_target)\n",
    "                    \n",
    "                update_pbar(p_bar_entry, start_time, 4, 'hosp_site')    \n",
    "                    \n",
    "                if(main_options.get('hosp_site')):\n",
    "                    df_pat_target = get_hosp_site(current_pat_client_id_code, target_date_range, batch_hospsite)\n",
    "                    patient_vector.append(df_pat_target)\n",
    "                    \n",
    "                update_pbar(p_bar_entry, start_time, 1, 'core_resus')     \n",
    "                    \n",
    "                if(main_options.get('core_resus')):\n",
    "                    df_pat_target = get_core_resus(current_pat_client_id_code, target_date_range, batch_resus)\n",
    "                    patient_vector.append(df_pat_target)\n",
    "                    \n",
    "                update_pbar(p_bar_entry, start_time, 2, 'news')      \n",
    "                    \n",
    "                if(main_options.get('news')):\n",
    "                    df_pat_target = get_news(current_pat_client_id_code, target_date_range, batch_news)\n",
    "                    patient_vector.append(df_pat_target)\n",
    "                    \n",
    "                update_pbar(p_bar_entry, start_time, 2, 'concatenating')\n",
    "                \n",
    "                target_date_vector = enum_target_date_vector(target_date_range, current_pat_client_id_code)\n",
    "                \n",
    "                    \n",
    "                patient_vector.append(target_date_vector)\n",
    "                \n",
    "\n",
    "                pat_concatted = pd.concat(patient_vector, axis=1)\n",
    "\n",
    "                pat_concatted.drop('client_idcode', axis=1, inplace=True)\n",
    "\n",
    "                pat_concatted.insert(0, 'client_idcode', current_pat_client_id_code)\n",
    "                \n",
    "                \n",
    "                                \n",
    "                update_pbar(p_bar_entry, start_time, 2, 'saving...')\n",
    "                \n",
    "                output_path = current_pat_line_path  + current_pat_client_id_code + \"/\" +str(current_pat_client_id_code) + \"_\" + str(target_date_range)+\".csv\"\n",
    "                \n",
    "                \n",
    "                if(remote_dump==False):\n",
    "                \n",
    "                    pat_concatted.to_csv(output_path)   \n",
    "                else:\n",
    "                    \n",
    "                    if(multi_process == True):\n",
    "                        \n",
    "                        write_remote(output_path, pat_concatted, sftp_obj)\n",
    "                    else:\n",
    "                        with sftp_client.open(output_path, 'w') as file:\n",
    "                            pat_concatted.to_csv(file)\n",
    "                        \n",
    "                        \n",
    "                #display(type(pat_concatted))\n",
    "                try:\n",
    "                    update_pbar(p_bar_entry, start_time, 2, f'Done {len(pat_concatted.columns)} cols in {int(time.time() - start_time)}s, {int((len(pat_concatted.columns)+1)/int(time.time() - start_time)+1)} p/s')\n",
    "                except:\n",
    "                    update_pbar(p_bar_entry, start_time, 2, f'Columns n={len(pat_concatted.columns)}')\n",
    "                    pass\n",
    "                \n",
    "                #display(pat_concatted)\n",
    "                #print(time.time() - start_time, current_pat_client_id_code)\n",
    "            except RuntimeError as RuntimeError_exception:\n",
    "                print(\"Caught runtime error... is torch?\")\n",
    "                print(RuntimeError)\n",
    "                print(\"sleeping 1h\")\n",
    "                time.sleep(3600)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(traceback.format_exc())\n",
    "                print(f\"Reproduce on {current_pat_client_id_code, target_date_range}\")\n",
    "                template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n",
    "                message = template.format(type(e).__name__, e.args)\n",
    "                print(message)\n",
    "    \n",
    "        else:\n",
    "            if(multi_process == False):\n",
    "                skipped_counter = skipped_counter + 1\n",
    "            else:\n",
    "                with skipped_counter.get_lock():\n",
    "                    skipped_counter.value += 1\n",
    "            pass\n",
    "            #print(f\"{current_pat_client_id_code} done already\")\n",
    "            #pat_concatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_start_year, global_start_month, global_end_year, global_end_month = '1995', '01', '2023', '11' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18af7a05",
   "metadata": {},
   "source": [
    "#get_pat batches\n",
    "\n",
    "search_term = 'CORE_SmokingStatus'\n",
    "\n",
    "batch_smoking = get_pat_batch_obs(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "search_term = 'CORE_SpO2'\n",
    "\n",
    "batch_core_02 = get_pat_batch_obs(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "search_term = 'CORE_BedNumber3'\n",
    "\n",
    "batch_bednumber = get_pat_batch_obs(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "search_term = 'CORE_VTE_STATUS'\n",
    "\n",
    "batch_vte = get_pat_batch_obs(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "search_term = 'CORE_HospitalSite'\n",
    "\n",
    "batch_hospsite = get_pat_batch_obs(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "search_term = 'CORE_RESUS_STATUS'\n",
    "\n",
    "batch_resus = get_pat_batch_obs(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "search_term = None # inside function\n",
    "batch_news = get_pat_batch_news(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "search_term = None # inside function\n",
    "batch_bmi = get_pat_batch_bmi(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "search_term = None # inside function\n",
    "batch_diagnostics = get_pat_batch_diagnostics(current_pat_client_id_code, search_term)\n",
    "\n",
    "search_term = None # inside function\n",
    "batch_drugs = get_pat_batch_drugs(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "search_term = None # inside function\n",
    "batch_epr = get_pat_batch_epr_docs(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "search_term = None # inside function\n",
    "batch_mct = get_pat_batch_mct_docs(current_pat_client_id_code, search_term)\n",
    "\n",
    "search_term = None # inside function\n",
    "batch_demo = get_pat_batch_demo(current_pat_client_id_code, search_term)\n",
    "\n",
    "search_term = None # inside function\n",
    "batch_bloods =  get_pat_batch_bloods(current_pat_client_id_code, search_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd4685f",
   "metadata": {},
   "source": [
    "batch_bloods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c7f6a8",
   "metadata": {},
   "source": [
    "target_date_range = (2022, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3434830b",
   "metadata": {},
   "source": [
    "target_date_range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcd433c",
   "metadata": {},
   "source": [
    "#pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a894c1",
   "metadata": {},
   "source": [
    "#%%prun\n",
    "\n",
    "\n",
    "main_batch(current_pat_client_id_code,\n",
    "               target_date_range,\n",
    "               batch_demo = batch_demo,\n",
    "               batch_smoking = batch_smoking,\n",
    "               batch_core_02 = batch_core_02,\n",
    "               batch_bednumber = batch_bednumber,\n",
    "               batch_vte = batch_vte,\n",
    "               batch_hospsite = batch_hospsite,\n",
    "               batch_resus = batch_resus,\n",
    "               batch_news = batch_news,\n",
    "               batch_bmi = batch_bmi,\n",
    "               batch_diagnostics = batch_diagnostics,\n",
    "               batch_epr = batch_epr,\n",
    "               batch_mct = batch_mct,\n",
    "               batch_bloods = batch_bloods,\n",
    "               batch_drugs = batch_drugs\n",
    "            \n",
    "              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f0356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotate_only = False\n",
    "\n",
    "if(annotate_only):\n",
    "    random.seed()\n",
    "    random.shuffle(all_patient_list)\n",
    "\n",
    "    skipped_counter = 0\n",
    "    t = trange(len(all_patient_list), desc='Bar desc', leave=True, colour=\"GREEN\", position=0, total=len(all_patient_list))\n",
    "    \n",
    "    for i in t:\n",
    "        global_start_year, global_start_month, global_end_year, global_end_month = '1995', '01', '2023', '11' \n",
    "\n",
    "\n",
    "        current_pat_doc_batch = cohort_searcher_with_terms_and_search(index_name=\"epr_documents\", \n",
    "                                                                          fields_list = \"\"\"client_idcode document_guid\tdocument_description\tbody_analysed updatetime clientvisit_visitidcode\"\"\".split(),\n",
    "                                                                          term_name = \"client_idcode.keyword\", \n",
    "                                                                          entered_list = [all_patient_list[i]],\n",
    "                                                                        search_string = f'updatetime:[{global_start_year}-{global_start_month} TO {global_end_year}-{global_end_month}] ')\n",
    "\n",
    "        current_pat_doc_mct_batch = cohort_searcher_with_terms_and_search(index_name=\"observations\", \n",
    "                                                                           fields_list=\"\"\"observation_guid client_idcode\tobscatalogmasteritem_displayname\tobservation_valuetext_analysed\tobservationdocument_recordeddtm clientvisit_visitidcode\"\"\".split(),\n",
    "                                                                           term_name=\"client_idcode.keyword\", \n",
    "                                                                           entered_list=[all_patient_list[i]], \n",
    "                                                                           search_string=\"obscatalogmasteritem_displayname:(\\\"AoMRC_ClinicalSummary_FT\\\") AND \" + f'observationdocument_recordeddtm:[{global_start_year}-{global_start_month} TO {global_end_year}-{global_end_month}]')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for j in range(0, len(combinations)):\n",
    "            try:\n",
    "                if(all_patient_list[i] not in stripped_list):\n",
    "                    get_current_pat_annotations_batch_to_file(all_patient_list[i], combinations[j], current_pat_doc_batch)\n",
    "\n",
    "                    get_current_pat_annotations_mct_batch_to_file(all_patient_list[i], combinations[j], current_pat_doc_mct_batch)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(all_patient_list[i], combinations[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9c86ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_pat_line_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb3bb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_remote(path, csv_file, sftp_obj=None):\n",
    "    #print(\"writing remote\")\n",
    "    if(share_sftp == False):\n",
    "        ssh_client = paramiko.SSHClient()\n",
    "        ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "        ssh_client.connect(hostname=hostname, username=username, password=password)\n",
    "\n",
    "        sftp_client = ssh_client.open_sftp()\n",
    "        sftp_obj = sftp_client\n",
    "\n",
    "\n",
    "    with sftp_obj.open(path, 'w') as file:\n",
    "        csv_file.to_csv(file)\n",
    "\n",
    "    if(share_sftp == False):\n",
    "        sftp_obj.close()\n",
    "        sftp_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0872b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed()\n",
    "random.shuffle(all_patient_list)\n",
    "\n",
    "skipped_counter = 0\n",
    "t = trange(len(all_patient_list), desc='Bar desc', leave=True, colour=\"GREEN\", position=0, total=len(all_patient_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114b4dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801a4b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63c31e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2105df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pat_maker(i):\n",
    "    global skipped_counter\n",
    "    global stripped_list\n",
    "    \n",
    "    current_pat_client_id_code = all_patient_list[i]\n",
    "    \n",
    "    p_bar_entry = current_pat_client_id_code\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    update_pbar(p_bar_entry, start_time, 0, f'Pat_maker called on {i}...')\n",
    "    \n",
    "    #time.sleep(random.randint(1, 50))\n",
    "    #i, sftp_obj = i[0], i[1]\n",
    "    if(remote_dump):\n",
    "        ssh_client = paramiko.SSHClient()\n",
    "        ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "        ssh_client.connect(hostname=hostname, username=username, password=password, timeout=60)\n",
    "\n",
    "        sftp_obj = ssh_client.open_sftp()\n",
    "    else:\n",
    "        sftp_obj = None\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    #get_pat batches\n",
    "    \n",
    "    stripped_list = stripped_list_start.copy()\n",
    "    \n",
    "    \n",
    "    if(current_pat_client_id_code not in stripped_list_start):\n",
    "        \n",
    "        update_pbar(p_bar_entry, start_time, 0, 'Getting batches...')\n",
    "    \n",
    "    \n",
    "        search_term = None # inside function\n",
    "        batch_epr = get_pat_batch_epr_docs(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "        search_term = None # inside function\n",
    "        batch_mct = get_pat_batch_mct_docs(current_pat_client_id_code, search_term)\n",
    "\n",
    "        if(annot_first == False):\n",
    "\n",
    "            search_term = 'CORE_SmokingStatus'\n",
    "\n",
    "            batch_smoking = get_pat_batch_obs(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "            search_term = 'CORE_SpO2'\n",
    "\n",
    "            batch_core_02 = get_pat_batch_obs(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "            search_term = 'CORE_BedNumber3'\n",
    "\n",
    "            batch_bednumber = get_pat_batch_obs(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "            search_term = 'CORE_VTE_STATUS'\n",
    "\n",
    "            batch_vte = get_pat_batch_obs(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "            search_term = 'CORE_HospitalSite'\n",
    "\n",
    "            batch_hospsite = get_pat_batch_obs(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "            search_term = 'CORE_RESUS_STATUS'\n",
    "\n",
    "            batch_resus = get_pat_batch_obs(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "            search_term = None # inside function\n",
    "            batch_news = get_pat_batch_news(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "            search_term = None # inside function\n",
    "            batch_bmi = get_pat_batch_bmi(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "            search_term = None # inside function\n",
    "            batch_diagnostics = get_pat_batch_diagnostics(current_pat_client_id_code, search_term)\n",
    "\n",
    "            search_term = None # inside function\n",
    "            batch_drugs = get_pat_batch_drugs(current_pat_client_id_code, search_term)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            search_term = None # inside function\n",
    "            batch_demo = get_pat_batch_demo(current_pat_client_id_code, search_term)\n",
    "\n",
    "            search_term = None # inside function\n",
    "            batch_bloods =  get_pat_batch_bloods(current_pat_client_id_code, search_term)\n",
    "\n",
    "        update_pbar(p_bar_entry, start_time, 0, f'Done batches in {time.time()-start_time}')\n",
    "\n",
    "\n",
    "        run_on_pat = False\n",
    "\n",
    "        only_check_last = True\n",
    "\n",
    "        last_check = all_patient_list[i] not in stripped_list\n",
    "\n",
    "        skip_check = last_check\n",
    "\n",
    "        for j in range(0, len(combinations)):\n",
    "            try:\n",
    "                if(only_check_last):\n",
    "                    run_on_pat = last_check\n",
    "                else:\n",
    "                    run_on_pat = all_patient_list[i]  not in stripped_list\n",
    "\n",
    "\n",
    "                if(run_on_pat):   \n",
    "                    if(annot_first):\n",
    "\n",
    "                        get_current_pat_annotations_batch_to_file(all_patient_list[i], combinations[j], batch_epr, sftp_obj, skip_check=skip_check)\n",
    "\n",
    "                        get_current_pat_annotations_mct_batch_to_file(all_patient_list[i], combinations[j], batch_mct, sftp_obj, skip_check=skip_check)\n",
    "\n",
    "                    else:\n",
    "                        main_batch(all_patient_list[i],\n",
    "                           combinations[j],\n",
    "                           batch_demo = batch_demo,\n",
    "                           batch_smoking = batch_smoking,\n",
    "                           batch_core_02 = batch_core_02,\n",
    "                           batch_bednumber = batch_bednumber,\n",
    "                           batch_vte = batch_vte,\n",
    "                           batch_hospsite = batch_hospsite,\n",
    "                           batch_resus = batch_resus,\n",
    "                           batch_news = batch_news,\n",
    "                           batch_bmi = batch_bmi,\n",
    "                           batch_diagnostics = batch_diagnostics,\n",
    "                           batch_epr = batch_epr,\n",
    "                           batch_mct = batch_mct,\n",
    "                           batch_bloods = batch_bloods,\n",
    "                           batch_drugs = batch_drugs,\n",
    "                           sftp_obj = sftp_obj\n",
    "\n",
    "                          )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(f\"Exception in patmaker on {all_patient_list[i], combinations[j]}\")\n",
    "                print(traceback.format_exc())\n",
    "        if(remote_dump):\n",
    "            sftp_obj.close()\n",
    "            ssh_client.close()\n",
    "    else:\n",
    "        if(multi_process == False):\n",
    "            skipped_counter = skipped_counter + 1\n",
    "            update_pbar(str(i), start_time, 0, f'Skipped {i}')\n",
    "        else:\n",
    "            with skipped_counter.get_lock():\n",
    "                skipped_counter.value += 1\n",
    "            update_pbar(str(i), start_time, 0, f'Skipped {i}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6693d2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed()\n",
    "random.shuffle(all_patient_list)\n",
    "all_patient_list = [x for x in all_patient_list if x not in stripped_list_start]\n",
    "\n",
    "skipped_counter = 0\n",
    "t = trange(len(all_patient_list), desc='Bar desc', leave=True, colour=\"GREEN\", position=0, total=len(all_patient_list))\n",
    "\n",
    "all_patient_list = [x for x in all_patient_list if x not in stripped_list_start]\n",
    "\n",
    "skipped_counter = 0\n",
    "t = trange(len(all_patient_list), desc='Bar desc', leave=True, colour=\"GREEN\", position=0, total=len(all_patient_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92c3d78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab53e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pat_maker(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576284bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed()\n",
    "# random.shuffle(all_patient_list)\n",
    "\n",
    "# skipped_counter = 0\n",
    "# t = trange(10, desc='Bar desc', leave=True, colour=\"GREEN\", position=0, total=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9074505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1655ea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    " #pat_maker(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a63a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssh_client = paramiko.SSHClient()\n",
    "# ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "# ssh_client.connect(hostname=hostname, username=username, password=password, timeout=10)\n",
    "\n",
    "# sftp_client = ssh_client.open_sftp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7913af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#could annotate every document in the batch and store result in a dictionary of dates, use dict lookup to fill files in loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd38ac95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d507dd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, Value\n",
    "skipped_counter = None\n",
    "\n",
    "def init(args):\n",
    "    ''' store the counter for later use '''\n",
    "    global skipped_counter\n",
    "    skipped_counter = args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c486899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#init?\n",
    "stripped_list = stripped_list_start.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab9bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_process = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f40db86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%%prun\n",
    "pat_maker(0)\n",
    "stripped_list_start.append(all_patient_list[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9217a691",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipped_counter = 0\n",
    "t = trange(len(all_patient_list), desc='Bar desc', leave=True, colour=\"GREEN\", position=0, total=len(all_patient_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35338d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_process = True\n",
    "\n",
    "# if(multi_process):\n",
    "#     #sftp_client.close()\n",
    "#     ssh_client.close()\n",
    "\n",
    "if(multi_process):\n",
    "\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        skipped_counter = Value('i', 0)\n",
    "\n",
    "        pool=Pool(processes=3, initializer = init, initargs = (skipped_counter, ))\n",
    "        #pool = eventlet.GreenPool(size=1000)\n",
    "        for patient in pool.imap(pat_maker, [x for x in range(len(all_patient_list))]):\n",
    "\n",
    "            _\n",
    "        pool.close()\n",
    "else:\n",
    "    for i in t:\n",
    "        pat_maker(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba9629c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49a8d2bb",
   "metadata": {},
   "source": [
    "# Threadpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9037a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "if(multi_process):\n",
    "    if __name__ == '__main__':\n",
    "        skipped_counter = Value('i', 0)\n",
    "\n",
    "        pool=ThreadPool(processes=8, initializer = init, initargs = (skipped_counter, ))\n",
    "        #pool = ThreadPool(80)\n",
    "        results = pool.map(pat_maker, [x for x in range(0, len(all_patient_list))])\n",
    "    pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2f7098",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519c890c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
